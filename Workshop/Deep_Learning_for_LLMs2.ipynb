{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["$${\\color{yellow}{\\text{Deep Learning for LLMs}}}$$\n","\n"],"metadata":{"id":"M7g7bxFCHxGP"}},{"cell_type":"markdown","source":["---\n","\n","Load essential libraries\n","\n","---"],"metadata":{"id":"0_BbyTKXQflD"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import matplotlib.pyplot as plt\n","plt.style.use('dark_background')\n","%matplotlib inline\n","import sys\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n","from sklearn.metrics import confusion_matrix\n","import gensim.downloader\n","import nltk\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"20W0d4ruQjE4","executionInfo":{"status":"ok","timestamp":1733650917598,"user_tz":-330,"elapsed":22708,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Mount Google Drive folder if running Google Colab\n","\n","---"],"metadata":{"id":"sfYXkqmLiVLM"}},{"cell_type":"code","source":["## Mount Google drive folder if running in Colab\n","if('google.colab' in sys.modules):\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount = True)\n","    DIR = '/content/drive/MyDrive/Colab Notebooks/OddSemester2024/Workshop'\n","    DATA_DIR = DIR+'/Data/'\n","else:\n","    DATA_DIR = 'Data/'"],"metadata":{"id":"VYzBBBxqiaGa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733650954754,"user_tz":-330,"elapsed":37168,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"f3e481e0-acfb-4869-f7b6-86aa690cdbe9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["---\n","\n","**We will now use Pytorch to create tensors**\n","\n","The patient data matrix:\n","\n","![patient data matrix](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=1000)\n","\n","**Notation**:\n","\n","Zeroth patient vector $\\mathbf{x}^{(0)}= \\begin{bmatrix}72\\\\120\\\\37.3\\\\104\\\\32.5\\end{bmatrix}$ and zeroth feature (heart rate vector) $\\mathbf{x}_0 = \\begin{bmatrix}72\\\\85\\\\68\\\\90\\\\84\\\\78\\end{bmatrix}.$\n","\n","---\n","\n"],"metadata":{"id":"avVZ6D1ZgEUT"}},{"cell_type":"code","source":["## Create a patient data matrix as a constant tensor\n","X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n","                 [85, 130, 37.0, 110, 14],\n","                 [68, 110, 38.5, 125, 34],\n","                 [90, 140, 38.0, 130, 26],\n","                 [84, 132, 38.3, 146, 30],\n","                 [78, 128, 37.2, 102, 12]])\n","print(X)\n","print(X.shape)\n","# X is a rank-2 tensor which is similar to a numpy 2D array\n","print(X[0]) # this is patient-0 info which is a rank-1 tensor\n","print(X[0, 2])"],"metadata":{"id":"zrPnepAEvr0O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733650962141,"user_tz":-330,"elapsed":1020,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"aea68bd7-b6c4-496f-85a1-f54f2aceba9d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n","        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n","        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n","        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n","        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n","        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]])\n","torch.Size([6, 5])\n","tensor([ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000])\n","tensor(37.3000)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","**Convert a PyTorch object into a numpy array**\n","\n","---"],"metadata":{"id":"cevtn_b4gek5"}},{"cell_type":"code","source":["X_numpy = X.numpy()\n","print(X_numpy)\n","print(type(X_numpy))\n","print(X_numpy.shape)"],"metadata":{"id":"JrYQ2moygfPu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733650963849,"user_tz":-330,"elapsed":1078,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"ec0d332c-73f6-45e7-e1f3-987546fda9fe"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 72.  120.   37.3 104.   32.5]\n"," [ 85.  130.   37.  110.   14. ]\n"," [ 68.  110.   38.5 125.   34. ]\n"," [ 90.  140.   38.  130.   26. ]\n"," [ 84.  132.   38.3 146.   30. ]\n"," [ 78.  128.   37.2 102.   12. ]]\n","<class 'numpy.ndarray'>\n","(6, 5)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","**Addition and subtraction of vectors, scalar multiplication (apply operation componentwise)**\n","\n","![vector addition](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NokBAAAAAZLAaAoWwhtn8Vk26NotALo?width=256)\n","\n","![vector subtracton](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3M4kBAAAAAU_n_mAEv006QFZm_sUj2Dc?width=256)\n","\n","![vector multiplication](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NIkBAAAAAa_qL04bLT4kWoNeHcrR9LQ?width=256)\n","\n","![vector geometry1](https://1drv.ms/i/c/37720f927b6ddc34/IQSGNMr5z3SSRry7LSKL7LybAcGYuzgw5smabV8-6DudXIs?width=230)\n","\n","![vector geometry2](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=192)\n","\n","\n","---"],"metadata":{"id":"QS3MmzwsgkWU"}},{"cell_type":"code","source":["# Vector addition\n","print(X[1, :] + X[2, :])\n","\n","# Vector subtraction\n","print(X[1, :] - X[2, :]) # how different patient-1 and patient-2 are\n","\n","# Scalar-vector multiplication\n","print(X[:, 2])\n","print((9/5)*X[:, 2] + 32)\n","\n","# Average patient\n","print((1/6)*(X[0, :] + X[1, :] + X[2, :] + X[3, :] + X[4, :] + X[5, :]))\n","print(torch.mean(X, dim = 0)) # dim = 0 means top-to-bottom operation or each row is an element"],"metadata":{"id":"TgPtJP0sglQP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733650966590,"user_tz":-330,"elapsed":485,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"e4200368-62f7-4fae-a629-2455ff6fb900"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([153.0000, 240.0000,  75.5000, 235.0000,  48.0000])\n","tensor([ 17.0000,  20.0000,  -1.5000, -15.0000, -20.0000])\n","tensor([37.3000, 37.0000, 38.5000, 38.0000, 38.3000, 37.2000])\n","tensor([ 99.1400,  98.6000, 101.3000, 100.4000, 100.9400,  98.9600])\n","tensor([ 79.5000, 126.6667,  37.7167, 119.5000,  24.7500])\n","tensor([ 79.5000, 126.6667,  37.7167, 119.5000,  24.7500])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Application of vector subtraction in natural language processing (NLP): download the word embedding model trained on Wikipedia articles.\n","\n","---"],"metadata":{"id":"1t_qXrlCROKA"}},{"cell_type":"code","source":["model = gensim.downloader.load('glove-wiki-gigaword-50')"],"metadata":{"id":"_e13FnW0RUwy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019358,"user_tz":-330,"elapsed":50951,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"194898ad-73c3-47d3-a3c1-81cb82b947ec"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Now we will see what embedding vector comes as a result of applying the model for the words *cricket* and *football*.\n","\n","Next, we will do an *intuitive* subtraction of word embeddings as in\n","\n","1. Cricket without Tendulkar\n","2. Football without Messi\n","\n","Note that the embedding vectors have 50 components corresponding to the 50-dimensional embedding of model suggested by the name '**glove-wiki-gigaword-50**'\n","\n","---"],"metadata":{"id":"7YRVJferRlK5"}},{"cell_type":"code","source":["print(model['cricket'])\n","print(model['football'])\n","a = model['cricket'] - model['tendulkar']\n","b = model['football'] - model['messi']\n","print(a)\n","print(b)"],"metadata":{"id":"HVVFzeQyR3Wb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019359,"user_tz":-330,"elapsed":34,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"656632c0-3ec3-4269-e90e-d8a0b6aa2cd0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.2233    -0.28292   -1.7429     0.56621   -0.13787   -0.88088\n"," -0.26887    0.41893    0.94977   -0.61332    0.0092522  1.0045\n"," -0.89004   -0.55148    0.61202    0.42256    0.92935    0.83307\n"," -1.5568     0.084345  -0.018224   0.84803    0.68321    0.97059\n","  0.26561   -1.0538     0.40724   -0.45079   -0.89013    0.94153\n","  2.2055     0.75363    0.5166     0.47977    0.86824    0.57228\n","  0.81846   -0.070934  -0.9391    -0.81628   -0.35486   -0.010408\n"," -0.83316    1.1001    -0.087408   1.8452    -0.83112    0.43732\n","  0.63007   -0.81023  ]\n","[-1.8209    0.70094  -1.1403    0.34363  -0.42266  -0.92479  -1.3942\n","  0.28512  -0.78416  -0.52579   0.89627   0.35899  -0.80087  -0.34636\n","  1.0854   -0.087046  0.63411   1.1429   -1.6264    0.41326  -1.1283\n"," -0.16645   0.17424   0.99585  -0.81838  -1.7724    0.078281  0.13382\n"," -0.59779  -0.45068   2.5474    1.0693   -0.27017  -0.75646   0.24757\n","  1.0261    0.11329   0.17668  -0.23257  -1.1561   -0.10665  -0.25377\n"," -0.65102   0.32393  -0.58262   0.88137  -0.13465   0.96903  -0.076259\n"," -0.59909 ]\n","[-0.7716      0.41267997 -1.725968   -0.10445005 -1.1475699  -0.854661\n"," -1.089      -0.08342999  0.62349    -1.67822    -0.2488078  -0.49199998\n","  0.18756002 -1.67098     0.6117872   0.42784432  1.05656     0.91583097\n"," -0.03299999 -0.04422501  0.200326   -0.33737004  0.31068     1.37842\n"," -1.13689    -0.57445    -0.70685995  0.41552    -0.28937     0.54485\n","  1.0492998   0.62732    -0.8105     -1.27723    -0.02612001  0.53963\n"," -0.14065999 -0.738244   -0.30487    -1.18129     0.05651999 -0.993618\n"," -0.911399   -0.09289992  0.535432    0.26259995 -0.63031     0.64473\n","  0.77843     0.15099996]\n","[-2.06898     0.66804904 -1.077512    0.79964995 -0.27109998 -0.26289004\n"," -0.881       0.377503   -0.10869002 -2.47329    -0.23453003 -0.58438\n","  0.10404003 -0.52671003 -0.03030002  0.237764    0.19168997  1.60344\n"," -0.42980003  0.59058     0.59800005 -0.67075     0.45888     1.4538\n"," -1.15642    -1.63534    -1.1248189  -0.20879    -0.00812     0.25545004\n","  1.92044     0.30049008  0.19949001 -0.675167   -0.15230002  0.13278002\n"," -0.29492003 -0.55414    -0.30988902 -0.34549004 -0.72603    -1.20504\n"," -0.45038998  0.51834     0.12448996  0.787596   -1.13398     0.91365004\n"," -0.280479    0.76741004]\n"]}]},{"cell_type":"markdown","source":["---\n","\n","A tensor of rank 3 corresponding to 4 time stamps (hourly), 3 samples (patients), 2 features (HR and BP)\n","\n","---"],"metadata":{"id":"8VPICS8ggvvg"}},{"cell_type":"code","source":["# A rank-3 patient tensor with shape (4, 3, 2)\n","# with meaning for\n","# axis-0 as 4 hourly timestamps,\n","# axis-1 as 3 patients, and\n","# axis-2 as 2 features (HR and BP)\n","T = torch.tensor([[[74., 128], [79, 116], [71, 116]],\n","                 [[78, 118], [82, 124], [72, 128]],\n","                 [[84, 138], [84, 130], [74, 120]],\n","                 [[82, 126], [76, 156], [82, 132]]])\n","print(T)\n","print(T.shape)"],"metadata":{"id":"qn6KT_pBgwUe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019359,"user_tz":-330,"elapsed":33,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"a9ccdbdf-4d7b-45aa-c012-5b9f91cf2a11"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 74., 128.],\n","         [ 79., 116.],\n","         [ 71., 116.]],\n","\n","        [[ 78., 118.],\n","         [ 82., 124.],\n","         [ 72., 128.]],\n","\n","        [[ 84., 138.],\n","         [ 84., 130.],\n","         [ 74., 120.]],\n","\n","        [[ 82., 126.],\n","         [ 76., 156.],\n","         [ 82., 132.]]])\n","torch.Size([4, 3, 2])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","**Accessing elements of a tensor**\n","\n","---"],"metadata":{"id":"JV0fpSojg2EZ"}},{"cell_type":"code","source":["## Accessing elements of a tensor\n","# Rank-3 tensor T has axes order (timestamps, patients, features)\n","\n","# Element of T at postion 3 w.r.t. axis-0, position 2 w.r.t. axis-1,\n","# position-1 w.r.t axis-2\n","print(T[3, 2, 1]) # 3rd timestamp, 2nd patient, 1st feature (BP)\n","\n","print(T[0]) # element-0 of object T which is also the info for all patients at admission time 9AM\n","\n","print(T[3, 2]) # patient-2 info at 12PM"],"metadata":{"id":"1GbZuDYqg22n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019359,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"ed079cd5-5b6f-4967-ef3e-d1f1c4782fa3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(132.)\n","tensor([[ 74., 128.],\n","        [ 79., 116.],\n","        [ 71., 116.]])\n","tensor([ 82., 132.])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","**Exercise**: interpret $\\texttt{T[:, -1, :]}$\n","\n","---"],"metadata":{"id":"0o6kEXfCpDzo"}},{"cell_type":"code","source":["T[:, -1, :] # information at the last timestamp for all patients"],"metadata":{"id":"X6lEPZEWo6wo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019359,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"c10b022f-3de7-4e89-92ac-ac03f0adc914"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 71., 116.],\n","        [ 72., 128.],\n","        [ 74., 120.],\n","        [ 82., 132.]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["---\n","\n","$l_2$ norm or the geometric length of a vector denoted as $\\lVert \\mathbf{a}\\rVert$ tells us how long a vector is. In 2-dimensions, $$\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2}$$ and in $n$-dimensions, $$\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}.$$\n","\n","![vector norm](https://1drv.ms/i/c/37720f927b6ddc34/IQT817WmpQjlRqZ1R0d5Cfv6AUW6c4robL-gk06i9wmCaFU?width=500)\n","\n","---"],"metadata":{"id":"gc9EJuZQhD9i"}},{"cell_type":"code","source":["## l2 norm of a vector\n","x = torch.tensor([76., 124])\n","print(x)\n","print(torch.norm(x)) # sqrt(76^2+124^2)"],"metadata":{"id":"OM65UP4_hEso","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019360,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"9682d33e-c53b-41cd-f74c-f32e6a0e1303"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 76., 124.])\n","tensor(145.4373)\n"]}]},{"cell_type":"markdown","source":["\n","---\n","\n","**Dot Product of Vectors**\n","\n","A scalar resulting from an elementwise multiplication and addition: $$\\mathbf{a}{\\color{cyan}\\cdot}\\mathbf{b} = {\\color{red}{a_1b_1}}+{\\color{green}{a_2b_2}}+\\cdots+{\\color{magenta}{a_nb_n}}$$\n","\n","The <font color=\"cyan\">dot</font> ${\\color{cyan}\\cdot}$ represents the computation of the dot product.\n","\n","\n","---"],"metadata":{"id":"SRbanrUmwLX7"}},{"cell_type":"code","source":["## Dot product of vectors\n","a = torch.tensor([1., 2, 3])\n","b = torch.tensor([4., 5, 6])\n","print(torch.dot(a, b)) # elementwise product followed by a summation"],"metadata":{"id":"s91XY1JZwU2w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019360,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"d1a33b16-4364-44cd-8e9c-a065ef967617"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(32.)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","The dot product is a measure of similarity between vectors (or, how aligned they are geometrically).\n","\n","![dot product](https://1drv.ms/i/c/37720f927b6ddc34/IQTbcGSjdbhSTJ7J39d5BCWAAWS6-y5U6J87vHuDWeAqGwM?width=6000)\n","---"],"metadata":{"id":"2-b90m-QXyFp"}},{"cell_type":"code","source":["a = torch.tensor([1.0, 2.0])\n","b = torch.tensor([2.0, 4.0])  # b is exactly aligned with a\n","c = torch.tensor([-2.0, 1.0]) # c is perpendicular or orthogonal to a\n","d = torch.tensor([-1.0, -2.0])  # d is anti-aligned with a\n","print(torch.dot(a, b))\n","print(torch.dot(a, c))\n","print(torch.dot(a, d))"],"metadata":{"id":"3GxZ95uXXz3P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019360,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"edbde9fd-ef15-46b2-88e8-d7424663f5cb"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.)\n","tensor(0.)\n","tensor(-5.)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Cauchy-Schwarz inequality $-1\\leq\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\leq1.$\n","\n","This is a normalized measure of similarity (or extent of alignment) between vectors.\n","\n","Angle between vectors $\\mathbf{x}$ and $\\mathbf{y} = \\cos^{-1}\\left(\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\right).$\n","\n","![angle](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=400)\n","\n","\n","---"],"metadata":{"id":"U6CS4_8byCs8"}},{"cell_type":"code","source":["x = torch.tensor([1.0, 2.0])\n","y = torch.tensor([2.0, 1.0])\n","print(torch.dot(x, y) / (torch.norm(x) * torch.norm(y))) # normalized similarity measure\n","print(torch.acos(torch.dot(x, y) / (torch.norm(x) * torch.norm(y)))) # angle in radians\n","print((180/torch.pi)*torch.acos(torch.dot(x, y) / (torch.norm(x) * torch.norm(y)))) # angle in degrees"],"metadata":{"id":"q4UhBnPUx7TV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019360,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"6bd5b67e-4473-4c27-c215-6daa889b269f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.8000)\n","tensor(0.6435)\n","tensor(36.8699)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Application of the Cauchy-Schwarz inequality: is \"Cricket without Tendulkar\" same as \"Football without Messi\"?\n","\n","---"],"metadata":{"id":"1bnmEkg3Tctx"}},{"cell_type":"code","source":["a = model['cricket'] - model['tendulkar']\n","b = model['football'] - model['messi']\n","print(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))) # normalized similarity\n","print((180/np.pi)*np.arccos(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)))) # angular difference in degrees\n","print(np.linalg.norm(a-b)) # linear difference"],"metadata":{"id":"KrmCknO5TkNZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019360,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"fd426bdf-eea2-4f47-d986-86d74a967933"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7371284\n","42.51263077162803\n","4.2349043\n"]}]},{"cell_type":"markdown","source":["\n","---\n","\n","**Hadamard Product of Vectors**\n","\n","A vector resulting from an elementwise multiplication: $$\\mathbf{a}{\\color{cyan}\\otimes}\\mathbf{b} = \\begin{bmatrix}{\\color{red}{a_1\\times b_1}}\\\\{\\color{green}{a_2\\times b_2}}\\\\\\vdots\\\\{\\color{magenta}{a_n\\times b_n}}\\end{bmatrix}.$$\n","\n","The <font color=\"cyan\">$\\otimes$</font> represents the computation of the Hadamard product.\n","\n","---"],"metadata":{"id":"ayzM_0_synRF"}},{"cell_type":"code","source":["## Hadamard product\n","a = torch.tensor([1.0, 2.0, 3.0])\n","b = torch.tensor([4.0, 5.0, 6.0])\n","\n","# Element-wise multiplication (Hadamard product)\n","print(a * b)  # Using the * operator\n","print(torch.mul(a, b))  # Using torch.mul function"],"metadata":{"id":"UPojS0rIzR8p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019360,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"6acc537d-5290-484a-cc38-2a673e43a3c8"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 4., 10., 18.])\n","tensor([ 4., 10., 18.])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","A matrix-vector product is simply a sequence of dot products of the rows of matrix (seen as vectors) with the vector\n","\n","![matrvec product](https://1drv.ms/i/c/37720f927b6ddc34/IQQ1cQ8fZdFmS4cnGkBlsZbAAaL2zMtzWdjHe-HCMt4UTA0?width=700)\n","\n","---"],"metadata":{"id":"oruyV_EjhqCR"}},{"cell_type":"code","source":["## Matrix-vector product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","x = torch.tensor([4.0, 2.0, -2.0])\n","\n","# Matrix-vector multiplication\n","print(A)\n","print(x)\n","print(torch.matmul(A, x))"],"metadata":{"id":"A_IScSWzhpi7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019360,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"c875fdb6-69f1-4039-fb20-9a2aba35acce"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.,  2.,  4.],\n","        [ 2., -1.,  3.]])\n","tensor([ 4.,  2., -2.])\n","tensor([0., 0.])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Here we create a simple sentence in English and tokenize it\n","\n","---"],"metadata":{"id":"uTnGSJ3vT4EN"}},{"cell_type":"code","source":["sentence = 'i swam quickly across the river to get to the other bank'\n","nltk.download('punkt_tab')\n","tokens = word_tokenize(sentence)\n","print(len(tokens))\n","print(tokens)"],"metadata":{"id":"pQ73kkevT5L3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019913,"user_tz":-330,"elapsed":558,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"d718a838-f6bd-44e8-a5ec-56b313f7f4fe"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["12\n","['i', 'swam', 'quickly', 'across', 'the', 'river', 'to', 'get', 'to', 'the', 'other', 'bank']\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Generate the word embeddings for the tokens and store them in a matrix $\\mathbf{X}$ such that each row of the matrix corresponds to a token.\n","\n","---"],"metadata":{"id":"M40pqI8UUbX4"}},{"cell_type":"code","source":["X_word = torch.tensor(model[tokens])\n","np.set_printoptions(precision=3, suppress=True)\n","print(X_word)\n","print(X_word.shape)\n","print(X_word[1]) # embedding vector for the word \"swam\""],"metadata":{"id":"1mKKVRyxUh5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019913,"user_tz":-330,"elapsed":32,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"93ac5360-fb65-4400-ec15-8e72160b8e16"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n","         -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04,\n","         -1.5043e-01,  8.3770e-01, -1.0797e+00, -5.1460e-01,  1.3188e+00,\n","          6.2007e-01,  1.3779e-01,  4.7108e-01, -7.2874e-02, -7.2675e-01,\n","         -7.4116e-01,  7.5263e-01,  8.8180e-01,  2.9561e-01,  1.3548e+00,\n","         -2.5701e+00, -1.3523e+00,  4.5880e-01,  1.0068e+00, -1.1856e+00,\n","          3.4737e+00,  7.7898e-01, -7.2929e-01,  2.5102e-01, -2.6156e-01,\n","         -3.4684e-01,  5.5841e-01,  7.5098e-01,  4.9830e-01, -2.6823e-01,\n","         -2.7443e-03, -1.8298e-02, -2.8096e-01,  5.5318e-01,  3.7706e-02,\n","          1.8555e-01, -1.5025e-01, -5.7512e-01, -2.6671e-01,  9.2121e-01],\n","        [-3.5303e-01,  3.6953e-01,  4.7266e-01,  1.3832e-02, -1.6484e-01,\n","         -5.2687e-01, -7.3986e-01,  1.2058e+00,  1.1147e+00, -4.6772e-01,\n","          1.7928e-01, -5.9239e-01,  2.5257e-01,  4.3449e-01,  6.7023e-01,\n","         -2.8594e-01,  7.3105e-01,  3.1828e-01, -1.5825e+00, -3.5711e-01,\n","         -1.2347e-01,  1.3207e+00,  9.0049e-01,  1.2197e-01,  1.4916e+00,\n","         -5.4105e-01,  1.2264e+00,  3.3936e-01, -3.3048e-01, -3.5457e-01,\n","         -9.6055e-02,  5.0327e-02,  1.2475e-01, -2.2816e-02, -3.5395e-01,\n","         -4.3186e-01,  3.7120e-02,  7.1439e-01, -1.5775e-02, -4.6025e-01,\n","         -3.8091e-01, -6.5412e-01,  1.2373e+00,  7.7759e-01,  6.3607e-02,\n","         -1.4376e+00,  2.4166e-01, -1.5705e+00, -1.0645e-01, -8.2711e-01],\n","        [ 5.6036e-01, -1.1257e+00,  5.5507e-01, -7.1007e-01,  5.9383e-02,\n","         -2.9997e-01, -7.8756e-01,  7.0269e-01, -2.8640e-01, -1.5299e-01,\n","          3.6951e-01,  2.8308e-01, -6.1024e-01,  5.8911e-03,  2.2412e-01,\n","          6.4999e-01, -2.3358e-01, -4.7678e-01, -1.2418e-01, -7.5310e-01,\n","          2.6407e-01, -4.9331e-02,  5.9658e-01, -2.3219e-01,  5.5754e-01,\n","         -1.5649e+00,  2.1960e-01,  2.6784e-01,  7.2161e-01, -1.3073e-01,\n","          3.0699e+00,  2.1293e-01, -1.4069e-01, -8.7782e-01, -3.6846e-01,\n","          1.1815e-01, -2.7351e-01,  5.1589e-01, -4.3990e-06, -5.4707e-01,\n","         -2.6419e-01, -2.3358e-01, -2.7178e-01, -1.6913e-01,  7.4022e-02,\n","          1.0568e-01,  8.5594e-02, -5.5750e-01, -2.7034e-01, -2.5920e-01],\n","        [ 5.2360e-01, -2.0293e-02,  2.6881e-01, -4.8425e-01, -5.4396e-01,\n","         -4.6181e-01, -9.0864e-01, -3.2993e-01,  6.2731e-01, -6.8066e-01,\n","         -5.4416e-01, -1.0720e+00,  3.9323e-02,  2.9368e-02, -4.9019e-01,\n","         -5.9847e-02,  2.3170e-01, -1.7236e-01, -6.2349e-01, -6.9779e-01,\n","          4.8163e-01,  2.1039e-01,  3.0509e-01,  5.0297e-01,  1.3997e-01,\n","         -1.2732e+00,  8.5410e-02,  7.0401e-01,  2.0331e-01, -6.5306e-01,\n","          3.6790e+00,  5.5571e-01,  5.1758e-01, -4.6839e-01, -6.0765e-01,\n","          8.2281e-02, -9.4349e-01, -3.8319e-01, -3.8270e-01,  7.0752e-01,\n","         -5.6429e-01,  4.8173e-01,  3.8864e-01,  3.8322e-02, -2.1097e-01,\n","          1.4094e-01,  1.4637e-01, -9.1432e-01, -6.1570e-01, -1.3112e+00],\n","        [ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n","         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n","          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n","          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n","         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n","         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n","          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n","          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n","         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n","          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n","        [ 7.3109e-01,  1.0242e+00, -2.6714e-01, -3.3449e-01, -1.4873e+00,\n","         -3.7840e-01, -1.2873e+00,  7.1923e-01,  1.1322e+00, -9.0059e-01,\n","         -1.2594e-01,  7.2664e-01,  7.2766e-01, -1.0212e+00, -1.6100e+00,\n","         -2.3452e-01,  1.2707e+00, -6.4512e-02, -6.8331e-01,  2.6957e-01,\n","         -2.1492e-01, -1.0354e+00,  7.7650e-01,  1.5299e-04,  6.2976e-01,\n","         -1.3818e+00, -1.9453e-01,  1.3069e+00, -3.0326e-01, -1.4769e+00,\n","          2.9696e+00, -5.6795e-01,  3.3913e-01,  3.8007e-01,  1.8625e-03,\n","         -2.8222e-01, -8.7329e-01, -7.9432e-01,  1.3823e+00,  7.9638e-01,\n","         -7.1245e-01, -2.8103e-01,  5.6754e-02, -9.7843e-01, -1.1465e+00,\n","         -6.2487e-01,  2.6281e-01, -1.4434e+00,  4.0201e-01, -1.3707e+00],\n","        [ 6.8047e-01, -3.9263e-02,  3.0186e-01, -1.7792e-01,  4.2962e-01,\n","          3.2246e-02, -4.1376e-01,  1.3228e-01, -2.9847e-01, -8.5253e-02,\n","          1.7118e-01,  2.2419e-01, -1.0046e-01, -4.3653e-01,  3.3418e-01,\n","          6.7846e-01,  5.7204e-02, -3.4448e-01, -4.2785e-01, -4.3275e-01,\n","          5.5963e-01,  1.0032e-01,  1.8677e-01, -2.6854e-01,  3.7334e-02,\n","         -2.0932e+00,  2.2171e-01, -3.9868e-01,  2.0912e-01, -5.5725e-01,\n","          3.8826e+00,  4.7466e-01, -9.5658e-01, -3.7788e-01,  2.0869e-01,\n","         -3.2752e-01,  1.2751e-01,  8.8359e-02,  1.6351e-01, -2.1634e-01,\n","         -9.4375e-02,  1.8324e-02,  2.1048e-01, -3.0880e-02, -1.9722e-01,\n","          8.2279e-02, -9.4340e-02, -7.3297e-02, -6.4699e-02, -2.6044e-01],\n","        [ 1.5910e-01, -2.1428e-01,  6.3099e-01, -5.9950e-01,  3.1248e-01,\n","         -1.6615e-01, -9.0548e-01,  4.5115e-01,  5.1568e-02,  2.5910e-01,\n","         -3.2882e-01,  4.8155e-01, -3.4982e-01,  1.2905e-01,  1.0758e+00,\n","          4.8690e-01,  5.3420e-01,  5.9762e-02,  2.1660e-01, -1.1059e+00,\n","         -2.5591e-01,  5.7462e-01,  5.4562e-01,  3.1043e-01,  3.7765e-01,\n","         -2.0337e+00, -2.2496e-01,  1.8447e-01,  8.2587e-01, -1.1991e+00,\n","          3.6042e+00,  1.1605e+00, -5.9787e-01,  1.3000e-01,  1.5678e-01,\n","          1.3166e-01,  1.8510e-01,  3.6308e-01,  5.7538e-01, -8.9593e-01,\n","         -3.6366e-01,  2.8397e-01,  4.8614e-02,  7.8780e-01, -8.7311e-02,\n","         -2.3394e-01, -1.4237e-01,  2.1215e-02, -1.4219e-01,  6.6955e-01],\n","        [ 6.8047e-01, -3.9263e-02,  3.0186e-01, -1.7792e-01,  4.2962e-01,\n","          3.2246e-02, -4.1376e-01,  1.3228e-01, -2.9847e-01, -8.5253e-02,\n","          1.7118e-01,  2.2419e-01, -1.0046e-01, -4.3653e-01,  3.3418e-01,\n","          6.7846e-01,  5.7204e-02, -3.4448e-01, -4.2785e-01, -4.3275e-01,\n","          5.5963e-01,  1.0032e-01,  1.8677e-01, -2.6854e-01,  3.7334e-02,\n","         -2.0932e+00,  2.2171e-01, -3.9868e-01,  2.0912e-01, -5.5725e-01,\n","          3.8826e+00,  4.7466e-01, -9.5658e-01, -3.7788e-01,  2.0869e-01,\n","         -3.2752e-01,  1.2751e-01,  8.8359e-02,  1.6351e-01, -2.1634e-01,\n","         -9.4375e-02,  1.8324e-02,  2.1048e-01, -3.0880e-02, -1.9722e-01,\n","          8.2279e-02, -9.4340e-02, -7.3297e-02, -6.4699e-02, -2.6044e-01],\n","        [ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n","         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n","          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n","          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n","         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n","         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n","          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n","          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n","         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n","          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n","        [ 6.4756e-01,  1.6000e-01,  2.9191e-02,  3.5118e-01,  8.9119e-02,\n","          6.1115e-01, -6.6362e-01, -5.1724e-01, -4.6521e-01, -8.8450e-02,\n","          5.0200e-02,  2.6329e-01,  1.2407e-01,  4.3832e-02,  1.7283e-01,\n","          1.3170e-02,  1.4168e-01, -1.5827e-01, -1.0427e-01, -9.3070e-01,\n","          2.1646e-01, -1.0753e-01,  6.2087e-01,  3.6761e-01, -4.8144e-01,\n","         -1.2800e+00, -5.5152e-01, -7.2023e-01, -1.7097e-01, -4.7993e-01,\n","          4.0165e+00,  4.7054e-01,  9.3614e-02, -8.6341e-01,  5.0881e-01,\n","          3.3353e-01, -3.5962e-01, -1.6648e-01, -3.1803e-01,  4.9003e-01,\n","         -3.6697e-01,  3.2051e-01,  7.0932e-01,  6.2878e-01,  7.0128e-01,\n","          1.3020e-01, -7.3769e-01,  1.0325e-01, -3.0964e-01, -4.4213e-01],\n","        [ 6.6488e-01, -1.1391e-01,  6.7844e-01,  1.7951e-01,  6.8280e-01,\n","         -4.7787e-01, -3.0761e-01,  1.7489e-01, -7.0512e-01, -5.5022e-01,\n","          1.5140e-01,  1.0214e-01, -4.5063e-01, -3.3069e-01,  5.6133e-02,\n","          1.2271e+00,  5.5607e-01, -6.8297e-01,  3.7364e-02,  7.0266e-01,\n","          1.9093e+00, -6.1483e-01, -8.3329e-01, -3.0230e-01, -1.1118e+00,\n","         -1.5500e+00,  2.6040e-01,  2.2957e-01, -1.0375e+00, -3.1789e-01,\n","          3.5091e+00, -2.5871e-01,  1.0151e+00,  6.5927e-01, -1.8231e-01,\n","         -7.5859e-01, -3.0927e-01, -9.1678e-01,  1.0633e+00, -6.6761e-01,\n","         -3.7464e-01, -2.9143e-01,  6.5606e-01, -4.4642e-01, -7.5495e-02,\n","         -1.0552e+00, -6.0501e-01,  7.3582e-01,  1.0139e+00, -2.7749e-01]])\n","torch.Size([12, 50])\n","tensor([-0.3530,  0.3695,  0.4727,  0.0138, -0.1648, -0.5269, -0.7399,  1.2058,\n","         1.1147, -0.4677,  0.1793, -0.5924,  0.2526,  0.4345,  0.6702, -0.2859,\n","         0.7311,  0.3183, -1.5825, -0.3571, -0.1235,  1.3207,  0.9005,  0.1220,\n","         1.4916, -0.5411,  1.2264,  0.3394, -0.3305, -0.3546, -0.0961,  0.0503,\n","         0.1248, -0.0228, -0.3539, -0.4319,  0.0371,  0.7144, -0.0158, -0.4602,\n","        -0.3809, -0.6541,  1.2373,  0.7776,  0.0636, -1.4376,  0.2417, -1.5705,\n","        -0.1064, -0.8271])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","A matrix-matrix product is simply a sequence of matrix-vector products.\n","\n","![matmatprod](https://1drv.ms/i/c/37720f927b6ddc34/IQQ-B3z7tbWHQqBrW9k2ElDVAUc5fWzM24txLkgBK7f8Yac?width=550)\n","\n","\n","---"],"metadata":{"id":"0Z0pZQisxtY-"}},{"cell_type":"code","source":["## Matrix-matrix product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","B = torch.tensor([[4.0, -1.0],\n","                  [2.0, 0.0],\n","                  [-2.0, 3.0]])\n","torch.matmul(A, B)"],"metadata":{"id":"YSg1brJ9yKnM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019913,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"81cface4-8419-4740-efbf-c0c4c60c008b"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0., 11.],\n","        [ 0.,  7.]])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["---\n","\n","The similarity between each pair of words represented in the word embeddings matrix $\\mathbf{X}_\\mathrm{word}$ is the matrix-matrix product $\\mathbf{X}_\\mathrm{word}\\mathbf{X}_\\mathrm{word}^\\mathrm{T}.$\n","\n","---"],"metadata":{"id":"mVoJRc6kUtI2"}},{"cell_type":"code","source":["S = torch.matmul(X_word, X_word.T)\n","print(S)\n","print(S.shape)"],"metadata":{"id":"ms9Qg5AoVJy_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019913,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"6b0a849e-f991-4946-af37-2490a28d7a95"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[39.0197,  4.5574, 20.2995, 14.5481, 19.4420, 12.0143, 22.6571, 29.8528,\n","         22.6571, 19.4420, 17.8858, 11.2500],\n","        [ 4.5574, 24.7064,  4.2668,  7.2292,  0.9357,  8.8437,  2.3160,  5.4718,\n","          2.3160,  0.9357, -0.6112, -1.5239],\n","        [20.2995,  4.2668, 21.2841, 16.5796, 16.1261, 12.0684, 18.8431, 19.4523,\n","         18.8431, 16.1261, 15.2439, 12.9733],\n","        [14.5481,  7.2292, 16.5796, 28.8804, 19.3706, 22.9543, 17.9298, 16.6827,\n","         17.9298, 19.3706, 19.6938, 14.1935],\n","        [19.4420,  0.9357, 16.1261, 19.3706, 24.6793, 18.2549, 21.1141, 17.5502,\n","         21.1141, 24.6793, 19.8692, 18.0800],\n","        [12.0143,  8.8437, 12.0684, 22.9543, 18.2549, 44.2210, 14.7193, 12.0669,\n","         14.7193, 18.2549, 12.5996, 16.6162],\n","        [22.6571,  2.3160, 18.8431, 17.9298, 21.1141, 14.7193, 24.5706, 22.2478,\n","         24.5706, 21.1141, 20.6623, 19.2235],\n","        [29.8528,  5.4718, 19.4523, 16.6827, 17.5502, 12.0669, 22.2478, 30.1930,\n","         22.2478, 17.5502, 19.3905, 14.3078],\n","        [22.6571,  2.3160, 18.8431, 17.9298, 21.1141, 14.7193, 24.5706, 22.2478,\n","         24.5706, 21.1141, 20.6623, 19.2235],\n","        [19.4420,  0.9357, 16.1261, 19.3706, 24.6793, 18.2549, 21.1141, 17.5502,\n","         21.1141, 24.6793, 19.8692, 18.0800],\n","        [17.8858, -0.6112, 15.2439, 19.6938, 19.8692, 12.5996, 20.6623, 19.3905,\n","         20.6623, 19.8692, 26.9230, 15.7850],\n","        [11.2500, -1.5239, 12.9733, 14.1935, 18.0800, 16.6162, 19.2235, 14.3078,\n","         19.2235, 18.0800, 15.7850, 36.3920]])\n","torch.Size([12, 12])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Matrix-matrix product using q patient data matrix and a weights matrix:\n","\n","![Patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hscharGu916tjWNzZQ?embed=1&width=660)\n","\n","---"],"metadata":{"id":"QH_sW6XW0MDT"}},{"cell_type":"code","source":["# Patients data matrix\n","X = torch.tensor([[72, 120, 36.5],\n","                  [85, 130, 37.0],\n","                  [68, 110, 38.5],\n","                  [90, 140, 38.0]])\n","print(X)\n","\n","# Weights matrix\n","W = torch.tensor([[0.5, 0.3, -0.6],\n","                  [0.9, 0.3, -0.25],\n","                  [-1.5, 0.4, 0.1]])\n","print(W)\n","\n","# Raw scores matrix (Matrix-matrix multiplication)\n","Z = torch.matmul(X, W) # PyTorch matmul() also does matrix-matrix multiplication\n","print(Z)\n","\n","# The raw scores are also referred to as the logits"],"metadata":{"id":"s0RFtdhxhkvZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019913,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"c781d4a3-01c6-4341-8a32-c9817c0ae76a"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 72.0000, 120.0000,  36.5000],\n","        [ 85.0000, 130.0000,  37.0000],\n","        [ 68.0000, 110.0000,  38.5000],\n","        [ 90.0000, 140.0000,  38.0000]])\n","tensor([[ 0.5000,  0.3000, -0.6000],\n","        [ 0.9000,  0.3000, -0.2500],\n","        [-1.5000,  0.4000,  0.1000]])\n","tensor([[ 89.2500,  72.2000, -69.5500],\n","        [104.0000,  79.3000, -79.8000],\n","        [ 75.2500,  68.8000, -64.4500],\n","        [114.0000,  84.2000, -85.2000]])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","The softmax function\n","\n","![softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hscmdol7J2G4GDo5WQ?embed=1&width=660)\n","\n","---"],"metadata":{"id":"sOnZvS5Vjjrd"}},{"cell_type":"code","source":["## In-built softmax function in PyTorch (dim = 1 corresponds to applying row-by-row)\n","## applied to the word embeddings similarity matrix\n","S_softmax = torch.nn.functional.softmax(S, dim = 1)\n","print(S_softmax)"],"metadata":{"id":"TDkPV0qXVVyp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019913,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"1b601115-f5d4-47cd-fea8-7de32a60777c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[9.9990e-01, 1.0794e-15, 7.4108e-09, 2.3555e-11, 3.1439e-09, 1.8692e-12,\n","         7.8300e-08, 1.0443e-04, 7.8300e-08, 3.1439e-09, 6.6318e-10, 8.7044e-13],\n","        [1.7758e-09, 1.0000e+00, 1.3280e-09, 2.5688e-08, 4.7482e-11, 1.2910e-07,\n","         1.8879e-10, 4.4313e-09, 1.8879e-10, 4.7482e-11, 1.0109e-11, 4.0579e-12],\n","        [2.1580e-01, 2.3505e-08, 5.7765e-01, 5.2304e-03, 3.3235e-03, 5.7455e-05,\n","         5.0300e-02, 9.2497e-02, 5.0300e-02, 3.3235e-03, 1.3755e-03, 1.4202e-04],\n","        [5.9469e-07, 3.9420e-10, 4.5349e-06, 9.9704e-01, 7.3902e-05, 2.6611e-03,\n","         1.7496e-05, 5.0274e-06, 1.7496e-05, 7.3902e-05, 1.0210e-04, 4.1714e-07],\n","        [2.5563e-03, 2.3467e-11, 9.2803e-05, 2.3801e-03, 4.8101e-01, 7.7993e-04,\n","         1.3608e-02, 3.8551e-04, 1.3608e-02, 4.8101e-01, 3.9187e-03, 6.5480e-04],\n","        [1.0299e-14, 4.3238e-16, 1.0872e-14, 5.8078e-10, 5.2853e-12, 1.0000e+00,\n","         1.5403e-13, 1.0855e-14, 1.5403e-13, 5.2853e-12, 1.8493e-14, 1.0266e-12],\n","        [6.3112e-02, 9.2493e-11, 1.3923e-03, 5.5856e-04, 1.3490e-02, 2.2532e-05,\n","         4.2770e-01, 4.1917e-02, 4.2770e-01, 1.3490e-02, 8.5862e-03, 2.0366e-03],\n","        [4.1558e-01, 1.0719e-11, 1.2640e-05, 7.9241e-07, 1.8868e-06, 7.8399e-09,\n","         2.0694e-04, 5.8397e-01, 2.0694e-04, 1.8868e-06, 1.1883e-05, 7.3716e-08],\n","        [6.3112e-02, 9.2493e-11, 1.3923e-03, 5.5856e-04, 1.3490e-02, 2.2532e-05,\n","         4.2770e-01, 4.1917e-02, 4.2770e-01, 1.3490e-02, 8.5862e-03, 2.0366e-03],\n","        [2.5563e-03, 2.3467e-11, 9.2803e-05, 2.3801e-03, 4.8101e-01, 7.7993e-04,\n","         1.3608e-02, 3.8551e-04, 1.3608e-02, 4.8101e-01, 3.9187e-03, 6.5480e-04],\n","        [1.1808e-04, 1.0940e-12, 8.4105e-06, 7.2006e-04, 8.5813e-04, 5.9757e-07,\n","         1.8967e-03, 5.3167e-04, 1.8967e-03, 8.5813e-04, 9.9310e-01, 1.4448e-05],\n","        [1.2049e-11, 3.4143e-17, 6.7511e-11, 2.2871e-10, 1.1148e-08, 2.5790e-09,\n","         3.4977e-08, 2.5642e-10, 3.4977e-08, 1.1148e-08, 1.1233e-09, 1.0000e+00]])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Transform the word embeddings using the softmax-normalized similarity matrix.\n","\n","---"],"metadata":{"id":"_OVqwridV5T_"}},{"cell_type":"code","source":["X_word = torch.tensor(model[tokens])\n","Y = torch.matmul(S_softmax, X_word)\n","print(Y)"],"metadata":{"id":"SF731yR3V3E7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019914,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"cde262df-1c78-461c-a4b1-16c75a669bb5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.1891e-01,  1.5251e-01, -8.1998e-02, -7.4142e-01,  7.5912e-01,\n","         -4.8325e-01, -3.1015e-01,  5.1475e-01, -9.8697e-01,  6.4455e-04,\n","         -1.5045e-01,  8.3766e-01, -1.0796e+00, -5.1453e-01,  1.3188e+00,\n","          6.2006e-01,  1.3783e-01,  4.7104e-01, -7.2844e-02, -7.2679e-01,\n","         -7.4111e-01,  7.5261e-01,  8.8176e-01,  2.9561e-01,  1.3547e+00,\n","         -2.5700e+00, -1.3522e+00,  4.5877e-01,  1.0068e+00, -1.1856e+00,\n","          3.4737e+00,  7.7902e-01, -7.2928e-01,  2.5101e-01, -2.6152e-01,\n","         -3.4679e-01,  5.5837e-01,  7.5094e-01,  4.9831e-01, -2.6830e-01,\n","         -2.7820e-03, -1.8266e-02, -2.8093e-01,  5.5320e-01,  3.7693e-02,\n","          1.8551e-01, -1.5025e-01, -5.7506e-01, -2.6670e-01,  9.2118e-01],\n","        [-3.5303e-01,  3.6953e-01,  4.7266e-01,  1.3832e-02, -1.6484e-01,\n","         -5.2687e-01, -7.3986e-01,  1.2058e+00,  1.1147e+00, -4.6772e-01,\n","          1.7928e-01, -5.9239e-01,  2.5257e-01,  4.3449e-01,  6.7023e-01,\n","         -2.8594e-01,  7.3105e-01,  3.1828e-01, -1.5825e+00, -3.5711e-01,\n","         -1.2347e-01,  1.3207e+00,  9.0049e-01,  1.2197e-01,  1.4916e+00,\n","         -5.4105e-01,  1.2264e+00,  3.3936e-01, -3.3048e-01, -3.5457e-01,\n","         -9.6054e-02,  5.0327e-02,  1.2475e-01, -2.2816e-02, -3.5395e-01,\n","         -4.3186e-01,  3.7120e-02,  7.1439e-01, -1.5775e-02, -4.6025e-01,\n","         -3.8091e-01, -6.5412e-01,  1.2373e+00,  7.7759e-01,  6.3607e-02,\n","         -1.4376e+00,  2.4166e-01, -1.5705e+00, -1.0645e-01, -8.2711e-01],\n","        [ 4.3907e-01, -6.3930e-01,  3.9044e-01, -6.4476e-01,  2.6984e-01,\n","         -2.9165e-01, -6.5632e-01,  5.6847e-01, -4.0110e-01, -8.1029e-02,\n","          1.6688e-01,  4.0522e-01, -6.3131e-01, -1.3854e-01,  5.4471e-01,\n","          6.2251e-01, -4.7790e-02, -2.0495e-01, -1.1952e-01, -7.4331e-01,\n","          2.8192e-02,  1.9576e-01,  6.0551e-01, -6.7079e-02,  6.5179e-01,\n","         -1.8785e+00, -1.6886e-01,  2.3414e-01,  7.2940e-01, -5.0384e-01,\n","          3.2990e+00,  4.4844e-01, -3.9066e-01, -4.8452e-01, -2.3629e-01,\n","         -2.6554e-02, -1.1883e-02,  4.9907e-01,  1.7507e-01, -4.7457e-01,\n","         -2.0187e-01, -1.0891e-01, -1.9116e-01,  9.2081e-02,  1.9823e-02,\n","          8.9705e-02, -5.9408e-03, -4.5740e-01, -2.3762e-01,  7.1990e-02],\n","        [ 5.2415e-01, -1.7462e-02,  2.6726e-01, -4.8367e-01, -5.4623e-01,\n","         -4.6140e-01, -9.0954e-01, -3.2711e-01,  6.2841e-01, -6.8115e-01,\n","         -5.4283e-01, -1.0669e+00,  4.1065e-02,  2.6575e-02, -4.9299e-01,\n","         -6.0261e-02,  2.3443e-01, -1.7207e-01, -6.2361e-01, -6.9515e-01,\n","          4.7967e-01,  2.0696e-01,  3.0631e-01,  5.0148e-01,  1.4116e-01,\n","         -1.2736e+00,  8.4477e-02,  7.0534e-01,  2.0184e-01, -6.5516e-01,\n","          3.6772e+00,  5.5260e-01,  5.1685e-01, -4.6614e-01, -6.0579e-01,\n","          8.1311e-02, -9.4303e-01, -3.8420e-01, -3.7791e-01,  7.0757e-01,\n","         -5.6461e-01,  4.7957e-01,  3.8767e-01,  3.5665e-02, -2.1340e-01,\n","          1.3890e-01,  1.4656e-01, -9.1548e-01, -6.1286e-01, -1.3111e+00],\n","        [ 4.2585e-01,  2.4063e-01, -3.8746e-01,  1.1012e-01,  3.4426e-01,\n","         -4.2530e-02, -4.9645e-01, -1.6882e-01, -1.1197e-02, -6.3692e-01,\n","          2.7094e-01, -1.3449e-01, -5.4045e-01,  1.2709e-01,  2.0455e-03,\n","          3.2044e-02,  1.0272e-01, -1.3278e-01, -8.2639e-01, -1.3595e-01,\n","          1.7693e-04, -3.1641e-01, -1.3848e-01, -2.2654e-01, -1.8164e-01,\n","         -1.8854e+00, -7.3774e-01,  8.5729e-02, -3.9770e-01, -2.1131e-01,\n","          4.0002e+00, -1.6095e-01, -5.2862e-01, -3.1822e-01,  6.0369e-03,\n","         -1.7909e-03,  1.7143e-01, -1.5120e-01,  1.7147e-02, -5.5345e-02,\n","         -2.9369e-01, -1.4898e-01, -3.2519e-01, -4.1542e-02, -4.2970e-01,\n","          1.8302e-01, -3.0532e-03, -1.8304e-01, -1.1499e-01, -7.6656e-01],\n","        [ 7.3109e-01,  1.0242e+00, -2.6714e-01, -3.3449e-01, -1.4873e+00,\n","         -3.7840e-01, -1.2873e+00,  7.1923e-01,  1.1322e+00, -9.0059e-01,\n","         -1.2594e-01,  7.2664e-01,  7.2766e-01, -1.0212e+00, -1.6100e+00,\n","         -2.3452e-01,  1.2707e+00, -6.4512e-02, -6.8331e-01,  2.6957e-01,\n","         -2.1492e-01, -1.0354e+00,  7.7650e-01,  1.5299e-04,  6.2976e-01,\n","         -1.3818e+00, -1.9453e-01,  1.3069e+00, -3.0326e-01, -1.4769e+00,\n","          2.9696e+00, -5.6795e-01,  3.3913e-01,  3.8007e-01,  1.8625e-03,\n","         -2.8222e-01, -8.7329e-01, -7.9432e-01,  1.3823e+00,  7.9638e-01,\n","         -7.1245e-01, -2.8103e-01,  5.6754e-02, -9.7843e-01, -1.1465e+00,\n","         -6.2487e-01,  2.6281e-01, -1.4434e+00,  4.0201e-01, -1.3707e+00],\n","        [ 6.1553e-01, -2.6617e-02,  2.7090e-01, -2.1872e-01,  4.3972e-01,\n","         -7.4917e-03, -4.3282e-01,  1.5646e-01, -3.2092e-01, -8.2234e-02,\n","          1.3161e-01,  2.6312e-01, -1.8442e-01, -3.9681e-01,  4.1553e-01,\n","          6.4369e-01,  8.4955e-02, -2.6939e-01, -3.8564e-01, -4.7367e-01,\n","          4.2713e-01,  1.4627e-01,  2.3875e-01, -2.0178e-01,  1.2257e-01,\n","         -2.1058e+00,  7.0311e-02, -3.0659e-01,  2.6320e-01, -6.1237e-01,\n","          3.8476e+00,  5.0292e-01, -9.0048e-01, -3.1804e-01,  1.7174e-01,\n","         -2.9481e-01,  1.5222e-01,  1.3110e-01,  1.9501e-01, -2.3850e-01,\n","         -1.0882e-01,  2.4269e-02,  1.6224e-01,  4.4542e-02, -1.7609e-01,\n","          7.6534e-02, -1.0343e-01, -1.0201e-01, -8.2547e-02, -1.6326e-01],\n","        [ 1.4263e-01, -6.1765e-02,  3.3451e-01, -6.5830e-01,  4.9816e-01,\n","         -2.9785e-01, -6.5784e-01,  4.7744e-01, -3.8023e-01,  1.5152e-01,\n","         -2.5446e-01,  6.2944e-01, -6.5304e-01, -1.3868e-01,  1.1765e+00,\n","          5.4232e-01,  3.6925e-01,  2.3052e-01,  9.6020e-02, -9.4804e-01,\n","         -4.5722e-01,  6.4838e-01,  6.8518e-01,  3.0402e-01,  7.8358e-01,\n","         -2.2566e+00, -6.9328e-01,  2.9823e-01,  9.0079e-01, -1.1932e+00,\n","          3.5501e+00,  1.0016e+00, -6.5262e-01,  1.8006e-01, -1.7057e-02,\n","         -6.7384e-02,  3.4020e-01,  5.2416e-01,  5.4316e-01, -6.3476e-01,\n","         -2.1356e-01,  1.5824e-01, -8.8281e-02,  6.8994e-01, -3.5392e-02,\n","         -5.9466e-02, -1.4563e-01, -2.2666e-01, -1.9391e-01,  7.7372e-01],\n","        [ 6.1553e-01, -2.6617e-02,  2.7090e-01, -2.1872e-01,  4.3972e-01,\n","         -7.4917e-03, -4.3282e-01,  1.5646e-01, -3.2092e-01, -8.2234e-02,\n","          1.3161e-01,  2.6312e-01, -1.8442e-01, -3.9681e-01,  4.1553e-01,\n","          6.4369e-01,  8.4955e-02, -2.6939e-01, -3.8564e-01, -4.7367e-01,\n","          4.2713e-01,  1.4627e-01,  2.3875e-01, -2.0178e-01,  1.2257e-01,\n","         -2.1058e+00,  7.0311e-02, -3.0659e-01,  2.6320e-01, -6.1237e-01,\n","          3.8476e+00,  5.0292e-01, -9.0048e-01, -3.1804e-01,  1.7174e-01,\n","         -2.9481e-01,  1.5222e-01,  1.3110e-01,  1.9501e-01, -2.3850e-01,\n","         -1.0882e-01,  2.4269e-02,  1.6224e-01,  4.4542e-02, -1.7609e-01,\n","          7.6534e-02, -1.0343e-01, -1.0201e-01, -8.2547e-02, -1.6326e-01],\n","        [ 4.2585e-01,  2.4063e-01, -3.8746e-01,  1.1012e-01,  3.4426e-01,\n","         -4.2530e-02, -4.9645e-01, -1.6882e-01, -1.1197e-02, -6.3692e-01,\n","          2.7094e-01, -1.3449e-01, -5.4045e-01,  1.2709e-01,  2.0455e-03,\n","          3.2044e-02,  1.0272e-01, -1.3278e-01, -8.2639e-01, -1.3595e-01,\n","          1.7693e-04, -3.1641e-01, -1.3848e-01, -2.2654e-01, -1.8164e-01,\n","         -1.8854e+00, -7.3774e-01,  8.5729e-02, -3.9770e-01, -2.1131e-01,\n","          4.0002e+00, -1.6095e-01, -5.2862e-01, -3.1822e-01,  6.0369e-03,\n","         -1.7909e-03,  1.7143e-01, -1.5120e-01,  1.7147e-02, -5.5345e-02,\n","         -2.9369e-01, -1.4898e-01, -3.2519e-01, -4.1542e-02, -4.2970e-01,\n","          1.8302e-01, -3.0532e-03, -1.8304e-01, -1.1499e-01, -7.6656e-01],\n","        [ 6.4688e-01,  1.5905e-01,  2.9960e-02,  3.4753e-01,  9.0600e-02,\n","          6.0649e-01, -6.6265e-01, -5.1340e-01, -4.6278e-01, -8.9652e-02,\n","          5.0401e-02,  2.6166e-01,  1.2158e-01,  4.2149e-02,  1.7326e-01,\n","          1.5985e-02,  1.4157e-01, -1.5875e-01, -1.0697e-01, -9.2730e-01,\n","          2.1721e-01, -1.0644e-01,  6.1763e-01,  3.6421e-01, -4.7785e-01,\n","         -1.2847e+00, -5.4840e-01, -7.1594e-01, -1.6902e-01, -4.8032e-01,\n","          4.0154e+00,  4.6988e-01,  8.8424e-02, -8.5966e-01,  5.0570e-01,\n","          3.3008e-01, -3.5687e-01, -1.6527e-01, -3.1509e-01,  4.8572e-01,\n","         -3.6592e-01,  3.1859e-01,  7.0491e-01,  6.2475e-01,  6.9474e-01,\n","          1.2992e-01, -7.3295e-01,  1.0123e-01, -3.0848e-01, -4.4190e-01],\n","        [ 6.6488e-01, -1.1391e-01,  6.7844e-01,  1.7951e-01,  6.8280e-01,\n","         -4.7787e-01, -3.0761e-01,  1.7489e-01, -7.0512e-01, -5.5022e-01,\n","          1.5140e-01,  1.0214e-01, -4.5063e-01, -3.3069e-01,  5.6133e-02,\n","          1.2271e+00,  5.5607e-01, -6.8297e-01,  3.7364e-02,  7.0266e-01,\n","          1.9093e+00, -6.1483e-01, -8.3329e-01, -3.0230e-01, -1.1118e+00,\n","         -1.5500e+00,  2.6040e-01,  2.2957e-01, -1.0375e+00, -3.1789e-01,\n","          3.5091e+00, -2.5871e-01,  1.0151e+00,  6.5927e-01, -1.8231e-01,\n","         -7.5859e-01, -3.0927e-01, -9.1678e-01,  1.0633e+00, -6.6761e-01,\n","         -3.7464e-01, -2.9143e-01,  6.5606e-01, -4.4642e-01, -7.5495e-02,\n","         -1.0552e+00, -6.0501e-01,  7.3582e-01,  1.0139e+00, -2.7749e-01]])\n"]}]},{"cell_type":"code","source":["## In-built softmax function in PyTorch (dim = 1 corresponds to row-by-row)\n","## applied to the toy patient data matrix\n","softmax_scores = torch.nn.functional.softmax(Z, dim = 1)\n","print(softmax_scores)"],"metadata":{"id":"0EG_bqPgjlV5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651019914,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"7573869d-e98d-4dcf-afca-e993a20d1d34"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000e+00, 3.9380e-08, 0.0000e+00],\n","        [1.0000e+00, 1.8747e-11, 0.0000e+00],\n","        [9.9842e-01, 1.5780e-03, 0.0000e+00],\n","        [1.0000e+00, 1.1429e-13, 0.0000e+00]])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","A toy data matrix with output labels and an initial weights matrix for the softmax classifier:\n","\n","![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n","\n","---"],"metadata":{"id":"6_8cPXFFkiUZ"}},{"cell_type":"code","source":["# Create the data matrix (read from a file typically)\n","X = np.array([[72, 120, 37.3, 104, 32.5],\n","              [85, 130, 37.0, 110, 14],\n","              [68, 110, 38.5, 125, 34],\n","              [90, 140, 38.0, 130, 26],\n","              [84, 132, 38.3, 146, 30],\n","              [78, 128, 37.2, 102, 12]])\n","\n","# Standardize the data matrix\n","sc = StandardScaler()\n","X_S = sc.fit_transform(X)  # fit(), fit_transform(), transform()\n","\n","# Convert to a PyTorch tensor\n","X_S = torch.tensor(X_S, dtype=torch.float32)\n","\n","# Get the number of samples and features\n","num_samples, num_features = X_S.shape\n","\n","# Create the output labels vector (also read from a file typically)\n","y = np.array(['non-diabetic',\n","              'diabetic',\n","              'non-diabetic',\n","              'pre-diabetic',\n","              'diabetic',\n","              'pre-diabetic'])\n","\n","# One-hot encoding of output labels using scikit-learn\n","ohe = OneHotEncoder(sparse_output=False)  # Use `sparse_output=False` for dense array\n","Y = ohe.fit_transform(y.reshape(-1, 1))\n","\n","# Convert to a PyTorch tensor\n","Y = torch.tensor(Y, dtype=torch.float32)\n","\n","# Get the number of labels\n","num_labels = Y.shape[1]\n","\n","# Create the weights matrix\n","W = torch.tensor([[-0.1, 0.5, 0.3],\n","                  [0.9, 0.3, 0.5],\n","                  [-1.5, 0.4, 0.1],\n","                  [0.1, 0.1, -1.0],\n","                  [-1.2, 0.5, -0.8]], dtype=torch.float32)\n","\n","print(X_S)\n","print(Y)\n","print(W)"],"metadata":{"id":"MJ3U-JCukmIG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651029837,"user_tz":-330,"elapsed":478,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"a706ae52-ba79-438b-a654-aa3d28dcc844"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.9799, -0.7019, -0.7238, -0.9871,  0.8920],\n","        [ 0.7186,  0.3509, -1.2449, -0.6050, -1.2374],\n","        [-1.5025, -1.7547,  1.3607,  0.3503,  1.0647],\n","        [ 1.3718,  1.4037,  0.4922,  0.6687,  0.1439],\n","        [ 0.5879,  0.5615,  1.0133,  1.6876,  0.6043],\n","        [-0.1960,  0.1404, -0.8975, -1.1144, -1.4676]])\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]])\n","tensor([[-0.1000,  0.5000,  0.3000],\n","        [ 0.9000,  0.3000,  0.5000],\n","        [-1.5000,  0.4000,  0.1000],\n","        [ 0.1000,  0.1000, -1.0000],\n","        [-1.2000,  0.5000, -0.8000]])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Bias trick to absorb the bias into the weights matrix\n","\n","![bias trick](https://1drv.ms/i/c/37720f927b6ddc34/IQR8NDbhvaddQa3W3F_46q4nATD7WBNgnwGJ7QC6HDL6g14?width=850)\n","\n","---"],"metadata":{"id":"1cQDyu7llDo9"}},{"cell_type":"code","source":["## Bias trick to absorb the bias into the weights matrix\n","# Concatenate a column of ones to X_S (bias term)\n","X_B = torch.cat([X_S, torch.ones((num_samples, 1))], dim=1)\n","\n","# Create the bias vector `b`\n","b = 0.1 * torch.ones((1, num_labels))\n","\n","# Concatenate the weights matrix `W` with the bias vector `b`\n","W_B = torch.cat([W, b], dim=0)\n","\n","print(X_B)\n","print(W_B)"],"metadata":{"id":"DlviiS0tlH7p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651034129,"user_tz":-330,"elapsed":488,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"126ff661-839a-46c0-f535-2488e11abca7"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.9799, -0.7019, -0.7238, -0.9871,  0.8920,  1.0000],\n","        [ 0.7186,  0.3509, -1.2449, -0.6050, -1.2374,  1.0000],\n","        [-1.5025, -1.7547,  1.3607,  0.3503,  1.0647,  1.0000],\n","        [ 1.3718,  1.4037,  0.4922,  0.6687,  0.1439,  1.0000],\n","        [ 0.5879,  0.5615,  1.0133,  1.6876,  0.6043,  1.0000],\n","        [-0.1960,  0.1404, -0.8975, -1.1144, -1.4676,  1.0000]])\n","tensor([[-0.1000,  0.5000,  0.3000],\n","        [ 0.9000,  0.3000,  0.5000],\n","        [-1.5000,  0.4000,  0.1000],\n","        [ 0.1000,  0.1000, -1.0000],\n","        [-1.2000,  0.5000, -0.8000],\n","        [ 0.1000,  0.1000,  0.1000]])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Forward propagation for the toy patient dataset: $$\\textbf{bias-added input }\\mathbf{X}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{raw scores }\\mathbf{Z}=\\mathbf{X}_B\\textbf{W}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{softmax activated scores }\\mathbf{A}=\\text{softmax}(\\mathbf{Z}).$$\n","\n","---"],"metadata":{"id":"5rZkNr8d1gAw"}},{"cell_type":"code","source":["# Raw scores matrix\n","Z = torch.matmul(X_B, W_B) # also alled logits\n","print(Z)\n","\n","# Softmax activated scores\n","A = torch.nn.functional.softmax(Z, dim = 1)\n","\n","# Predicted probabilities for each sample\n","print(A)\n","\n","# True output label for each sample\n","print(Y)"],"metadata":{"id":"A5g-D7NLlZBl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651039750,"user_tz":-330,"elapsed":815,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"fd0dc264-ab12-43ad-a354-7cd3254a4a2f"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.5171, -0.5427, -0.3438],\n","        [ 3.6357, -0.6126,  1.9614],\n","        [-4.6127, -0.0660, -2.2940],\n","        [ 0.3821,  1.5427,  0.4789],\n","        [-1.5298,  1.4386, -1.5126],\n","        [ 3.2418, -1.1601,  2.3101]])\n","tensor([[0.3161, 0.3081, 0.3759],\n","        [0.8321, 0.0119, 0.1560],\n","        [0.0095, 0.8942, 0.0963],\n","        [0.1889, 0.6030, 0.2081],\n","        [0.0466, 0.9061, 0.0474],\n","        [0.7112, 0.0087, 0.2801]])\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]])\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Loss for each sample can be quantified using the categorical crossentropy (CCE) loss function which is defined as $$\\color{yellow}{-\\log(\\text{predicted probability that a sample belongs its correct class})}$$\n","\n","For example, consider a sample with\n","\n","- true_label = [$\\color{yellow}{1}$ 0 0]\n","- predicted_label = [$\\color{yellow}{0.05}$, 0.99, 0.05]\n","\n","categorical crossentropy loss = $-\\log(\\color{yellow}{0.05}).$\n","\n","Here, we calculate the average CCE loss for all all samples and average them out.\n","\n","---"],"metadata":{"id":"dgtOD11HljXv"}},{"cell_type":"code","source":["## Calculate average CCE loss\n","loss = torch.mean(-torch.log(torch.sum(Y * A, dim = 1)))\n","print(loss)\n","\n","# Using the PyTorch in-built function for CCE loss\n","loss_fn = torch.nn.CrossEntropyLoss()\n","loss = loss_fn(Z, torch.argmax(Y, dim = 1))\n","print(loss)"],"metadata":{"id":"aiCY-GvwlpDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733651041785,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"fa6aa3d4-dc3c-469d-cc7f-37541e3e200d"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.2304)\n","tensor(1.2304)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Applying the gradient descent method with\n","\n","- a maximum number of iterations equal to 1000\n","- a stopping tolerance equal to $10^{-6}$\n","- a learning rate of 0.01\n","\n"," to minimize $$L(\\mathbf{w}) = (w_1-2)^2+(w_2+3)^2$$ starting from $\\mathbf{w} = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$\n","\n","---"],"metadata":{"id":"Iro8wo7tnv2g"}},{"cell_type":"code","source":["# Initialize weights as tensors with gradients\n","w = torch.tensor([0.0, 0.0], requires_grad=True)\n","\n","# Hyperparameters\n","maxiter = 1000\n","tol = 1e-06\n","lr = 1e-02\n","norm_grad = float('inf')\n","\n","k = 0\n","while k < maxiter and norm_grad > tol:\n","    # Zero the gradients\n","    if w.grad is not None:\n","        w.grad.zero_()\n","\n","    # Define the loss function\n","    L = (w[0] - 2)**2 + (w[1] + 3)**2\n","\n","    # Backpropagate to compute gradients\n","    L.backward()\n","\n","    # Update weights using gradient descent\n","    with torch.no_grad():\n","        w -= lr * w.grad\n","\n","    # Compute the norm of the gradient\n","    norm_grad = w.grad.norm().item()\n","    k += 1\n","\n","    print(f'Iteration {k}: ||grad|| = {norm_grad}')"],"metadata":{"id":"qcM3YnJmnwrY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733653358961,"user_tz":-330,"elapsed":1033,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"714cdd13-c282-43b5-c14f-be54940c7288"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1: ||grad|| = 7.211102485656738\n","Iteration 2: ||grad|| = 7.066880702972412\n","Iteration 3: ||grad|| = 6.925542831420898\n","Iteration 4: ||grad|| = 6.787032127380371\n","Iteration 5: ||grad|| = 6.6512908935546875\n","Iteration 6: ||grad|| = 6.518265724182129\n","Iteration 7: ||grad|| = 6.387900352478027\n","Iteration 8: ||grad|| = 6.2601423263549805\n","Iteration 9: ||grad|| = 6.134939193725586\n","Iteration 10: ||grad|| = 6.012240409851074\n","Iteration 11: ||grad|| = 5.891995906829834\n","Iteration 12: ||grad|| = 5.774156093597412\n","Iteration 13: ||grad|| = 5.65867280960083\n","Iteration 14: ||grad|| = 5.545499324798584\n","Iteration 15: ||grad|| = 5.434589385986328\n","Iteration 16: ||grad|| = 5.325897693634033\n","Iteration 17: ||grad|| = 5.219379901885986\n","Iteration 18: ||grad|| = 5.114992141723633\n","Iteration 19: ||grad|| = 5.012691974639893\n","Iteration 20: ||grad|| = 4.91243839263916\n","Iteration 21: ||grad|| = 4.814189434051514\n","Iteration 22: ||grad|| = 4.717905521392822\n","Iteration 23: ||grad|| = 4.623547554016113\n","Iteration 24: ||grad|| = 4.531076431274414\n","Iteration 25: ||grad|| = 4.440454959869385\n","Iteration 26: ||grad|| = 4.3516459465026855\n","Iteration 27: ||grad|| = 4.264613151550293\n","Iteration 28: ||grad|| = 4.179320812225342\n","Iteration 29: ||grad|| = 4.095734119415283\n","Iteration 30: ||grad|| = 4.013819694519043\n","Iteration 31: ||grad|| = 3.9335432052612305\n","Iteration 32: ||grad|| = 3.854872465133667\n","Iteration 33: ||grad|| = 3.7777750492095947\n","Iteration 34: ||grad|| = 3.7022194862365723\n","Iteration 35: ||grad|| = 3.6281752586364746\n","Iteration 36: ||grad|| = 3.5556118488311768\n","Iteration 37: ||grad|| = 3.484499454498291\n","Iteration 38: ||grad|| = 3.4148097038269043\n","Iteration 39: ||grad|| = 3.346513509750366\n","Iteration 40: ||grad|| = 3.279583215713501\n","Iteration 41: ||grad|| = 3.213991641998291\n","Iteration 42: ||grad|| = 3.149711847305298\n","Iteration 43: ||grad|| = 3.0867176055908203\n","Iteration 44: ||grad|| = 3.0249834060668945\n","Iteration 45: ||grad|| = 2.9644837379455566\n","Iteration 46: ||grad|| = 2.905194044113159\n","Iteration 47: ||grad|| = 2.847090482711792\n","Iteration 48: ||grad|| = 2.7901482582092285\n","Iteration 49: ||grad|| = 2.7343454360961914\n","Iteration 50: ||grad|| = 2.6796586513519287\n","Iteration 51: ||grad|| = 2.626065492630005\n","Iteration 52: ||grad|| = 2.5735440254211426\n","Iteration 53: ||grad|| = 2.522073268890381\n","Iteration 54: ||grad|| = 2.4716317653656006\n","Iteration 55: ||grad|| = 2.422199010848999\n","Iteration 56: ||grad|| = 2.3737549781799316\n","Iteration 57: ||grad|| = 2.326280117034912\n","Iteration 58: ||grad|| = 2.279754400253296\n","Iteration 59: ||grad|| = 2.234158992767334\n","Iteration 60: ||grad|| = 2.1894760131835938\n","Iteration 61: ||grad|| = 2.145686388015747\n","Iteration 62: ||grad|| = 2.1027727127075195\n","Iteration 63: ||grad|| = 2.0607173442840576\n","Iteration 64: ||grad|| = 2.019503116607666\n","Iteration 65: ||grad|| = 1.979112982749939\n","Iteration 66: ||grad|| = 1.9395304918289185\n","Iteration 67: ||grad|| = 1.9007399082183838\n","Iteration 68: ||grad|| = 1.8627251386642456\n","Iteration 69: ||grad|| = 1.8254705667495728\n","Iteration 70: ||grad|| = 1.7889609336853027\n","Iteration 71: ||grad|| = 1.7531819343566895\n","Iteration 72: ||grad|| = 1.7181183099746704\n","Iteration 73: ||grad|| = 1.6837559938430786\n","Iteration 74: ||grad|| = 1.6500808000564575\n","Iteration 75: ||grad|| = 1.6170790195465088\n","Iteration 76: ||grad|| = 1.5847375392913818\n","Iteration 77: ||grad|| = 1.553043007850647\n","Iteration 78: ||grad|| = 1.521982192993164\n","Iteration 79: ||grad|| = 1.4915425777435303\n","Iteration 80: ||grad|| = 1.4617117643356323\n","Iteration 81: ||grad|| = 1.432477355003357\n","Iteration 82: ||grad|| = 1.4038276672363281\n","Iteration 83: ||grad|| = 1.37575101852417\n","Iteration 84: ||grad|| = 1.3482359647750854\n","Iteration 85: ||grad|| = 1.3212710618972778\n","Iteration 86: ||grad|| = 1.2948455810546875\n","Iteration 87: ||grad|| = 1.2689487934112549\n","Iteration 88: ||grad|| = 1.2435698509216309\n","Iteration 89: ||grad|| = 1.218698501586914\n","Iteration 90: ||grad|| = 1.1943244934082031\n","Iteration 91: ||grad|| = 1.1704381704330444\n","Iteration 92: ||grad|| = 1.1470293998718262\n","Iteration 93: ||grad|| = 1.1240887641906738\n","Iteration 94: ||grad|| = 1.1016069650650024\n","Iteration 95: ||grad|| = 1.079574704170227\n","Iteration 96: ||grad|| = 1.057983160018921\n","Iteration 97: ||grad|| = 1.0368235111236572\n","Iteration 98: ||grad|| = 1.0160870552062988\n","Iteration 99: ||grad|| = 0.9957653284072876\n","Iteration 100: ||grad|| = 0.9758499264717102\n","Iteration 101: ||grad|| = 0.9563328623771667\n","Iteration 102: ||grad|| = 0.9372060894966125\n","Iteration 103: ||grad|| = 0.9184620380401611\n","Iteration 104: ||grad|| = 0.9000928401947021\n","Iteration 105: ||grad|| = 0.8820909857749939\n","Iteration 106: ||grad|| = 0.8644490838050842\n","Iteration 107: ||grad|| = 0.8471602201461792\n","Iteration 108: ||grad|| = 0.8302169442176819\n","Iteration 109: ||grad|| = 0.8136128187179565\n","Iteration 110: ||grad|| = 0.7973405718803406\n","Iteration 111: ||grad|| = 0.7813937067985535\n","Iteration 112: ||grad|| = 0.7657656669616699\n","Iteration 113: ||grad|| = 0.7504504919052124\n","Iteration 114: ||grad|| = 0.7354413270950317\n","Iteration 115: ||grad|| = 0.7207325100898743\n","Iteration 116: ||grad|| = 0.7063177227973938\n","Iteration 117: ||grad|| = 0.6921911835670471\n","Iteration 118: ||grad|| = 0.6783471703529358\n","Iteration 119: ||grad|| = 0.6647803783416748\n","Iteration 120: ||grad|| = 0.6514847874641418\n","Iteration 121: ||grad|| = 0.6384550333023071\n","Iteration 122: ||grad|| = 0.6256861090660095\n","Iteration 123: ||grad|| = 0.6131722927093506\n","Iteration 124: ||grad|| = 0.6009087562561035\n","Iteration 125: ||grad|| = 0.5888906121253967\n","Iteration 126: ||grad|| = 0.57711261510849\n","Iteration 127: ||grad|| = 0.5655705332756042\n","Iteration 128: ||grad|| = 0.5542590022087097\n","Iteration 129: ||grad|| = 0.5431737303733826\n","Iteration 130: ||grad|| = 0.5323102474212646\n","Iteration 131: ||grad|| = 0.5216640830039978\n","Iteration 132: ||grad|| = 0.5112309455871582\n","Iteration 133: ||grad|| = 0.5010064840316772\n","Iteration 134: ||grad|| = 0.49098649621009827\n","Iteration 135: ||grad|| = 0.4811667501926422\n","Iteration 136: ||grad|| = 0.4715435206890106\n","Iteration 137: ||grad|| = 0.462112694978714\n","Iteration 138: ||grad|| = 0.4528704881668091\n","Iteration 139: ||grad|| = 0.44381290674209595\n","Iteration 140: ||grad|| = 0.4349364936351776\n","Iteration 141: ||grad|| = 0.42623767256736755\n","Iteration 142: ||grad|| = 0.417712926864624\n","Iteration 143: ||grad|| = 0.40935850143432617\n","Iteration 144: ||grad|| = 0.40117138624191284\n","Iteration 145: ||grad|| = 0.39314812421798706\n","Iteration 146: ||grad|| = 0.38528528809547424\n","Iteration 147: ||grad|| = 0.37757956981658936\n","Iteration 148: ||grad|| = 0.3700280487537384\n","Iteration 149: ||grad|| = 0.36262768507003784\n","Iteration 150: ||grad|| = 0.35537517070770264\n","Iteration 151: ||grad|| = 0.3482677638530731\n","Iteration 152: ||grad|| = 0.3413025140762329\n","Iteration 153: ||grad|| = 0.334476500749588\n","Iteration 154: ||grad|| = 0.3277868628501892\n","Iteration 155: ||grad|| = 0.32123130559921265\n","Iteration 156: ||grad|| = 0.31480658054351807\n","Iteration 157: ||grad|| = 0.3085106313228607\n","Iteration 158: ||grad|| = 0.30234020948410034\n","Iteration 159: ||grad|| = 0.29629358649253845\n","Iteration 160: ||grad|| = 0.29036781191825867\n","Iteration 161: ||grad|| = 0.2845606803894043\n","Iteration 162: ||grad|| = 0.2788693904876709\n","Iteration 163: ||grad|| = 0.2732921242713928\n","Iteration 164: ||grad|| = 0.26782605051994324\n","Iteration 165: ||grad|| = 0.26246950030326843\n","Iteration 166: ||grad|| = 0.257220059633255\n","Iteration 167: ||grad|| = 0.2520754933357239\n","Iteration 168: ||grad|| = 0.2470339685678482\n","Iteration 169: ||grad|| = 0.24209333956241608\n","Iteration 170: ||grad|| = 0.2372513711452484\n","Iteration 171: ||grad|| = 0.23250623047351837\n","Iteration 172: ||grad|| = 0.22785614430904388\n","Iteration 173: ||grad|| = 0.22329892218112946\n","Iteration 174: ||grad|| = 0.21883293986320496\n","Iteration 175: ||grad|| = 0.2144562304019928\n","Iteration 176: ||grad|| = 0.21016719937324524\n","Iteration 177: ||grad|| = 0.20596374571323395\n","Iteration 178: ||grad|| = 0.20184439420700073\n","Iteration 179: ||grad|| = 0.19780756533145905\n","Iteration 180: ||grad|| = 0.1938515454530716\n","Iteration 181: ||grad|| = 0.1899746209383011\n","Iteration 182: ||grad|| = 0.18617506325244904\n","Iteration 183: ||grad|| = 0.18245166540145874\n","Iteration 184: ||grad|| = 0.1788027286529541\n","Iteration 185: ||grad|| = 0.17522667348384857\n","Iteration 186: ||grad|| = 0.1717221438884735\n","Iteration 187: ||grad|| = 0.16828759014606476\n","Iteration 188: ||grad|| = 0.1649218201637268\n","Iteration 189: ||grad|| = 0.16162320971488953\n","Iteration 190: ||grad|| = 0.1583908647298813\n","Iteration 191: ||grad|| = 0.15522293746471405\n","Iteration 192: ||grad|| = 0.1521184742450714\n","Iteration 193: ||grad|| = 0.14907604455947876\n","Iteration 194: ||grad|| = 0.14609432220458984\n","Iteration 195: ||grad|| = 0.1431722342967987\n","Iteration 196: ||grad|| = 0.14030861854553223\n","Iteration 197: ||grad|| = 0.1375022530555725\n","Iteration 198: ||grad|| = 0.134752094745636\n","Iteration 199: ||grad|| = 0.13205695152282715\n","Iteration 200: ||grad|| = 0.12941564619541168\n","Iteration 201: ||grad|| = 0.12682749330997467\n","Iteration 202: ||grad|| = 0.12429092079401016\n","Iteration 203: ||grad|| = 0.12180500477552414\n","Iteration 204: ||grad|| = 0.11936880648136139\n","Iteration 205: ||grad|| = 0.11698141694068909\n","Iteration 206: ||grad|| = 0.11464203149080276\n","Iteration 207: ||grad|| = 0.11234906315803528\n","Iteration 208: ||grad|| = 0.11010199040174484\n","Iteration 209: ||grad|| = 0.10790015012025833\n","Iteration 210: ||grad|| = 0.105741947889328\n","Iteration 211: ||grad|| = 0.10362725704908371\n","Iteration 212: ||grad|| = 0.10155488550662994\n","Iteration 213: ||grad|| = 0.09952377527952194\n","Iteration 214: ||grad|| = 0.09753340482711792\n","Iteration 215: ||grad|| = 0.09558270126581192\n","Iteration 216: ||grad|| = 0.09367088973522186\n","Iteration 217: ||grad|| = 0.09179741889238358\n","Iteration 218: ||grad|| = 0.08996125310659409\n","Iteration 219: ||grad|| = 0.08816184103488922\n","Iteration 220: ||grad|| = 0.08639854192733765\n","Iteration 221: ||grad|| = 0.08467068523168564\n","Iteration 222: ||grad|| = 0.08297721296548843\n","Iteration 223: ||grad|| = 0.08131759613752365\n","Iteration 224: ||grad|| = 0.07969117909669876\n","Iteration 225: ||grad|| = 0.07809742540121078\n","Iteration 226: ||grad|| = 0.0765356719493866\n","Iteration 227: ||grad|| = 0.07500500231981277\n","Iteration 228: ||grad|| = 0.07350475341081619\n","Iteration 229: ||grad|| = 0.07203477621078491\n","Iteration 230: ||grad|| = 0.07059403508901596\n","Iteration 231: ||grad|| = 0.06918199360370636\n","Iteration 232: ||grad|| = 0.06779851764440536\n","Iteration 233: ||grad|| = 0.06644255667924881\n","Iteration 234: ||grad|| = 0.06511356681585312\n","Iteration 235: ||grad|| = 0.06381142139434814\n","Iteration 236: ||grad|| = 0.06253520399332047\n","Iteration 237: ||grad|| = 0.061284638941287994\n","Iteration 238: ||grad|| = 0.06005880609154701\n","Iteration 239: ||grad|| = 0.058857571333646774\n","Iteration 240: ||grad|| = 0.0576804094016552\n","Iteration 241: ||grad|| = 0.0565267838537693\n","Iteration 242: ||grad|| = 0.055396173149347305\n","Iteration 243: ||grad|| = 0.05428830534219742\n","Iteration 244: ||grad|| = 0.053202394396066666\n","Iteration 245: ||grad|| = 0.05213817209005356\n","Iteration 246: ||grad|| = 0.05109523981809616\n","Iteration 247: ||grad|| = 0.05007333680987358\n","Iteration 248: ||grad|| = 0.04907206818461418\n","Iteration 249: ||grad|| = 0.04809050261974335\n","Iteration 250: ||grad|| = 0.04712877422571182\n","Iteration 251: ||grad|| = 0.046186089515686035\n","Iteration 252: ||grad|| = 0.04526231810450554\n","Iteration 253: ||grad|| = 0.04435693100094795\n","Iteration 254: ||grad|| = 0.04346979036927223\n","Iteration 255: ||grad|| = 0.042600374668836594\n","Iteration 256: ||grad|| = 0.0417482815682888\n","Iteration 257: ||grad|| = 0.04091325029730797\n","Iteration 258: ||grad|| = 0.04009488224983215\n","Iteration 259: ||grad|| = 0.03929304704070091\n","Iteration 260: ||grad|| = 0.03850734233856201\n","Iteration 261: ||grad|| = 0.03773711249232292\n","Iteration 262: ||grad|| = 0.03698235750198364\n","Iteration 263: ||grad|| = 0.03624254837632179\n","Iteration 264: ||grad|| = 0.03551768139004707\n","Iteration 265: ||grad|| = 0.034807492047548294\n","Iteration 266: ||grad|| = 0.03411119431257248\n","Iteration 267: ||grad|| = 0.03342917561531067\n","Iteration 268: ||grad|| = 0.0327603854238987\n","Iteration 269: ||grad|| = 0.03210534527897835\n","Iteration 270: ||grad|| = 0.03146339952945709\n","Iteration 271: ||grad|| = 0.030834149569272995\n","Iteration 272: ||grad|| = 0.03021746501326561\n","Iteration 273: ||grad|| = 0.029612945392727852\n","Iteration 274: ||grad|| = 0.02902085706591606\n","Iteration 275: ||grad|| = 0.028440410271286964\n","Iteration 276: ||grad|| = 0.02787146531045437\n","Iteration 277: ||grad|| = 0.027313897386193275\n","Iteration 278: ||grad|| = 0.026767700910568237\n","Iteration 279: ||grad|| = 0.026232348755002022\n","Iteration 280: ||grad|| = 0.025707842782139778\n","Iteration 281: ||grad|| = 0.02519378252327442\n","Iteration 282: ||grad|| = 0.024690039455890656\n","Iteration 283: ||grad|| = 0.02419608272612095\n","Iteration 284: ||grad|| = 0.023712310940027237\n","Iteration 285: ||grad|| = 0.0232379250228405\n","Iteration 286: ||grad|| = 0.022773196920752525\n","Iteration 287: ||grad|| = 0.022317592054605484\n","Iteration 288: ||grad|| = 0.021871114149689674\n","Iteration 289: ||grad|| = 0.021433759480714798\n","Iteration 290: ||grad|| = 0.021005135029554367\n","Iteration 291: ||grad|| = 0.020585106685757637\n","Iteration 292: ||grad|| = 0.020173542201519012\n","Iteration 293: ||grad|| = 0.019770044833421707\n","Iteration 294: ||grad|| = 0.01937461458146572\n","Iteration 295: ||grad|| = 0.01898711919784546\n","Iteration 296: ||grad|| = 0.018607163801789284\n","Iteration 297: ||grad|| = 0.018235141411423683\n","Iteration 298: ||grad|| = 0.017870524898171425\n","Iteration 299: ||grad|| = 0.017512919381260872\n","Iteration 300: ||grad|| = 0.01716271974146366\n","Iteration 301: ||grad|| = 0.01681939698755741\n","Iteration 302: ||grad|| = 0.016482951119542122\n","Iteration 303: ||grad|| = 0.016153382137417793\n","Iteration 304: ||grad|| = 0.015830159187316895\n","Iteration 305: ||grad|| = 0.015513683669269085\n","Iteration 306: ||grad|| = 0.015203556045889854\n","Iteration 307: ||grad|| = 0.014899378642439842\n","Iteration 308: ||grad|| = 0.014601417817175388\n","Iteration 309: ||grad|| = 0.014309275895357132\n","Iteration 310: ||grad|| = 0.014023217372596264\n","Iteration 311: ||grad|| = 0.013742845505475998\n","Iteration 312: ||grad|| = 0.013467895798385143\n","Iteration 313: ||grad|| = 0.013198500499129295\n","Iteration 314: ||grad|| = 0.012934396043419838\n","Iteration 315: ||grad|| = 0.012675845995545387\n","Iteration 316: ||grad|| = 0.012422452680766582\n","Iteration 317: ||grad|| = 0.012173821218311787\n","Iteration 318: ||grad|| = 0.011930347420275211\n","Iteration 319: ||grad|| = 0.011691899970173836\n","Iteration 320: ||grad|| = 0.011458080261945724\n","Iteration 321: ||grad|| = 0.011228889226913452\n","Iteration 322: ||grad|| = 0.011004326865077019\n","Iteration 323: ||grad|| = 0.010784261859953403\n","Iteration 324: ||grad|| = 0.010568693280220032\n","Iteration 325: ||grad|| = 0.010357224382460117\n","Iteration 326: ||grad|| = 0.010150250978767872\n","Iteration 327: ||grad|| = 0.009947378188371658\n","Iteration 328: ||grad|| = 0.009748473763465881\n","Iteration 329: ||grad|| = 0.009553535841405392\n","Iteration 330: ||grad|| = 0.009362565353512764\n","Iteration 331: ||grad|| = 0.009175166487693787\n","Iteration 332: ||grad|| = 0.008991734124720097\n","Iteration 333: ||grad|| = 0.008811873383820057\n","Iteration 334: ||grad|| = 0.008635450154542923\n","Iteration 335: ||grad|| = 0.00846286304295063\n","Iteration 336: ||grad|| = 0.008293714374303818\n","Iteration 337: ||grad|| = 0.008128004148602486\n","Iteration 338: ||grad|| = 0.007965335622429848\n","Iteration 339: ||grad|| = 0.007805973291397095\n","Iteration 340: ||grad|| = 0.007650049403309822\n","Iteration 341: ||grad|| = 0.0074970354326069355\n","Iteration 342: ||grad|| = 0.00734693044796586\n","Iteration 343: ||grad|| = 0.0072001321241259575\n","Iteration 344: ||grad|| = 0.007056243252009153\n","Iteration 345: ||grad|| = 0.006915263831615448\n","Iteration 346: ||grad|| = 0.006777061615139246\n","Iteration 347: ||grad|| = 0.0066413721069693565\n","Iteration 348: ||grad|| = 0.006508460268378258\n","Iteration 349: ||grad|| = 0.006378325633704662\n","Iteration 350: ||grad|| = 0.006250570993870497\n","Iteration 351: ||grad|| = 0.006125594023615122\n","Iteration 352: ||grad|| = 0.006002997513860464\n","Iteration 353: ||grad|| = 0.0058827814646065235\n","Iteration 354: ||grad|| = 0.005765210837125778\n","Iteration 355: ||grad|| = 0.005650019738823175\n","Iteration 356: ||grad|| = 0.005537078250199556\n","Iteration 357: ||grad|| = 0.005426384042948484\n","Iteration 358: ||grad|| = 0.005318070761859417\n","Iteration 359: ||grad|| = 0.005211608484387398\n","Iteration 360: ||grad|| = 0.005107394885271788\n","Iteration 361: ||grad|| = 0.005005297251045704\n","Iteration 362: ||grad|| = 0.004905051086097956\n","Iteration 363: ||grad|| = 0.004807053133845329\n","Iteration 364: ||grad|| = 0.00471077486872673\n","Iteration 365: ||grad|| = 0.004616744350641966\n","Iteration 366: ||grad|| = 0.004524433519691229\n","Iteration 367: ||grad|| = 0.00443397369235754\n","Iteration 368: ||grad|| = 0.004345233552157879\n","Iteration 369: ||grad|| = 0.0042582121677696705\n","Iteration 370: ||grad|| = 0.00417291047051549\n","Iteration 371: ||grad|| = 0.004089327994734049\n","Iteration 372: ||grad|| = 0.004007464740425348\n","Iteration 373: ||grad|| = 0.003927320707589388\n","Iteration 374: ||grad|| = 0.0038487636484205723\n","Iteration 375: ||grad|| = 0.0037719260435551405\n","Iteration 376: ||grad|| = 0.003696410683915019\n","Iteration 377: ||grad|| = 0.003622482530772686\n","Iteration 378: ||grad|| = 0.0035501413512974977\n","Iteration 379: ||grad|| = 0.0034791226498782635\n","Iteration 380: ||grad|| = 0.003409690922126174\n","Iteration 381: ||grad|| = 0.0033414496574550867\n","Iteration 382: ||grad|| = 0.0032745306380093098\n","Iteration 383: ||grad|| = 0.0032091985922306776\n","Iteration 384: ||grad|| = 0.0031450570095330477\n","Iteration 385: ||grad|| = 0.0030821056570857763\n","Iteration 386: ||grad|| = 0.0030203445348888636\n","Iteration 387: ||grad|| = 0.0029600381385535\n","Iteration 388: ||grad|| = 0.0029009219724684954\n","Iteration 389: ||grad|| = 0.002842996036633849\n","Iteration 390: ||grad|| = 0.002786260563880205\n","Iteration 391: ||grad|| = 0.0027305828407406807\n","Iteration 392: ||grad|| = 0.0026760955806821585\n","Iteration 393: ||grad|| = 0.0026226663030683994\n","Iteration 394: ||grad|| = 0.0025700305122882128\n","Iteration 395: ||grad|| = 0.0025184527039527893\n","Iteration 396: ||grad|| = 0.0024680651258677244\n","Iteration 397: ||grad|| = 0.0024187355302274227\n","Iteration 398: ||grad|| = 0.002370463917031884\n","Iteration 399: ||grad|| = 0.0023229860235005617\n","Iteration 400: ||grad|| = 0.0022765658795833588\n","Iteration 401: ||grad|| = 0.0022312039509415627\n","Iteration 402: ||grad|| = 0.0021865030284971\n","Iteration 403: ||grad|| = 0.002142860321328044\n","Iteration 404: ||grad|| = 0.002099878853186965\n","Iteration 405: ||grad|| = 0.0020579553674906492\n","Iteration 406: ||grad|| = 0.0020166931208223104\n","Iteration 407: ||grad|| = 0.001976488856598735\n","Iteration 408: ||grad|| = 0.001936945947818458\n","Iteration 409: ||grad|| = 0.0018980641616508365\n","Iteration 410: ||grad|| = 0.001860240357927978\n","Iteration 411: ||grad|| = 0.001822945661842823\n","Iteration 412: ||grad|| = 0.001786312204785645\n","Iteration 413: ||grad|| = 0.0017507367301732302\n","Iteration 414: ||grad|| = 0.0017158224945887923\n","Iteration 415: ||grad|| = 0.001681437250226736\n","Iteration 416: ||grad|| = 0.0016477132448926568\n","Iteration 417: ||grad|| = 0.001614518347196281\n","Iteration 418: ||grad|| = 0.0015823813155293465\n","Iteration 419: ||grad|| = 0.0015507735079154372\n","Iteration 420: ||grad|| = 0.0015198268229141831\n","Iteration 421: ||grad|| = 0.0014894091291353106\n","Iteration 422: ||grad|| = 0.0014596526743844151\n","Iteration 423: ||grad|| = 0.0014304252108559012\n","Iteration 424: ||grad|| = 0.0014017268549650908\n","Iteration 425: ||grad|| = 0.0013736896216869354\n","Iteration 426: ||grad|| = 0.0013461814960464835\n","Iteration 427: ||grad|| = 0.0013192023616284132\n","Iteration 428: ||grad|| = 0.001292884349822998\n","Iteration 429: ||grad|| = 0.0012670954456552863\n","Iteration 430: ||grad|| = 0.001241835649125278\n","Iteration 431: ||grad|| = 0.0012171048438176513\n","Iteration 432: ||grad|| = 0.0011925061699002981\n","Iteration 433: ||grad|| = 0.0011685686185956001\n","Iteration 434: ||grad|| = 0.0011451601749286056\n","Iteration 435: ||grad|| = 0.0011222807224839926\n","Iteration 436: ||grad|| = 0.0010999302612617612\n","Iteration 437: ||grad|| = 0.001078109024092555\n","Iteration 438: ||grad|| = 0.0010564198018983006\n","Iteration 439: ||grad|| = 0.0010352595709264278\n","Iteration 440: ||grad|| = 0.0010146284475922585\n","Iteration 441: ||grad|| = 0.0009945264318957925\n","Iteration 442: ||grad|| = 0.0009745564893819392\n","Iteration 443: ||grad|| = 0.0009551155962981284\n","Iteration 444: ||grad|| = 0.0009362036944366992\n","Iteration 445: ||grad|| = 0.0009172918507829309\n","Iteration 446: ||grad|| = 0.0008989089983515441\n","Iteration 447: ||grad|| = 0.0008810551953501999\n","Iteration 448: ||grad|| = 0.0008633335819467902\n","Iteration 449: ||grad|| = 0.0008461409597657621\n","Iteration 450: ||grad|| = 0.0008290805271826684\n","Iteration 451: ||grad|| = 0.0008124169544316828\n","Iteration 452: ||grad|| = 0.0007962823729030788\n","Iteration 453: ||grad|| = 0.0007802800391800702\n","Iteration 454: ||grad|| = 0.0007648066966794431\n","Iteration 455: ||grad|| = 0.0007493332959711552\n","Iteration 456: ||grad|| = 0.0007343890029005706\n","Iteration 457: ||grad|| = 0.0007195768412202597\n","Iteration 458: ||grad|| = 0.0007052937289699912\n","Iteration 459: ||grad|| = 0.0006910106167197227\n","Iteration 460: ||grad|| = 0.0006772565538994968\n","Iteration 461: ||grad|| = 0.0006636346806772053\n","Iteration 462: ||grad|| = 0.000650409609079361\n","Iteration 463: ||grad|| = 0.000637316785287112\n","Iteration 464: ||grad|| = 0.0006246207049116492\n","Iteration 465: ||grad|| = 0.0006120568723417819\n","Iteration 466: ||grad|| = 0.0006000220309942961\n","Iteration 467: ||grad|| = 0.0005879871896468103\n","Iteration 468: ||grad|| = 0.0005760846543125808\n","Iteration 469: ||grad|| = 0.0005645788041874766\n","Iteration 470: ||grad|| = 0.0005532053182832897\n","Iteration 471: ||grad|| = 0.0005422284011729062\n","Iteration 472: ||grad|| = 0.0005313839064911008\n","Iteration 473: ||grad|| = 0.0005205393536016345\n","Iteration 474: ||grad|| = 0.0005102237919345498\n","Iteration 475: ||grad|| = 0.0004999082302674651\n","Iteration 476: ||grad|| = 0.0004901216016151011\n","Iteration 477: ||grad|| = 0.000480335031170398\n","Iteration 478: ||grad|| = 0.00047068079584278166\n","Iteration 479: ||grad|| = 0.0004614231875166297\n","Iteration 480: ||grad|| = 0.00045229788520373404\n","Iteration 481: ||grad|| = 0.00044317261199466884\n","Iteration 482: ||grad|| = 0.00043444399489089847\n","Iteration 483: ||grad|| = 0.0004258476838003844\n","Iteration 484: ||grad|| = 0.00041725137270987034\n","Iteration 485: ||grad|| = 0.00040878739673644304\n","Iteration 486: ||grad|| = 0.00040072007686831057\n","Iteration 487: ||grad|| = 0.0003926527570001781\n","Iteration 488: ||grad|| = 0.0003847177722491324\n","Iteration 489: ||grad|| = 0.0003771793853957206\n","Iteration 490: ||grad|| = 0.0003696410858538002\n","Iteration 491: ||grad|| = 0.00036223503411747515\n","Iteration 492: ||grad|| = 0.0003548290114849806\n","Iteration 493: ||grad|| = 0.00034781970316544175\n","Iteration 494: ||grad|| = 0.0003409426426514983\n","Iteration 495: ||grad|| = 0.00033406561124138534\n","Iteration 496: ||grad|| = 0.00032718857983127236\n","Iteration 497: ||grad|| = 0.000320840539643541\n","Iteration 498: ||grad|| = 0.00031449252855964005\n","Iteration 499: ||grad|| = 0.0003081445465795696\n","Iteration 500: ||grad|| = 0.0003019286668859422\n","Iteration 501: ||grad|| = 0.00029571287450380623\n","Iteration 502: ||grad|| = 0.0002898938546422869\n","Iteration 503: ||grad|| = 0.0002842070534825325\n","Iteration 504: ||grad|| = 0.0002785202523227781\n","Iteration 505: ||grad|| = 0.00027283348026685417\n","Iteration 506: ||grad|| = 0.00026714670821093023\n","Iteration 507: ||grad|| = 0.0002619889273773879\n","Iteration 508: ||grad|| = 0.00025683114654384553\n","Iteration 509: ||grad|| = 0.0002516733657103032\n","Iteration 510: ||grad|| = 0.00024651558487676084\n","Iteration 511: ||grad|| = 0.00024149024102371186\n","Iteration 512: ||grad|| = 0.0002368613932048902\n","Iteration 513: ||grad|| = 0.00023223254538606852\n","Iteration 514: ||grad|| = 0.0002276037266710773\n","Iteration 515: ||grad|| = 0.0002229749079560861\n","Iteration 516: ||grad|| = 0.00021847846801392734\n","Iteration 517: ||grad|| = 0.00021398210083134472\n","Iteration 518: ||grad|| = 0.0002098821714753285\n","Iteration 519: ||grad|| = 0.0002057823003269732\n","Iteration 520: ||grad|| = 0.0002016825310420245\n","Iteration 521: ||grad|| = 0.00019771499501075596\n","Iteration 522: ||grad|| = 0.00019374748808331788\n","Iteration 523: ||grad|| = 0.00018977999570779502\n","Iteration 524: ||grad|| = 0.00018581253243610263\n","Iteration 525: ||grad|| = 0.00018224165251012892\n","Iteration 526: ||grad|| = 0.0001788031222531572\n","Iteration 527: ||grad|| = 0.0001753646065481007\n","Iteration 528: ||grad|| = 0.000171926076291129\n","Iteration 529: ||grad|| = 0.0001684875605860725\n","Iteration 530: ||grad|| = 0.00016504904488101602\n","Iteration 531: ||grad|| = 0.0001616105146240443\n","Iteration 532: ||grad|| = 0.0001581719989189878\n","Iteration 533: ||grad|| = 0.00015486584743484855\n","Iteration 534: ||grad|| = 0.00015195626474451274\n","Iteration 535: ||grad|| = 0.00014904669660609215\n","Iteration 536: ||grad|| = 0.00014613717212341726\n","Iteration 537: ||grad|| = 0.0001432276621926576\n","Iteration 538: ||grad|| = 0.0001403181959176436\n","Iteration 539: ||grad|| = 0.00013740875874646008\n","Iteration 540: ||grad|| = 0.00013463136565405875\n","Iteration 541: ||grad|| = 0.00013185410352889448\n","Iteration 542: ||grad|| = 0.00012907695781905204\n","Iteration 543: ||grad|| = 0.0001262999721802771\n","Iteration 544: ||grad|| = 0.00012391919153742492\n","Iteration 545: ||grad|| = 0.00012153853458585218\n","Iteration 546: ||grad|| = 0.0001191580158774741\n","Iteration 547: ||grad|| = 0.00011677765724016353\n","Iteration 548: ||grad|| = 0.00011452929902588949\n","Iteration 549: ||grad|| = 0.00011228097719140351\n","Iteration 550: ||grad|| = 0.0001100326917367056\n","Iteration 551: ||grad|| = 0.00010778444993775338\n","Iteration 552: ||grad|| = 0.00010553624451858923\n","Iteration 553: ||grad|| = 0.00010328809003112838\n","Iteration 554: ||grad|| = 0.00010103997919941321\n","Iteration 555: ||grad|| = 9.879192657535896e-05\n","Iteration 556: ||grad|| = 9.693994797999039e-05\n","Iteration 557: ||grad|| = 9.508836956229061e-05\n","Iteration 558: ||grad|| = 9.336911170976236e-05\n","Iteration 559: ||grad|| = 9.164985385723412e-05\n","Iteration 560: ||grad|| = 8.993058872874826e-05\n","Iteration 561: ||grad|| = 8.821133087622002e-05\n","Iteration 562: ||grad|| = 8.649207302369177e-05\n","Iteration 563: ||grad|| = 8.477280789520591e-05\n","Iteration 564: ||grad|| = 8.305355004267767e-05\n","Iteration 565: ||grad|| = 8.133429219014943e-05\n","Iteration 566: ||grad|| = 7.961502706166357e-05\n","Iteration 567: ||grad|| = 7.789576920913532e-05\n","Iteration 568: ||grad|| = 7.617651135660708e-05\n","Iteration 569: ||grad|| = 7.445724622812122e-05\n","Iteration 570: ||grad|| = 7.287033076863736e-05\n","Iteration 571: ||grad|| = 7.128396828193218e-05\n","Iteration 572: ||grad|| = 7.009343971731141e-05\n","Iteration 573: ||grad|| = 6.890296936035156e-05\n","Iteration 574: ||grad|| = 6.771255721105263e-05\n","Iteration 575: ||grad|| = 6.65222032694146e-05\n","Iteration 576: ||grad|| = 6.533191481139511e-05\n","Iteration 577: ||grad|| = 6.414168456103653e-05\n","Iteration 578: ||grad|| = 6.29515343462117e-05\n","Iteration 579: ||grad|| = 6.176145689096302e-05\n","Iteration 580: ||grad|| = 6.057145947124809e-05\n","Iteration 581: ||grad|| = 5.9381545725045726e-05\n","Iteration 582: ||grad|| = 5.819171929033473e-05\n","Iteration 583: ||grad|| = 5.700198744307272e-05\n","Iteration 584: ||grad|| = 5.581235745921731e-05\n","Iteration 585: ||grad|| = 5.462283661472611e-05\n","Iteration 586: ||grad|| = 5.343342854757793e-05\n","Iteration 587: ||grad|| = 5.2374001825228333e-05\n","Iteration 588: ||grad|| = 5.131485522724688e-05\n","Iteration 589: ||grad|| = 5.025601421948522e-05\n","Iteration 590: ||grad|| = 4.9197486077900976e-05\n","Iteration 591: ||grad|| = 4.81393035443034e-05\n","Iteration 592: ||grad|| = 4.708148117060773e-05\n","Iteration 593: ||grad|| = 4.602404806064442e-05\n","Iteration 594: ||grad|| = 4.496703331824392e-05\n","Iteration 595: ||grad|| = 4.391046240925789e-05\n","Iteration 596: ||grad|| = 4.285437171347439e-05\n","Iteration 597: ||grad|| = 4.219133188598789e-05\n","Iteration 598: ||grad|| = 4.15286558563821e-05\n","Iteration 599: ||grad|| = 4.0866361814551055e-05\n","Iteration 600: ||grad|| = 4.0204471588367596e-05\n","Iteration 601: ||grad|| = 3.954299972974695e-05\n","Iteration 602: ||grad|| = 3.888196806656197e-05\n","Iteration 603: ||grad|| = 3.8221405702643096e-05\n","Iteration 604: ||grad|| = 3.7561330827884376e-05\n","Iteration 605: ||grad|| = 3.690177254611626e-05\n","Iteration 606: ||grad|| = 3.624275996116921e-05\n","Iteration 607: ||grad|| = 3.5584322176873684e-05\n","Iteration 608: ||grad|| = 3.492649193503894e-05\n","Iteration 609: ||grad|| = 3.426930197747424e-05\n","Iteration 610: ||grad|| = 3.361279232194647e-05\n","Iteration 611: ||grad|| = 3.30818111251574e-05\n","Iteration 612: ||grad|| = 3.255090268794447e-05\n","Iteration 613: ||grad|| = 3.2020067010307685e-05\n","Iteration 614: ||grad|| = 3.1489307730225846e-05\n","Iteration 615: ||grad|| = 3.095863212365657e-05\n","Iteration 616: ||grad|| = 3.042804019059986e-05\n","Iteration 617: ||grad|| = 2.989753738802392e-05\n","Iteration 618: ||grad|| = 2.936713099188637e-05\n","Iteration 619: ||grad|| = 2.883682282117661e-05\n","Iteration 620: ||grad|| = 2.830662197084166e-05\n","Iteration 621: ||grad|| = 2.7776532078860328e-05\n","Iteration 622: ||grad|| = 2.7246560421190225e-05\n","Iteration 623: ||grad|| = 2.6716714273788966e-05\n","Iteration 624: ||grad|| = 2.6187000912614167e-05\n","Iteration 625: ||grad|| = 2.565742761362344e-05\n","Iteration 626: ||grad|| = 2.512800710974261e-05\n","Iteration 627: ||grad|| = 2.4598743038950488e-05\n","Iteration 628: ||grad|| = 2.40696517721517e-05\n","Iteration 629: ||grad|| = 2.3540740585303865e-05\n","Iteration 630: ||grad|| = 2.301202403032221e-05\n","Iteration 631: ||grad|| = 2.248351665912196e-05\n","Iteration 632: ||grad|| = 2.1955231204628944e-05\n","Iteration 633: ||grad|| = 2.1427185856737196e-05\n","Iteration 634: ||grad|| = 2.089939698635135e-05\n","Iteration 635: ||grad|| = 2.0371888240333647e-05\n","Iteration 636: ||grad|| = 1.9844677808578126e-05\n","Iteration 637: ||grad|| = 1.9317791156936437e-05\n","Iteration 638: ||grad|| = 1.8791255570249632e-05\n","Iteration 639: ||grad|| = 1.8265103790326975e-05\n","Iteration 640: ||grad|| = 1.773936855897773e-05\n","Iteration 641: ||grad|| = 1.721408625598997e-05\n","Iteration 642: ||grad|| = 1.6689300537109375e-05\n","Iteration 643: ||grad|| = 1.6165060515049845e-05\n","Iteration 644: ||grad|| = 1.564142257848289e-05\n","Iteration 645: ||grad|| = 1.511844493506942e-05\n","Iteration 646: ||grad|| = 1.4596203072869685e-05\n","Iteration 647: ||grad|| = 1.4449425179918762e-05\n","Iteration 648: ||grad|| = 1.430511474609375e-05\n","Iteration 649: ||grad|| = 1.4163348168949597e-05\n","Iteration 650: ||grad|| = 1.4024201846041251e-05\n","Iteration 651: ||grad|| = 1.3887753993913066e-05\n","Iteration 652: ||grad|| = 1.37540864670882e-05\n","Iteration 653: ||grad|| = 1.3623280210595112e-05\n","Iteration 654: ||grad|| = 1.3495417988451663e-05\n","Iteration 655: ||grad|| = 1.337058529315982e-05\n","Iteration 656: ||grad|| = 1.3248866707726847e-05\n","Iteration 657: ||grad|| = 1.3130349543644115e-05\n","Iteration 658: ||grad|| = 1.3015121112402994e-05\n","Iteration 659: ||grad|| = 1.2903269634989556e-05\n","Iteration 660: ||grad|| = 1.2794883332389873e-05\n","Iteration 661: ||grad|| = 1.2794883332389873e-05\n","Iteration 662: ||grad|| = 1.2794883332389873e-05\n","Iteration 663: ||grad|| = 1.2794883332389873e-05\n","Iteration 664: ||grad|| = 1.2794883332389873e-05\n","Iteration 665: ||grad|| = 1.2794883332389873e-05\n","Iteration 666: ||grad|| = 1.2794883332389873e-05\n","Iteration 667: ||grad|| = 1.2794883332389873e-05\n","Iteration 668: ||grad|| = 1.2794883332389873e-05\n","Iteration 669: ||grad|| = 1.2794883332389873e-05\n","Iteration 670: ||grad|| = 1.2794883332389873e-05\n","Iteration 671: ||grad|| = 1.2794883332389873e-05\n","Iteration 672: ||grad|| = 1.2794883332389873e-05\n","Iteration 673: ||grad|| = 1.2794883332389873e-05\n","Iteration 674: ||grad|| = 1.2794883332389873e-05\n","Iteration 675: ||grad|| = 1.2794883332389873e-05\n","Iteration 676: ||grad|| = 1.2794883332389873e-05\n","Iteration 677: ||grad|| = 1.2794883332389873e-05\n","Iteration 678: ||grad|| = 1.2794883332389873e-05\n","Iteration 679: ||grad|| = 1.2794883332389873e-05\n","Iteration 680: ||grad|| = 1.2794883332389873e-05\n","Iteration 681: ||grad|| = 1.2794883332389873e-05\n","Iteration 682: ||grad|| = 1.2794883332389873e-05\n","Iteration 683: ||grad|| = 1.2794883332389873e-05\n","Iteration 684: ||grad|| = 1.2794883332389873e-05\n","Iteration 685: ||grad|| = 1.2794883332389873e-05\n","Iteration 686: ||grad|| = 1.2794883332389873e-05\n","Iteration 687: ||grad|| = 1.2794883332389873e-05\n","Iteration 688: ||grad|| = 1.2794883332389873e-05\n","Iteration 689: ||grad|| = 1.2794883332389873e-05\n","Iteration 690: ||grad|| = 1.2794883332389873e-05\n","Iteration 691: ||grad|| = 1.2794883332389873e-05\n","Iteration 692: ||grad|| = 1.2794883332389873e-05\n","Iteration 693: ||grad|| = 1.2794883332389873e-05\n","Iteration 694: ||grad|| = 1.2794883332389873e-05\n","Iteration 695: ||grad|| = 1.2794883332389873e-05\n","Iteration 696: ||grad|| = 1.2794883332389873e-05\n","Iteration 697: ||grad|| = 1.2794883332389873e-05\n","Iteration 698: ||grad|| = 1.2794883332389873e-05\n","Iteration 699: ||grad|| = 1.2794883332389873e-05\n","Iteration 700: ||grad|| = 1.2794883332389873e-05\n","Iteration 701: ||grad|| = 1.2794883332389873e-05\n","Iteration 702: ||grad|| = 1.2794883332389873e-05\n","Iteration 703: ||grad|| = 1.2794883332389873e-05\n","Iteration 704: ||grad|| = 1.2794883332389873e-05\n","Iteration 705: ||grad|| = 1.2794883332389873e-05\n","Iteration 706: ||grad|| = 1.2794883332389873e-05\n","Iteration 707: ||grad|| = 1.2794883332389873e-05\n","Iteration 708: ||grad|| = 1.2794883332389873e-05\n","Iteration 709: ||grad|| = 1.2794883332389873e-05\n","Iteration 710: ||grad|| = 1.2794883332389873e-05\n","Iteration 711: ||grad|| = 1.2794883332389873e-05\n","Iteration 712: ||grad|| = 1.2794883332389873e-05\n","Iteration 713: ||grad|| = 1.2794883332389873e-05\n","Iteration 714: ||grad|| = 1.2794883332389873e-05\n","Iteration 715: ||grad|| = 1.2794883332389873e-05\n","Iteration 716: ||grad|| = 1.2794883332389873e-05\n","Iteration 717: ||grad|| = 1.2794883332389873e-05\n","Iteration 718: ||grad|| = 1.2794883332389873e-05\n","Iteration 719: ||grad|| = 1.2794883332389873e-05\n","Iteration 720: ||grad|| = 1.2794883332389873e-05\n","Iteration 721: ||grad|| = 1.2794883332389873e-05\n","Iteration 722: ||grad|| = 1.2794883332389873e-05\n","Iteration 723: ||grad|| = 1.2794883332389873e-05\n","Iteration 724: ||grad|| = 1.2794883332389873e-05\n","Iteration 725: ||grad|| = 1.2794883332389873e-05\n","Iteration 726: ||grad|| = 1.2794883332389873e-05\n","Iteration 727: ||grad|| = 1.2794883332389873e-05\n","Iteration 728: ||grad|| = 1.2794883332389873e-05\n","Iteration 729: ||grad|| = 1.2794883332389873e-05\n","Iteration 730: ||grad|| = 1.2794883332389873e-05\n","Iteration 731: ||grad|| = 1.2794883332389873e-05\n","Iteration 732: ||grad|| = 1.2794883332389873e-05\n","Iteration 733: ||grad|| = 1.2794883332389873e-05\n","Iteration 734: ||grad|| = 1.2794883332389873e-05\n","Iteration 735: ||grad|| = 1.2794883332389873e-05\n","Iteration 736: ||grad|| = 1.2794883332389873e-05\n","Iteration 737: ||grad|| = 1.2794883332389873e-05\n","Iteration 738: ||grad|| = 1.2794883332389873e-05\n","Iteration 739: ||grad|| = 1.2794883332389873e-05\n","Iteration 740: ||grad|| = 1.2794883332389873e-05\n","Iteration 741: ||grad|| = 1.2794883332389873e-05\n","Iteration 742: ||grad|| = 1.2794883332389873e-05\n","Iteration 743: ||grad|| = 1.2794883332389873e-05\n","Iteration 744: ||grad|| = 1.2794883332389873e-05\n","Iteration 745: ||grad|| = 1.2794883332389873e-05\n","Iteration 746: ||grad|| = 1.2794883332389873e-05\n","Iteration 747: ||grad|| = 1.2794883332389873e-05\n","Iteration 748: ||grad|| = 1.2794883332389873e-05\n","Iteration 749: ||grad|| = 1.2794883332389873e-05\n","Iteration 750: ||grad|| = 1.2794883332389873e-05\n","Iteration 751: ||grad|| = 1.2794883332389873e-05\n","Iteration 752: ||grad|| = 1.2794883332389873e-05\n","Iteration 753: ||grad|| = 1.2794883332389873e-05\n","Iteration 754: ||grad|| = 1.2794883332389873e-05\n","Iteration 755: ||grad|| = 1.2794883332389873e-05\n","Iteration 756: ||grad|| = 1.2794883332389873e-05\n","Iteration 757: ||grad|| = 1.2794883332389873e-05\n","Iteration 758: ||grad|| = 1.2794883332389873e-05\n","Iteration 759: ||grad|| = 1.2794883332389873e-05\n","Iteration 760: ||grad|| = 1.2794883332389873e-05\n","Iteration 761: ||grad|| = 1.2794883332389873e-05\n","Iteration 762: ||grad|| = 1.2794883332389873e-05\n","Iteration 763: ||grad|| = 1.2794883332389873e-05\n","Iteration 764: ||grad|| = 1.2794883332389873e-05\n","Iteration 765: ||grad|| = 1.2794883332389873e-05\n","Iteration 766: ||grad|| = 1.2794883332389873e-05\n","Iteration 767: ||grad|| = 1.2794883332389873e-05\n","Iteration 768: ||grad|| = 1.2794883332389873e-05\n","Iteration 769: ||grad|| = 1.2794883332389873e-05\n","Iteration 770: ||grad|| = 1.2794883332389873e-05\n","Iteration 771: ||grad|| = 1.2794883332389873e-05\n","Iteration 772: ||grad|| = 1.2794883332389873e-05\n","Iteration 773: ||grad|| = 1.2794883332389873e-05\n","Iteration 774: ||grad|| = 1.2794883332389873e-05\n","Iteration 775: ||grad|| = 1.2794883332389873e-05\n","Iteration 776: ||grad|| = 1.2794883332389873e-05\n","Iteration 777: ||grad|| = 1.2794883332389873e-05\n","Iteration 778: ||grad|| = 1.2794883332389873e-05\n","Iteration 779: ||grad|| = 1.2794883332389873e-05\n","Iteration 780: ||grad|| = 1.2794883332389873e-05\n","Iteration 781: ||grad|| = 1.2794883332389873e-05\n","Iteration 782: ||grad|| = 1.2794883332389873e-05\n","Iteration 783: ||grad|| = 1.2794883332389873e-05\n","Iteration 784: ||grad|| = 1.2794883332389873e-05\n","Iteration 785: ||grad|| = 1.2794883332389873e-05\n","Iteration 786: ||grad|| = 1.2794883332389873e-05\n","Iteration 787: ||grad|| = 1.2794883332389873e-05\n","Iteration 788: ||grad|| = 1.2794883332389873e-05\n","Iteration 789: ||grad|| = 1.2794883332389873e-05\n","Iteration 790: ||grad|| = 1.2794883332389873e-05\n","Iteration 791: ||grad|| = 1.2794883332389873e-05\n","Iteration 792: ||grad|| = 1.2794883332389873e-05\n","Iteration 793: ||grad|| = 1.2794883332389873e-05\n","Iteration 794: ||grad|| = 1.2794883332389873e-05\n","Iteration 795: ||grad|| = 1.2794883332389873e-05\n","Iteration 796: ||grad|| = 1.2794883332389873e-05\n","Iteration 797: ||grad|| = 1.2794883332389873e-05\n","Iteration 798: ||grad|| = 1.2794883332389873e-05\n","Iteration 799: ||grad|| = 1.2794883332389873e-05\n","Iteration 800: ||grad|| = 1.2794883332389873e-05\n","Iteration 801: ||grad|| = 1.2794883332389873e-05\n","Iteration 802: ||grad|| = 1.2794883332389873e-05\n","Iteration 803: ||grad|| = 1.2794883332389873e-05\n","Iteration 804: ||grad|| = 1.2794883332389873e-05\n","Iteration 805: ||grad|| = 1.2794883332389873e-05\n","Iteration 806: ||grad|| = 1.2794883332389873e-05\n","Iteration 807: ||grad|| = 1.2794883332389873e-05\n","Iteration 808: ||grad|| = 1.2794883332389873e-05\n","Iteration 809: ||grad|| = 1.2794883332389873e-05\n","Iteration 810: ||grad|| = 1.2794883332389873e-05\n","Iteration 811: ||grad|| = 1.2794883332389873e-05\n","Iteration 812: ||grad|| = 1.2794883332389873e-05\n","Iteration 813: ||grad|| = 1.2794883332389873e-05\n","Iteration 814: ||grad|| = 1.2794883332389873e-05\n","Iteration 815: ||grad|| = 1.2794883332389873e-05\n","Iteration 816: ||grad|| = 1.2794883332389873e-05\n","Iteration 817: ||grad|| = 1.2794883332389873e-05\n","Iteration 818: ||grad|| = 1.2794883332389873e-05\n","Iteration 819: ||grad|| = 1.2794883332389873e-05\n","Iteration 820: ||grad|| = 1.2794883332389873e-05\n","Iteration 821: ||grad|| = 1.2794883332389873e-05\n","Iteration 822: ||grad|| = 1.2794883332389873e-05\n","Iteration 823: ||grad|| = 1.2794883332389873e-05\n","Iteration 824: ||grad|| = 1.2794883332389873e-05\n","Iteration 825: ||grad|| = 1.2794883332389873e-05\n","Iteration 826: ||grad|| = 1.2794883332389873e-05\n","Iteration 827: ||grad|| = 1.2794883332389873e-05\n","Iteration 828: ||grad|| = 1.2794883332389873e-05\n","Iteration 829: ||grad|| = 1.2794883332389873e-05\n","Iteration 830: ||grad|| = 1.2794883332389873e-05\n","Iteration 831: ||grad|| = 1.2794883332389873e-05\n","Iteration 832: ||grad|| = 1.2794883332389873e-05\n","Iteration 833: ||grad|| = 1.2794883332389873e-05\n","Iteration 834: ||grad|| = 1.2794883332389873e-05\n","Iteration 835: ||grad|| = 1.2794883332389873e-05\n","Iteration 836: ||grad|| = 1.2794883332389873e-05\n","Iteration 837: ||grad|| = 1.2794883332389873e-05\n","Iteration 838: ||grad|| = 1.2794883332389873e-05\n","Iteration 839: ||grad|| = 1.2794883332389873e-05\n","Iteration 840: ||grad|| = 1.2794883332389873e-05\n","Iteration 841: ||grad|| = 1.2794883332389873e-05\n","Iteration 842: ||grad|| = 1.2794883332389873e-05\n","Iteration 843: ||grad|| = 1.2794883332389873e-05\n","Iteration 844: ||grad|| = 1.2794883332389873e-05\n","Iteration 845: ||grad|| = 1.2794883332389873e-05\n","Iteration 846: ||grad|| = 1.2794883332389873e-05\n","Iteration 847: ||grad|| = 1.2794883332389873e-05\n","Iteration 848: ||grad|| = 1.2794883332389873e-05\n","Iteration 849: ||grad|| = 1.2794883332389873e-05\n","Iteration 850: ||grad|| = 1.2794883332389873e-05\n","Iteration 851: ||grad|| = 1.2794883332389873e-05\n","Iteration 852: ||grad|| = 1.2794883332389873e-05\n","Iteration 853: ||grad|| = 1.2794883332389873e-05\n","Iteration 854: ||grad|| = 1.2794883332389873e-05\n","Iteration 855: ||grad|| = 1.2794883332389873e-05\n","Iteration 856: ||grad|| = 1.2794883332389873e-05\n","Iteration 857: ||grad|| = 1.2794883332389873e-05\n","Iteration 858: ||grad|| = 1.2794883332389873e-05\n","Iteration 859: ||grad|| = 1.2794883332389873e-05\n","Iteration 860: ||grad|| = 1.2794883332389873e-05\n","Iteration 861: ||grad|| = 1.2794883332389873e-05\n","Iteration 862: ||grad|| = 1.2794883332389873e-05\n","Iteration 863: ||grad|| = 1.2794883332389873e-05\n","Iteration 864: ||grad|| = 1.2794883332389873e-05\n","Iteration 865: ||grad|| = 1.2794883332389873e-05\n","Iteration 866: ||grad|| = 1.2794883332389873e-05\n","Iteration 867: ||grad|| = 1.2794883332389873e-05\n","Iteration 868: ||grad|| = 1.2794883332389873e-05\n","Iteration 869: ||grad|| = 1.2794883332389873e-05\n","Iteration 870: ||grad|| = 1.2794883332389873e-05\n","Iteration 871: ||grad|| = 1.2794883332389873e-05\n","Iteration 872: ||grad|| = 1.2794883332389873e-05\n","Iteration 873: ||grad|| = 1.2794883332389873e-05\n","Iteration 874: ||grad|| = 1.2794883332389873e-05\n","Iteration 875: ||grad|| = 1.2794883332389873e-05\n","Iteration 876: ||grad|| = 1.2794883332389873e-05\n","Iteration 877: ||grad|| = 1.2794883332389873e-05\n","Iteration 878: ||grad|| = 1.2794883332389873e-05\n","Iteration 879: ||grad|| = 1.2794883332389873e-05\n","Iteration 880: ||grad|| = 1.2794883332389873e-05\n","Iteration 881: ||grad|| = 1.2794883332389873e-05\n","Iteration 882: ||grad|| = 1.2794883332389873e-05\n","Iteration 883: ||grad|| = 1.2794883332389873e-05\n","Iteration 884: ||grad|| = 1.2794883332389873e-05\n","Iteration 885: ||grad|| = 1.2794883332389873e-05\n","Iteration 886: ||grad|| = 1.2794883332389873e-05\n","Iteration 887: ||grad|| = 1.2794883332389873e-05\n","Iteration 888: ||grad|| = 1.2794883332389873e-05\n","Iteration 889: ||grad|| = 1.2794883332389873e-05\n","Iteration 890: ||grad|| = 1.2794883332389873e-05\n","Iteration 891: ||grad|| = 1.2794883332389873e-05\n","Iteration 892: ||grad|| = 1.2794883332389873e-05\n","Iteration 893: ||grad|| = 1.2794883332389873e-05\n","Iteration 894: ||grad|| = 1.2794883332389873e-05\n","Iteration 895: ||grad|| = 1.2794883332389873e-05\n","Iteration 896: ||grad|| = 1.2794883332389873e-05\n","Iteration 897: ||grad|| = 1.2794883332389873e-05\n","Iteration 898: ||grad|| = 1.2794883332389873e-05\n","Iteration 899: ||grad|| = 1.2794883332389873e-05\n","Iteration 900: ||grad|| = 1.2794883332389873e-05\n","Iteration 901: ||grad|| = 1.2794883332389873e-05\n","Iteration 902: ||grad|| = 1.2794883332389873e-05\n","Iteration 903: ||grad|| = 1.2794883332389873e-05\n","Iteration 904: ||grad|| = 1.2794883332389873e-05\n","Iteration 905: ||grad|| = 1.2794883332389873e-05\n","Iteration 906: ||grad|| = 1.2794883332389873e-05\n","Iteration 907: ||grad|| = 1.2794883332389873e-05\n","Iteration 908: ||grad|| = 1.2794883332389873e-05\n","Iteration 909: ||grad|| = 1.2794883332389873e-05\n","Iteration 910: ||grad|| = 1.2794883332389873e-05\n","Iteration 911: ||grad|| = 1.2794883332389873e-05\n","Iteration 912: ||grad|| = 1.2794883332389873e-05\n","Iteration 913: ||grad|| = 1.2794883332389873e-05\n","Iteration 914: ||grad|| = 1.2794883332389873e-05\n","Iteration 915: ||grad|| = 1.2794883332389873e-05\n","Iteration 916: ||grad|| = 1.2794883332389873e-05\n","Iteration 917: ||grad|| = 1.2794883332389873e-05\n","Iteration 918: ||grad|| = 1.2794883332389873e-05\n","Iteration 919: ||grad|| = 1.2794883332389873e-05\n","Iteration 920: ||grad|| = 1.2794883332389873e-05\n","Iteration 921: ||grad|| = 1.2794883332389873e-05\n","Iteration 922: ||grad|| = 1.2794883332389873e-05\n","Iteration 923: ||grad|| = 1.2794883332389873e-05\n","Iteration 924: ||grad|| = 1.2794883332389873e-05\n","Iteration 925: ||grad|| = 1.2794883332389873e-05\n","Iteration 926: ||grad|| = 1.2794883332389873e-05\n","Iteration 927: ||grad|| = 1.2794883332389873e-05\n","Iteration 928: ||grad|| = 1.2794883332389873e-05\n","Iteration 929: ||grad|| = 1.2794883332389873e-05\n","Iteration 930: ||grad|| = 1.2794883332389873e-05\n","Iteration 931: ||grad|| = 1.2794883332389873e-05\n","Iteration 932: ||grad|| = 1.2794883332389873e-05\n","Iteration 933: ||grad|| = 1.2794883332389873e-05\n","Iteration 934: ||grad|| = 1.2794883332389873e-05\n","Iteration 935: ||grad|| = 1.2794883332389873e-05\n","Iteration 936: ||grad|| = 1.2794883332389873e-05\n","Iteration 937: ||grad|| = 1.2794883332389873e-05\n","Iteration 938: ||grad|| = 1.2794883332389873e-05\n","Iteration 939: ||grad|| = 1.2794883332389873e-05\n","Iteration 940: ||grad|| = 1.2794883332389873e-05\n","Iteration 941: ||grad|| = 1.2794883332389873e-05\n","Iteration 942: ||grad|| = 1.2794883332389873e-05\n","Iteration 943: ||grad|| = 1.2794883332389873e-05\n","Iteration 944: ||grad|| = 1.2794883332389873e-05\n","Iteration 945: ||grad|| = 1.2794883332389873e-05\n","Iteration 946: ||grad|| = 1.2794883332389873e-05\n","Iteration 947: ||grad|| = 1.2794883332389873e-05\n","Iteration 948: ||grad|| = 1.2794883332389873e-05\n","Iteration 949: ||grad|| = 1.2794883332389873e-05\n","Iteration 950: ||grad|| = 1.2794883332389873e-05\n","Iteration 951: ||grad|| = 1.2794883332389873e-05\n","Iteration 952: ||grad|| = 1.2794883332389873e-05\n","Iteration 953: ||grad|| = 1.2794883332389873e-05\n","Iteration 954: ||grad|| = 1.2794883332389873e-05\n","Iteration 955: ||grad|| = 1.2794883332389873e-05\n","Iteration 956: ||grad|| = 1.2794883332389873e-05\n","Iteration 957: ||grad|| = 1.2794883332389873e-05\n","Iteration 958: ||grad|| = 1.2794883332389873e-05\n","Iteration 959: ||grad|| = 1.2794883332389873e-05\n","Iteration 960: ||grad|| = 1.2794883332389873e-05\n","Iteration 961: ||grad|| = 1.2794883332389873e-05\n","Iteration 962: ||grad|| = 1.2794883332389873e-05\n","Iteration 963: ||grad|| = 1.2794883332389873e-05\n","Iteration 964: ||grad|| = 1.2794883332389873e-05\n","Iteration 965: ||grad|| = 1.2794883332389873e-05\n","Iteration 966: ||grad|| = 1.2794883332389873e-05\n","Iteration 967: ||grad|| = 1.2794883332389873e-05\n","Iteration 968: ||grad|| = 1.2794883332389873e-05\n","Iteration 969: ||grad|| = 1.2794883332389873e-05\n","Iteration 970: ||grad|| = 1.2794883332389873e-05\n","Iteration 971: ||grad|| = 1.2794883332389873e-05\n","Iteration 972: ||grad|| = 1.2794883332389873e-05\n","Iteration 973: ||grad|| = 1.2794883332389873e-05\n","Iteration 974: ||grad|| = 1.2794883332389873e-05\n","Iteration 975: ||grad|| = 1.2794883332389873e-05\n","Iteration 976: ||grad|| = 1.2794883332389873e-05\n","Iteration 977: ||grad|| = 1.2794883332389873e-05\n","Iteration 978: ||grad|| = 1.2794883332389873e-05\n","Iteration 979: ||grad|| = 1.2794883332389873e-05\n","Iteration 980: ||grad|| = 1.2794883332389873e-05\n","Iteration 981: ||grad|| = 1.2794883332389873e-05\n","Iteration 982: ||grad|| = 1.2794883332389873e-05\n","Iteration 983: ||grad|| = 1.2794883332389873e-05\n","Iteration 984: ||grad|| = 1.2794883332389873e-05\n","Iteration 985: ||grad|| = 1.2794883332389873e-05\n","Iteration 986: ||grad|| = 1.2794883332389873e-05\n","Iteration 987: ||grad|| = 1.2794883332389873e-05\n","Iteration 988: ||grad|| = 1.2794883332389873e-05\n","Iteration 989: ||grad|| = 1.2794883332389873e-05\n","Iteration 990: ||grad|| = 1.2794883332389873e-05\n","Iteration 991: ||grad|| = 1.2794883332389873e-05\n","Iteration 992: ||grad|| = 1.2794883332389873e-05\n","Iteration 993: ||grad|| = 1.2794883332389873e-05\n","Iteration 994: ||grad|| = 1.2794883332389873e-05\n","Iteration 995: ||grad|| = 1.2794883332389873e-05\n","Iteration 996: ||grad|| = 1.2794883332389873e-05\n","Iteration 997: ||grad|| = 1.2794883332389873e-05\n","Iteration 998: ||grad|| = 1.2794883332389873e-05\n","Iteration 999: ||grad|| = 1.2794883332389873e-05\n","Iteration 1000: ||grad|| = 1.2794883332389873e-05\n"]}]},{"cell_type":"code","source":["w"],"metadata":{"id":"zpoqly8DE_g-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","We will consider again the same toy data matrix with 6 samples and 3 possible output labels :\n","\n","\n","![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=660)\n","\n","---"],"metadata":{"id":"IvRw58l8p2T3"}},{"cell_type":"markdown","source":["---\n","\n","Define the linear layer (dense layer) where the raw scores are calculated through the linear operation:\n","$$\\underbrace{\\mathbf{Z}}_{\\color{red}{6\\times3}} = \\underbrace{\\begin{bmatrix}{\\mathbf{z}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(1)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(2)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(3)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(4)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(5)}}^\\mathrm{T}\\end{bmatrix}}_{\\color{red}{6\\times3}}=\\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}} = \\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\mathbf{W}\\end{bmatrix}}_{\\color{red}{6\\times3}} + \\underbrace{\\begin{bmatrix}{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}}=\\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\end{bmatrix}}_{\\color{red}{6\\times5}}\\underbrace{\\mathbf{W}}_{\\color{red}{5\\times3}}=\\underbrace{\\underbrace{\\mathbf{X}}_{6\\times 5}\\underbrace{\\mathbf{W}}_{5\\times 3}}_{\\color{red}{6\\times3}} + \\underbrace{\\begin{bmatrix}{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}}.$$\n","\n","---"],"metadata":{"id":"1ylibPssqTWs"}},{"cell_type":"code","source":["class LinearLayer(torch.nn.Module):\n","    def __init__(self, input_dim, nodes = 2):\n","        super(LinearLayer, self).__init__()  # Initialize the parent class (nn.Module)\n","        self.nodes = nodes\n","        # Define the weights and bias as parameters\n","        self.W = torch.nn.Parameter(torch.randn(input_dim, self.nodes))\n","        torch.nn.init.xavier_uniform_(self.W)  # Xavier uniform initialization\n","        self.b = torch.nn.Parameter(torch.randn(self.nodes))  # Random Normal initialization\n","\n","    def forward(self, input):\n","        # Linear transformation (input * W + b)\n","        output = torch.matmul(input, self.W) + self.b\n","        return output"],"metadata":{"id":"vt14T0bgqUxF","executionInfo":{"status":"ok","timestamp":1733653363693,"user_tz":-330,"elapsed":495,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Defining a LinearLayer object and calling the forward() method applied to the toy patient data matrix.\n","\n","---"],"metadata":{"id":"4Sb0TWaWqosF"}},{"cell_type":"code","source":["layer1 = LinearLayer(num_features, 3)\n","print(layer1.W)\n","print(layer1.b)\n","layer1.forward(torch.tensor(X_S, dtype = torch.float32))"],"metadata":{"id":"XNrvxsYgqpve","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733654023491,"user_tz":-330,"elapsed":487,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"b761bbea-9615-4dd0-97c9-673ee1b493af"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 7.4733e-01, -4.1120e-01,  7.8882e-01],\n","        [ 8.5585e-01,  4.4081e-01, -6.9085e-01],\n","        [-2.0481e-01, -1.4040e-01, -2.1422e-04],\n","        [ 7.9504e-01, -1.1777e-01, -1.0940e-01],\n","        [-5.3541e-01, -1.3967e-01, -3.8278e-01]], requires_grad=True)\n","Parameter containing:\n","tensor([0.6176, 1.0290, 0.2699], requires_grad=True)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-32-f9419f0eff11>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  layer1.forward(torch.tensor(X_S, dtype = torch.float32))\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.8295,  1.2158, -0.2514],\n","        [ 1.8915,  1.3071,  1.1344],\n","        [-2.5772,  0.4924, -0.1492],\n","        [ 3.1980,  0.9158,  0.2540],\n","        [ 2.3482,  0.6094, -0.0703],\n","        [ 0.6749,  1.6337,  0.7022]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["---\n","\n","Define the softmax layer\n","\n","---"],"metadata":{"id":"4P8i-XsrroCB"}},{"cell_type":"code","source":["class SoftmaxLayer(torch.nn.Module):\n","    def __init__(self):\n","        super(SoftmaxLayer, self).__init__()\n","        self.activation = torch.nn.Softmax(dim = 1)\n","\n","    def forward(self, input):\n","        output = self.activation(input)  # Apply softmax activation\n","        return output"],"metadata":{"id":"DoACMi5FrokZ","executionInfo":{"status":"ok","timestamp":1733654027496,"user_tz":-330,"elapsed":495,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Defining a SoftmaxLayer object and calling the forward() method applied to the toy patient data matrix.\n","\n","---"],"metadata":{"id":"wiSxk97fr4xJ"}},{"cell_type":"code","source":["actlayer1 = SoftmaxLayer()\n","print(actlayer1.activation)\n","actlayer1.forward(layer1.forward(torch.tensor(X_S, dtype = torch.float32)))"],"metadata":{"id":"pn_dXu7xr7HV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733654169236,"user_tz":-330,"elapsed":489,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"553ef332-2211-4fd6-c86b-ea472e2e2a4c"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Softmax(dim=1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-34-b77cc0f53c71>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  actlayer1.forward(layer1.forward(torch.tensor(X_S, dtype = torch.float32)))\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0372, 0.7824, 0.1804],\n","        [0.4935, 0.2751, 0.2315],\n","        [0.0295, 0.6358, 0.3347],\n","        [0.8660, 0.0884, 0.0456],\n","        [0.7906, 0.1389, 0.0704],\n","        [0.2157, 0.5627, 0.2217]], grad_fn=<SoftmaxBackward0>)"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["---\n","\n","Define the softmax classifier model\n","\n","---"],"metadata":{"id":"JRQ0SwBxs3WL"}},{"cell_type":"code","source":["class SoftmaxClassifierModel(torch.nn.Module):\n","    def __init__(self, input_dim, nodes=2):\n","        super(SoftmaxClassifierModel, self).__init__()\n","        self.nodes = nodes\n","        self.linearLayer = LinearLayer(input_dim, self.nodes)  # Linear layer\n","        self.softmaxLayer = SoftmaxLayer()  # Softmax activation layer\n","\n","    def forward(self, input):\n","        output = self.linearLayer(input)  # Forward pass through the linear layer\n","        output = self.softmaxLayer(output)  # Apply softmax activation\n","        return output"],"metadata":{"id":"tKicexaas31_","executionInfo":{"status":"ok","timestamp":1733654281482,"user_tz":-330,"elapsed":524,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["model = SoftmaxClassifierModel(num_features, 3)\n","print(model.linearLayer.W)\n","print(model.linearLayer.b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DWWNqi432lE2","executionInfo":{"status":"ok","timestamp":1733654409146,"user_tz":-330,"elapsed":480,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"2498ea46-e1f1-4320-da63-d0a7323bd29d"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.6159, -0.7300, -0.7334],\n","        [ 0.4535,  0.0148, -0.2483],\n","        [-0.0698,  0.2613, -0.0265],\n","        [ 0.6619,  0.2710, -0.4630],\n","        [ 0.2722,  0.0659, -0.7571]], requires_grad=True)\n","Parameter containing:\n","tensor([-0.4045, -0.1053,  0.4219], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Perform forward propagation to the toy patient dataset using the SoftmaxClassifierModel built above.\n","\n","---"],"metadata":{"id":"upcq6QXttB8y"}},{"cell_type":"code","source":["model = SoftmaxClassifierModel(num_features, 3)\n","print(model(torch.tensor(X_S, dtype = torch.float32)))"],"metadata":{"id":"JQIET4VFtNSA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733654360473,"user_tz":-330,"elapsed":524,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"6b3b4461-a79d-43bc-9044-3173ec35ea4c"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.5525, 0.3197, 0.1277],\n","        [0.3217, 0.3844, 0.2939],\n","        [0.6789, 0.2389, 0.0822],\n","        [0.8029, 0.1679, 0.0292],\n","        [0.8249, 0.1462, 0.0289],\n","        [0.2551, 0.3829, 0.3620]], grad_fn=<SoftmaxBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-36-9c499bf3670e>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  print(model(torch.tensor(X_S, dtype = torch.float32)))\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Define loss function (categorical crossentropy).\n","\n","---"],"metadata":{"id":"V3G_qHiVtWvj"}},{"cell_type":"code","source":["def loss_fn(true_labels, predicted_probs):\n","  loss = torch.mean(-torch.log(torch.sum(true_labels * predicted_probs, dim = 1)))\n","  return(loss)"],"metadata":{"id":"t-QsTVjRtYAh","executionInfo":{"status":"ok","timestamp":1733654496857,"user_tz":-330,"elapsed":482,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Apply the softmax classifier model to the toy data set and calculate the loss.\n","\n","---"],"metadata":{"id":"_LpnJT06u5KV"}},{"cell_type":"code","source":["## Apply the softmax classifier model to the toy data set and calculate the loss\n","# Instantiate the model object\n","model = SoftmaxClassifierModel(num_features, 3) # invokes the constructor and sets up the layers\n","\n","# Calculate average data loss\n","loss_fn(Y, model(torch.tensor(X_S, dtype = torch.float32)))"],"metadata":{"id":"ih-XPwbdu5my","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733654692939,"user_tz":-330,"elapsed":549,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"42a90ead-fb07-4991-875b-e3bce67c2e9b"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-42-5241620b6a24>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss_fn(Y, model(torch.tensor(X_S, dtype = torch.float32)))\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(1.0896, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["---\n","\n","Softmax classifier for the [MNIST](https://www.tensorflow.org/datasets/catalog/mnist) dataset\n","\n","---"],"metadata":{"id":"s4A6iMHOzAAO"}},{"cell_type":"code","source":["## Load MNIST data (note that shape of X_train and y_train)\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","print(X_train.shape)\n","print(y_train.shape)"],"metadata":{"id":"Q6Wc0cWFzI7X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733654697827,"user_tz":-330,"elapsed":481,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"110eb1b6-e44d-44ef-f617-67e42b39140f"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n","(60000,)\n"]}]},{"cell_type":"code","source":["## Reshape X_train and X_test such that the samples are along the rows\n","X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n","X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"],"metadata":{"id":"cP47GmErzKzV","executionInfo":{"status":"ok","timestamp":1733654701384,"user_tz":-330,"elapsed":715,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["## Problem parameters\n","num_samples_train = X_train_reshaped.shape[0]\n","num_samples_test = X_test_reshaped.shape[0]\n","num_features = X_train_reshaped.shape[1]\n","num_labels = len(np.unique(y_train))\n","print(f'No. of training samples = {num_samples_train},\\\n"," No. of test samples = {num_samples_test}, \\\n"," no. of features = {num_features}, no. of labels = {num_labels}')"],"metadata":{"id":"z54AQD5rzO12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733654706927,"user_tz":-330,"elapsed":565,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"500f70d2-5d37-47cf-e991-6e131b01e1a9"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["No. of training samples = 60000, No. of test samples = 10000,  no. of features = 784, no. of labels = 10\n"]}]},{"cell_type":"code","source":["## One-hot encode output labels using scikit-learn (observe the shape of Y_train)\n","ohe = OneHotEncoder(sparse_output=False)\n","Y_train = torch.tensor(ohe.fit_transform(y_train.reshape(-1, 1)), dtype = torch.float32)\n","Y_test = torch.tensor(ohe.transform(y_test.reshape(-1, 1)), dtype = torch.float32)"],"metadata":{"id":"yeTfB5-xzSGi","executionInfo":{"status":"ok","timestamp":1733655326050,"user_tz":-330,"elapsed":467,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["## Min-max scale the images using scikit-learn\n","mms = MinMaxScaler()\n","X_train_reshaped_scaled = torch.tensor(mms.fit_transform(X_train_reshaped), dtype=torch.float32)\n","X_test_reshaped_scaled = torch.tensor(mms.transform(X_test_reshaped), dtype=torch.float32)"],"metadata":{"id":"FCD_yzX1zUE0","executionInfo":{"status":"ok","timestamp":1733655329133,"user_tz":-330,"elapsed":1005,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Train the softmax classifier on the MNIST dataset\n","\n","---"],"metadata":{"id":"2OpSKsccees6"}},{"cell_type":"code","source":["## Train the softmax classifier on the MNIST dataset\n","# Initialize model\n","model = SoftmaxClassifierModel(num_features, num_labels)\n","\n","# Gradient descent\n","maxiter = 250\n","lr = 1e-03\n","\n","# Define loss function (CrossEntropyLoss in PyTorch includes softmax)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer (RMSprop)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n","\n","# Lists to store training and test losses\n","loss_train = [None] * maxiter\n","loss_test = [None] * maxiter\n","\n","# Start training loop\n","for k in range(maxiter):\n","    model.train()  # Set model to training mode\n","\n","    # Forward pass: compute predicted probabilities\n","    Yhat = model(X_train_reshaped_scaled)  # predicted probabilities\n","\n","    # Compute training loss\n","    L_train = loss_fn(Y_train, Yhat)  # CrossEntropyLoss expects raw logits (no softmax needed)\n","\n","    # Append training and test loss values\n","    loss_train[k] = L_train.item()  # Convert to scalar\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation for testing\n","        Yhat_test = model(X_test_reshaped_scaled)  # predicted probabilities for test set\n","        L_test = loss_fn(Y_test, Yhat_test)  # Compute test loss\n","    loss_test[k] = L_test.item()  # Convert to scalar\n","\n","    # Print losses\n","    print(f'Iteration {k+1}, Training loss = {loss_train[k]}, Test loss = {loss_test[k]}')\n","\n","    # Backward pass: compute gradients\n","    optimizer.zero_grad()  # Zero the gradients before the backward pass\n","    L_train.backward()  # Backpropagate the gradients\n","\n","    # Update model parameters using optimizer\n","    optimizer.step()  # Perform one optimization step"],"metadata":{"id":"00eKgH6ez7Ix","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733655384985,"user_tz":-330,"elapsed":40485,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"b05e20d2-b44d-4a38-933f-a82bfa527f7d"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, Training loss = 2.3665847778320312, Test loss = 2.366879463195801\n","Iteration 2, Training loss = 2.308274745941162, Test loss = 2.3077452182769775\n","Iteration 3, Training loss = 2.2478291988372803, Test loss = 2.247987985610962\n","Iteration 4, Training loss = 2.192112922668457, Test loss = 2.1906931400299072\n","Iteration 5, Training loss = 2.139152765274048, Test loss = 2.1356472969055176\n","Iteration 6, Training loss = 2.088893413543701, Test loss = 2.0826594829559326\n","Iteration 7, Training loss = 2.046062707901001, Test loss = 2.03859543800354\n","Iteration 8, Training loss = 2.0127193927764893, Test loss = 2.004368782043457\n","Iteration 9, Training loss = 1.9859257936477661, Test loss = 1.9771190881729126\n","Iteration 10, Training loss = 1.963690161705017, Test loss = 1.9545292854309082\n","Iteration 11, Training loss = 1.944726824760437, Test loss = 1.9353094100952148\n","Iteration 12, Training loss = 1.928168535232544, Test loss = 1.9185277223587036\n","Iteration 13, Training loss = 1.9133996963500977, Test loss = 1.9035332202911377\n","Iteration 14, Training loss = 1.8999910354614258, Test loss = 1.889880657196045\n","Iteration 15, Training loss = 1.8876900672912598, Test loss = 1.8773174285888672\n","Iteration 16, Training loss = 1.876393437385559, Test loss = 1.8657746315002441\n","Iteration 17, Training loss = 1.866045355796814, Test loss = 1.8552331924438477\n","Iteration 18, Training loss = 1.8565559387207031, Test loss = 1.8456159830093384\n","Iteration 19, Training loss = 1.847820520401001, Test loss = 1.8368046283721924\n","Iteration 20, Training loss = 1.8397473096847534, Test loss = 1.8286904096603394\n","Iteration 21, Training loss = 1.8322595357894897, Test loss = 1.8211859464645386\n","Iteration 22, Training loss = 1.8252918720245361, Test loss = 1.8142179250717163\n","Iteration 23, Training loss = 1.8187874555587769, Test loss = 1.8077260255813599\n","Iteration 24, Training loss = 1.812696933746338, Test loss = 1.8016574382781982\n","Iteration 25, Training loss = 1.8069781064987183, Test loss = 1.7959673404693604\n","Iteration 26, Training loss = 1.8015944957733154, Test loss = 1.7906173467636108\n","Iteration 27, Training loss = 1.7965129613876343, Test loss = 1.7855734825134277\n","Iteration 28, Training loss = 1.7917062044143677, Test loss = 1.7808064222335815\n","Iteration 29, Training loss = 1.7871493101119995, Test loss = 1.7762913703918457\n","Iteration 30, Training loss = 1.782820463180542, Test loss = 1.7720056772232056\n","Iteration 31, Training loss = 1.7787007093429565, Test loss = 1.7679299116134644\n","Iteration 32, Training loss = 1.7747728824615479, Test loss = 1.7640464305877686\n","Iteration 33, Training loss = 1.7710217237472534, Test loss = 1.7603402137756348\n","Iteration 34, Training loss = 1.7674341201782227, Test loss = 1.7567973136901855\n","Iteration 35, Training loss = 1.763998031616211, Test loss = 1.7534056901931763\n","Iteration 36, Training loss = 1.7607024908065796, Test loss = 1.7501541376113892\n","Iteration 37, Training loss = 1.757537603378296, Test loss = 1.7470329999923706\n","Iteration 38, Training loss = 1.7544949054718018, Test loss = 1.7440338134765625\n","Iteration 39, Training loss = 1.7515664100646973, Test loss = 1.7411478757858276\n","Iteration 40, Training loss = 1.7487448453903198, Test loss = 1.7383679151535034\n","Iteration 41, Training loss = 1.746023416519165, Test loss = 1.7356877326965332\n","Iteration 42, Training loss = 1.743396520614624, Test loss = 1.7331010103225708\n","Iteration 43, Training loss = 1.7408584356307983, Test loss = 1.7306023836135864\n","Iteration 44, Training loss = 1.7384041547775269, Test loss = 1.7281864881515503\n","Iteration 45, Training loss = 1.7360289096832275, Test loss = 1.72584867477417\n","Iteration 46, Training loss = 1.7337286472320557, Test loss = 1.7235850095748901\n","Iteration 47, Training loss = 1.731499433517456, Test loss = 1.7213914394378662\n","Iteration 48, Training loss = 1.7293373346328735, Test loss = 1.7192639112472534\n","Iteration 49, Training loss = 1.7272393703460693, Test loss = 1.7171992063522339\n","Iteration 50, Training loss = 1.7252018451690674, Test loss = 1.715194582939148\n","Iteration 51, Training loss = 1.723222017288208, Test loss = 1.713246464729309\n","Iteration 52, Training loss = 1.7212975025177002, Test loss = 1.7113523483276367\n","Iteration 53, Training loss = 1.7194252014160156, Test loss = 1.7095099687576294\n","Iteration 54, Training loss = 1.7176029682159424, Test loss = 1.7077165842056274\n","Iteration 55, Training loss = 1.715828776359558, Test loss = 1.705970287322998\n","Iteration 56, Training loss = 1.7141001224517822, Test loss = 1.7042686939239502\n","Iteration 57, Training loss = 1.7124152183532715, Test loss = 1.7026100158691406\n","Iteration 58, Training loss = 1.710772156715393, Test loss = 1.7009923458099365\n","Iteration 59, Training loss = 1.7091690301895142, Test loss = 1.6994140148162842\n","Iteration 60, Training loss = 1.7076044082641602, Test loss = 1.697873592376709\n","Iteration 61, Training loss = 1.7060763835906982, Test loss = 1.6963695287704468\n","Iteration 62, Training loss = 1.7045841217041016, Test loss = 1.694899559020996\n","Iteration 63, Training loss = 1.7031255960464478, Test loss = 1.693463683128357\n","Iteration 64, Training loss = 1.7016997337341309, Test loss = 1.6920593976974487\n","Iteration 65, Training loss = 1.7003053426742554, Test loss = 1.6906863451004028\n","Iteration 66, Training loss = 1.6989411115646362, Test loss = 1.689342975616455\n","Iteration 67, Training loss = 1.697606086730957, Test loss = 1.68802809715271\n","Iteration 68, Training loss = 1.6962990760803223, Test loss = 1.6867409944534302\n","Iteration 69, Training loss = 1.695019006729126, Test loss = 1.6854804754257202\n","Iteration 70, Training loss = 1.6937648057937622, Test loss = 1.6842453479766846\n","Iteration 71, Training loss = 1.6925361156463623, Test loss = 1.6830354928970337\n","Iteration 72, Training loss = 1.6913312673568726, Test loss = 1.6818493604660034\n","Iteration 73, Training loss = 1.690150260925293, Test loss = 1.6806864738464355\n","Iteration 74, Training loss = 1.6889914274215698, Test loss = 1.6795458793640137\n","Iteration 75, Training loss = 1.6878550052642822, Test loss = 1.678426742553711\n","Iteration 76, Training loss = 1.6867393255233765, Test loss = 1.6773289442062378\n","Iteration 77, Training loss = 1.6856441497802734, Test loss = 1.6762511730194092\n","Iteration 78, Training loss = 1.684569001197815, Test loss = 1.6751933097839355\n","Iteration 79, Training loss = 1.6835126876831055, Test loss = 1.674154281616211\n","Iteration 80, Training loss = 1.682475209236145, Test loss = 1.6731336116790771\n","Iteration 81, Training loss = 1.6814557313919067, Test loss = 1.672130823135376\n","Iteration 82, Training loss = 1.6804536581039429, Test loss = 1.6711453199386597\n","Iteration 83, Training loss = 1.6794685125350952, Test loss = 1.6701767444610596\n","Iteration 84, Training loss = 1.6784998178482056, Test loss = 1.6692246198654175\n","Iteration 85, Training loss = 1.6775469779968262, Test loss = 1.668288230895996\n","Iteration 86, Training loss = 1.6766096353530884, Test loss = 1.6673672199249268\n","Iteration 87, Training loss = 1.675687313079834, Test loss = 1.6664611101150513\n","Iteration 88, Training loss = 1.6747798919677734, Test loss = 1.665569543838501\n","Iteration 89, Training loss = 1.6738864183425903, Test loss = 1.6646921634674072\n","Iteration 90, Training loss = 1.6730066537857056, Test loss = 1.663828730583191\n","Iteration 91, Training loss = 1.67214035987854, Test loss = 1.662978172302246\n","Iteration 92, Training loss = 1.6712872982025146, Test loss = 1.66214120388031\n","Iteration 93, Training loss = 1.670446753501892, Test loss = 1.6613163948059082\n","Iteration 94, Training loss = 1.6696186065673828, Test loss = 1.6605043411254883\n","Iteration 95, Training loss = 1.6688024997711182, Test loss = 1.6597040891647339\n","Iteration 96, Training loss = 1.6679980754852295, Test loss = 1.658915638923645\n","Iteration 97, Training loss = 1.6672052145004272, Test loss = 1.658138632774353\n","Iteration 98, Training loss = 1.6664234399795532, Test loss = 1.6573724746704102\n","Iteration 99, Training loss = 1.6656523942947388, Test loss = 1.6566174030303955\n","Iteration 100, Training loss = 1.6648920774459839, Test loss = 1.6558728218078613\n","Iteration 101, Training loss = 1.6641420125961304, Test loss = 1.6551384925842285\n","Iteration 102, Training loss = 1.6634020805358887, Test loss = 1.6544142961502075\n","Iteration 103, Training loss = 1.6626719236373901, Test loss = 1.6536999940872192\n","Iteration 104, Training loss = 1.6619510650634766, Test loss = 1.6529951095581055\n","Iteration 105, Training loss = 1.6612399816513062, Test loss = 1.6522996425628662\n","Iteration 106, Training loss = 1.6605380773544312, Test loss = 1.651613473892212\n","Iteration 107, Training loss = 1.6598447561264038, Test loss = 1.6509361267089844\n","Iteration 108, Training loss = 1.6591602563858032, Test loss = 1.6502676010131836\n","Iteration 109, Training loss = 1.6584844589233398, Test loss = 1.6496071815490723\n","Iteration 110, Training loss = 1.657817006111145, Test loss = 1.6489557027816772\n","Iteration 111, Training loss = 1.65715754032135, Test loss = 1.6483120918273926\n","Iteration 112, Training loss = 1.6565062999725342, Test loss = 1.6476765871047974\n","Iteration 113, Training loss = 1.6558623313903809, Test loss = 1.6470484733581543\n","Iteration 114, Training loss = 1.6552263498306274, Test loss = 1.6464284658432007\n","Iteration 115, Training loss = 1.6545977592468262, Test loss = 1.6458158493041992\n","Iteration 116, Training loss = 1.6539764404296875, Test loss = 1.6452101469039917\n","Iteration 117, Training loss = 1.6533623933792114, Test loss = 1.6446120738983154\n","Iteration 118, Training loss = 1.6527553796768188, Test loss = 1.644020676612854\n","Iteration 119, Training loss = 1.6521549224853516, Test loss = 1.6434365510940552\n","Iteration 120, Training loss = 1.6515611410140991, Test loss = 1.642858624458313\n","Iteration 121, Training loss = 1.6509743928909302, Test loss = 1.6422874927520752\n","Iteration 122, Training loss = 1.6503934860229492, Test loss = 1.6417230367660522\n","Iteration 123, Training loss = 1.649819254875183, Test loss = 1.6411648988723755\n","Iteration 124, Training loss = 1.6492513418197632, Test loss = 1.6406127214431763\n","Iteration 125, Training loss = 1.6486890316009521, Test loss = 1.6400666236877441\n","Iteration 126, Training loss = 1.6481329202651978, Test loss = 1.639526605606079\n","Iteration 127, Training loss = 1.647582769393921, Test loss = 1.638992428779602\n","Iteration 128, Training loss = 1.6470381021499634, Test loss = 1.6384638547897339\n","Iteration 129, Training loss = 1.6464992761611938, Test loss = 1.6379410028457642\n","Iteration 130, Training loss = 1.645965576171875, Test loss = 1.6374237537384033\n","Iteration 131, Training loss = 1.6454377174377441, Test loss = 1.6369118690490723\n","Iteration 132, Training loss = 1.644914984703064, Test loss = 1.636405110359192\n","Iteration 133, Training loss = 1.6443971395492554, Test loss = 1.6359037160873413\n","Iteration 134, Training loss = 1.6438847780227661, Test loss = 1.6354074478149414\n","Iteration 135, Training loss = 1.6433771848678589, Test loss = 1.6349163055419922\n","Iteration 136, Training loss = 1.6428747177124023, Test loss = 1.634429931640625\n","Iteration 137, Training loss = 1.642377257347107, Test loss = 1.6339484453201294\n","Iteration 138, Training loss = 1.641884207725525, Test loss = 1.6334717273712158\n","Iteration 139, Training loss = 1.6413958072662354, Test loss = 1.6329997777938843\n","Iteration 140, Training loss = 1.6409120559692383, Test loss = 1.6325323581695557\n","Iteration 141, Training loss = 1.6404329538345337, Test loss = 1.6320695877075195\n","Iteration 142, Training loss = 1.6399580240249634, Test loss = 1.6316109895706177\n","Iteration 143, Training loss = 1.6394875049591064, Test loss = 1.6311569213867188\n","Iteration 144, Training loss = 1.639021396636963, Test loss = 1.6307072639465332\n","Iteration 145, Training loss = 1.638559341430664, Test loss = 1.6302616596221924\n","Iteration 146, Training loss = 1.638101577758789, Test loss = 1.6298201084136963\n","Iteration 147, Training loss = 1.6376478672027588, Test loss = 1.6293829679489136\n","Iteration 148, Training loss = 1.6371979713439941, Test loss = 1.6289496421813965\n","Iteration 149, Training loss = 1.6367522478103638, Test loss = 1.6285202503204346\n","Iteration 150, Training loss = 1.63631010055542, Test loss = 1.6280947923660278\n","Iteration 151, Training loss = 1.6358720064163208, Test loss = 1.6276732683181763\n","Iteration 152, Training loss = 1.6354376077651978, Test loss = 1.6272553205490112\n","Iteration 153, Training loss = 1.6350069046020508, Test loss = 1.6268411874771118\n","Iteration 154, Training loss = 1.6345797777175903, Test loss = 1.6264305114746094\n","Iteration 155, Training loss = 1.6341562271118164, Test loss = 1.626023769378662\n","Iteration 156, Training loss = 1.6337363719940186, Test loss = 1.6256203651428223\n","Iteration 157, Training loss = 1.6333197355270386, Test loss = 1.625220537185669\n","Iteration 158, Training loss = 1.6329067945480347, Test loss = 1.6248241662979126\n","Iteration 159, Training loss = 1.6324968338012695, Test loss = 1.6244310140609741\n","Iteration 160, Training loss = 1.6320903301239014, Test loss = 1.6240410804748535\n","Iteration 161, Training loss = 1.6316872835159302, Test loss = 1.6236547231674194\n","Iteration 162, Training loss = 1.6312872171401978, Test loss = 1.6232714653015137\n","Iteration 163, Training loss = 1.6308904886245728, Test loss = 1.6228914260864258\n","Iteration 164, Training loss = 1.6304967403411865, Test loss = 1.6225144863128662\n","Iteration 165, Training loss = 1.630105972290039, Test loss = 1.622140645980835\n","Iteration 166, Training loss = 1.62971830368042, Test loss = 1.6217697858810425\n","Iteration 167, Training loss = 1.6293338537216187, Test loss = 1.6214019060134888\n","Iteration 168, Training loss = 1.6289523839950562, Test loss = 1.6210371255874634\n","Iteration 169, Training loss = 1.6285734176635742, Test loss = 1.6206750869750977\n","Iteration 170, Training loss = 1.6281976699829102, Test loss = 1.6203160285949707\n","Iteration 171, Training loss = 1.6278246641159058, Test loss = 1.6199597120285034\n","Iteration 172, Training loss = 1.627454161643982, Test loss = 1.6196064949035645\n","Iteration 173, Training loss = 1.6270866394042969, Test loss = 1.619255542755127\n","Iteration 174, Training loss = 1.626721739768982, Test loss = 1.6189079284667969\n","Iteration 175, Training loss = 1.6263595819473267, Test loss = 1.6185624599456787\n","Iteration 176, Training loss = 1.6260000467300415, Test loss = 1.6182199716567993\n","Iteration 177, Training loss = 1.625643014907837, Test loss = 1.6178799867630005\n","Iteration 178, Training loss = 1.625288486480713, Test loss = 1.6175425052642822\n","Iteration 179, Training loss = 1.6249366998672485, Test loss = 1.617207646369934\n","Iteration 180, Training loss = 1.6245872974395752, Test loss = 1.6168752908706665\n","Iteration 181, Training loss = 1.6242403984069824, Test loss = 1.6165452003479004\n","Iteration 182, Training loss = 1.6238958835601807, Test loss = 1.616217851638794\n","Iteration 183, Training loss = 1.62355375289917, Test loss = 1.6158928871154785\n","Iteration 184, Training loss = 1.6232140064239502, Test loss = 1.615570068359375\n","Iteration 185, Training loss = 1.622876524925232, Test loss = 1.615249752998352\n","Iteration 186, Training loss = 1.6225415468215942, Test loss = 1.6149318218231201\n","Iteration 187, Training loss = 1.622208833694458, Test loss = 1.6146160364151\n","Iteration 188, Training loss = 1.6218781471252441, Test loss = 1.614302635192871\n","Iteration 189, Training loss = 1.6215498447418213, Test loss = 1.6139912605285645\n","Iteration 190, Training loss = 1.6212235689163208, Test loss = 1.6136823892593384\n","Iteration 191, Training loss = 1.6208999156951904, Test loss = 1.6133755445480347\n","Iteration 192, Training loss = 1.6205778121948242, Test loss = 1.6130708456039429\n","Iteration 193, Training loss = 1.6202583312988281, Test loss = 1.6127684116363525\n","Iteration 194, Training loss = 1.6199405193328857, Test loss = 1.612467885017395\n","Iteration 195, Training loss = 1.6196249723434448, Test loss = 1.6121693849563599\n","Iteration 196, Training loss = 1.6193115711212158, Test loss = 1.6118731498718262\n","Iteration 197, Training loss = 1.61899995803833, Test loss = 1.6115787029266357\n","Iteration 198, Training loss = 1.6186904907226562, Test loss = 1.6112866401672363\n","Iteration 199, Training loss = 1.6183828115463257, Test loss = 1.6109963655471802\n","Iteration 200, Training loss = 1.6180773973464966, Test loss = 1.6107078790664673\n","Iteration 201, Training loss = 1.6177736520767212, Test loss = 1.6104215383529663\n","Iteration 202, Training loss = 1.6174719333648682, Test loss = 1.610136866569519\n","Iteration 203, Training loss = 1.6171720027923584, Test loss = 1.6098544597625732\n","Iteration 204, Training loss = 1.6168739795684814, Test loss = 1.609573483467102\n","Iteration 205, Training loss = 1.6165777444839478, Test loss = 1.6092946529388428\n","Iteration 206, Training loss = 1.6162834167480469, Test loss = 1.6090176105499268\n","Iteration 207, Training loss = 1.6159908771514893, Test loss = 1.6087422370910645\n","Iteration 208, Training loss = 1.615700125694275, Test loss = 1.6084686517715454\n","Iteration 209, Training loss = 1.6154109239578247, Test loss = 1.6081968545913696\n","Iteration 210, Training loss = 1.6151233911514282, Test loss = 1.6079269647598267\n","Iteration 211, Training loss = 1.6148380041122437, Test loss = 1.6076587438583374\n","Iteration 212, Training loss = 1.6145540475845337, Test loss = 1.6073919534683228\n","Iteration 213, Training loss = 1.6142719984054565, Test loss = 1.6071271896362305\n","Iteration 214, Training loss = 1.613991379737854, Test loss = 1.6068638563156128\n","Iteration 215, Training loss = 1.6137125492095947, Test loss = 1.6066021919250488\n","Iteration 216, Training loss = 1.613434910774231, Test loss = 1.6063424348831177\n","Iteration 217, Training loss = 1.6131595373153687, Test loss = 1.6060841083526611\n","Iteration 218, Training loss = 1.6128851175308228, Test loss = 1.6058272123336792\n","Iteration 219, Training loss = 1.6126128435134888, Test loss = 1.60557222366333\n","Iteration 220, Training loss = 1.6123417615890503, Test loss = 1.605318546295166\n","Iteration 221, Training loss = 1.6120723485946655, Test loss = 1.6050666570663452\n","Iteration 222, Training loss = 1.611804485321045, Test loss = 1.60481595993042\n","Iteration 223, Training loss = 1.6115381717681885, Test loss = 1.604567050933838\n","Iteration 224, Training loss = 1.611273169517517, Test loss = 1.6043195724487305\n","Iteration 225, Training loss = 1.6110097169876099, Test loss = 1.6040736436843872\n","Iteration 226, Training loss = 1.6107479333877563, Test loss = 1.6038289070129395\n","Iteration 227, Training loss = 1.6104873418807983, Test loss = 1.603585958480835\n","Iteration 228, Training loss = 1.610228419303894, Test loss = 1.603344202041626\n","Iteration 229, Training loss = 1.6099705696105957, Test loss = 1.6031041145324707\n","Iteration 230, Training loss = 1.6097146272659302, Test loss = 1.6028653383255005\n","Iteration 231, Training loss = 1.609459638595581, Test loss = 1.6026278734207153\n","Iteration 232, Training loss = 1.609206199645996, Test loss = 1.6023918390274048\n","Iteration 233, Training loss = 1.6089541912078857, Test loss = 1.6021571159362793\n","Iteration 234, Training loss = 1.6087034940719604, Test loss = 1.6019238233566284\n","Iteration 235, Training loss = 1.6084539890289307, Test loss = 1.6016918420791626\n","Iteration 236, Training loss = 1.608206033706665, Test loss = 1.6014611721038818\n","Iteration 237, Training loss = 1.6079593896865845, Test loss = 1.6012318134307861\n","Iteration 238, Training loss = 1.6077138185501099, Test loss = 1.6010037660598755\n","Iteration 239, Training loss = 1.6074697971343994, Test loss = 1.6007771492004395\n","Iteration 240, Training loss = 1.607226848602295, Test loss = 1.600551724433899\n","Iteration 241, Training loss = 1.606985330581665, Test loss = 1.600327491760254\n","Iteration 242, Training loss = 1.6067448854446411, Test loss = 1.6001046895980835\n","Iteration 243, Training loss = 1.6065057516098022, Test loss = 1.599882960319519\n","Iteration 244, Training loss = 1.6062678098678589, Test loss = 1.5996625423431396\n","Iteration 245, Training loss = 1.6060312986373901, Test loss = 1.5994434356689453\n","Iteration 246, Training loss = 1.6057958602905273, Test loss = 1.599225401878357\n","Iteration 247, Training loss = 1.60556161403656, Test loss = 1.5990084409713745\n","Iteration 248, Training loss = 1.6053286790847778, Test loss = 1.5987927913665771\n","Iteration 249, Training loss = 1.6050965785980225, Test loss = 1.5985783338546753\n","Iteration 250, Training loss = 1.6048660278320312, Test loss = 1.5983651876449585\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Plot training and test loss in the same figure\n","\n","---"],"metadata":{"id":"Yd8PGQKs15kB"}},{"cell_type":"code","source":["## Plot the training and test loss\n","fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n","ax.plot(loss_train, 'b', label = 'Train')\n","ax.plot(loss_test, 'r', label = 'Test')\n","ax.set_xlabel('Iteration')\n","ax.set_ylabel('Loss')\n","ax.legend();"],"metadata":{"id":"HL1069mE16U_","colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"status":"ok","timestamp":1733657367242,"user_tz":-330,"elapsed":573,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"96a95827-f7d0-4bc9-d1e2-5c7782079ac5"},"execution_count":128,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 400x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["---\n","\n","Assess model performance on test data\n","\n","---"],"metadata":{"id":"Eqxn_hXR2BM9"}},{"cell_type":"code","source":["## Assess model performance on test data\n","Yhat = model(X_test_reshaped_scaled)\n","\n","ypred = np.array(torch.argmax(Yhat, axis = 1)) # predicted labels for the test samples\n","ytrue = np.array(torch.argmax(Y_test, axis = 1)) # true labels for the test samples\n","print('Accuracy on test data = %3.2f'%(np.mean(ytrue == ypred)*100))\n","# Print confusion matrix\n","print(confusion_matrix(ytrue, ypred))"],"metadata":{"id":"AVdiXg6q2B4I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657370547,"user_tz":-330,"elapsed":506,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"d2f67027-f9cc-4a16-d0af-8b9a09cc13c2"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on test data = 51.74\n","[[ 871    0    2   37    0    1   68    0    0    1]\n"," [   0 1064   18    5    0    2   10   25    0   11]\n"," [  25   18  823   14    0    5  139    1    0    7]\n"," [  78   12   15  729    0  107   59    1    0    9]\n"," [  12   60   58   64    0  447  113   36    0  192]\n"," [  13    2   18  137    0  678   26    4    0   14]\n"," [ 123   10   93   53    0   12  653    9    0    5]\n"," [  10  562   85   13    0  142   38   64    0  114]\n"," [  30   33   18  248    0  441   73   17    0  114]\n"," [   9   72   51   33    0  460   50   42    0  292]]\n"]}]},{"cell_type":"code","source":["## Plot a random test sample with its predicted label printed above the plot\n","test_index = np.random.choice(X_test.shape[0])\n","fig, ax = plt.subplots(1, 1, figsize = (2, 2))\n","print(f'Image classified as {ypred[test_index]}')\n","ax.imshow(tf.reshape(X_test_reshaped_scaled[test_index], [28, 28]).numpy(), cmap = 'gray');"],"metadata":{"id":"11ubRM9p2W3A","colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"status":"ok","timestamp":1733657380474,"user_tz":-330,"elapsed":494,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"3ece3bc4-1f13-4d0c-a9d5-7845ff755f9b"},"execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["Image classified as 5\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 200x200 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["---\n","\n","Define a nonlinear activation layer with ReLU activation\n","\n","---"],"metadata":{"id":"brjKnRYGbz6x"}},{"cell_type":"code","source":["class ReLULayer(torch.nn.Module):\n","    def __init__(self):\n","        super(ReLULayer, self).__init__()\n","        self.activation = torch.nn.ReLU()\n","\n","    def forward(self, input):\n","        output = self.activation(input)  # Apply softmax activation\n","        return output"],"metadata":{"id":"qKu3VuDGb41Y","executionInfo":{"status":"ok","timestamp":1733656878094,"user_tz":-330,"elapsed":851,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Define a one hidden layer neural network model\n","\n","---"],"metadata":{"id":"NrbyHyBXcRGj"}},{"cell_type":"code","source":["class NeuralNetworkModel(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_nodes = 2, nodes = 2):\n","        super(NeuralNetworkModel, self).__init__()\n","        self.hidden_nodes = hidden_nodes\n","        self.nodes = nodes\n","        self.linearLayer1 = LinearLayer(input_dim, self.hidden_nodes)  # 1st Linear layer\n","        self.actlayer1 = ReLULayer() # 1st activation layer (ReLU)\n","        self.linearLayer2 = LinearLayer(self.hidden_nodes, self.nodes)  # 2nd Linear layer\n","        self.softmaxLayer = SoftmaxLayer()  # Softmax activation layer\n","\n","    def forward(self, input):\n","        output = self.linearLayer1(input)  # Forward pass through the 1st linear layer\n","        output = self.actlayer1(output) # ReLU activation\n","        output = self.linearLayer2(output)  # Forward pass through the 2nd linear layer\n","        output = self.softmaxLayer(output)  # Apply softmax activation\n","        return output"],"metadata":{"id":"3ByD0iWjcXQF","executionInfo":{"status":"ok","timestamp":1733656878094,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Perform forward propagation to the toy patient dataset using the NeuralNetworkModel built above.\n","\n","---"],"metadata":{"id":"r3XJ5qcjdPaE"}},{"cell_type":"code","source":["model = NeuralNetworkModel(X_S.shape[1], 4, 3) # 4 nodes in hidden layer\n","print(model(torch.tensor(X_S, dtype = torch.float32)))"],"metadata":{"id":"skQYdCkadfCF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733656937066,"user_tz":-330,"elapsed":547,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"569bc93f-2826-4ea0-aa59-9f78328c3c2d"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1012, 0.8747, 0.0242],\n","        [0.3678, 0.5798, 0.0524],\n","        [0.0493, 0.7877, 0.1630],\n","        [0.3530, 0.5790, 0.0681],\n","        [0.2795, 0.5770, 0.1435],\n","        [0.2450, 0.7124, 0.0426]], grad_fn=<SoftmaxBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-121-7f708048102c>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  print(model(torch.tensor(X_S, dtype = torch.float32)))\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Train the 1-hidden layer neural network classifier on the MNIST dataset\n","\n","---"],"metadata":{"id":"Ie2z0taWekVQ"}},{"cell_type":"code","source":["# This is an exercise## Train the softmax classifier on the MNIST dataset\n","# Initialize model\n","model = NeuralNetworkModel(num_features, 4, num_labels)\n","\n","# Gradient descent\n","maxiter = 1000\n","lr = 1e-03\n","\n","# Define loss function (CrossEntropyLoss in PyTorch includes softmax)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer (RMSprop)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n","\n","# Lists to store training and test losses\n","loss_train = [None] * maxiter\n","loss_test = [None] * maxiter\n","\n","# Start training loop\n","for k in range(maxiter):\n","    model.train()  # Set model to training mode\n","\n","    # Forward pass: compute predicted probabilities\n","    Yhat = model(X_train_reshaped_scaled)  # predicted probabilities\n","\n","    # Compute training loss\n","    L_train = loss_fn(Y_train, Yhat)  # CrossEntropyLoss expects raw logits (no softmax needed)\n","\n","    # Append training and test loss values\n","    loss_train[k] = L_train.item()  # Convert to scalar\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation for testing\n","        Yhat_test = model(X_test_reshaped_scaled)  # predicted probabilities for test set\n","        L_test = loss_fn(Y_test, Yhat_test)  # Compute test loss\n","    loss_test[k] = L_test.item()  # Convert to scalar\n","\n","    # Print losses\n","    print(f'Iteration {k+1}, Training loss = {loss_train[k]}, Test loss = {loss_test[k]}')\n","\n","    # Backward pass: compute gradients\n","    optimizer.zero_grad()  # Zero the gradients before the backward pass\n","    L_train.backward()  # Backpropagate the gradients\n","\n","    # Update model parameters using optimizer\n","    optimizer.step()  # Perform one optimization step"],"metadata":{"id":"EWbCrpERen2d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657338807,"user_tz":-330,"elapsed":128640,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"6962da4c-844d-4075-adf1-94d0973b97a3"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, Training loss = 2.359372138977051, Test loss = 2.3581950664520264\n","Iteration 2, Training loss = 2.3530125617980957, Test loss = 2.3515079021453857\n","Iteration 3, Training loss = 2.3485448360443115, Test loss = 2.3467819690704346\n","Iteration 4, Training loss = 2.341587781906128, Test loss = 2.339256525039673\n","Iteration 5, Training loss = 2.3296117782592773, Test loss = 2.327005624771118\n","Iteration 6, Training loss = 2.3187286853790283, Test loss = 2.3160078525543213\n","Iteration 7, Training loss = 2.3102314472198486, Test loss = 2.307431697845459\n","Iteration 8, Training loss = 2.303359270095825, Test loss = 2.300520181655884\n","Iteration 9, Training loss = 2.297616481781006, Test loss = 2.2947471141815186\n","Iteration 10, Training loss = 2.292677402496338, Test loss = 2.2897965908050537\n","Iteration 11, Training loss = 2.2883355617523193, Test loss = 2.2854461669921875\n","Iteration 12, Training loss = 2.2844502925872803, Test loss = 2.2815651893615723\n","Iteration 13, Training loss = 2.2809252738952637, Test loss = 2.2780468463897705\n","Iteration 14, Training loss = 2.2776927947998047, Test loss = 2.274831533432007\n","Iteration 15, Training loss = 2.274704933166504, Test loss = 2.27185320854187\n","Iteration 16, Training loss = 2.27192759513855, Test loss = 2.2690956592559814\n","Iteration 17, Training loss = 2.269338607788086, Test loss = 2.2665181159973145\n","Iteration 18, Training loss = 2.266915798187256, Test loss = 2.264113187789917\n","Iteration 19, Training loss = 2.2646446228027344, Test loss = 2.2618517875671387\n","Iteration 20, Training loss = 2.2625091075897217, Test loss = 2.259730577468872\n","Iteration 21, Training loss = 2.260493516921997, Test loss = 2.257725715637207\n","Iteration 22, Training loss = 2.258584976196289, Test loss = 2.255828857421875\n","Iteration 23, Training loss = 2.256767511367798, Test loss = 2.2540228366851807\n","Iteration 24, Training loss = 2.255030632019043, Test loss = 2.2522993087768555\n","Iteration 25, Training loss = 2.2533578872680664, Test loss = 2.2506344318389893\n","Iteration 26, Training loss = 2.2517333030700684, Test loss = 2.249023199081421\n","Iteration 27, Training loss = 2.2501418590545654, Test loss = 2.24743914604187\n","Iteration 28, Training loss = 2.2485666275024414, Test loss = 2.245879888534546\n","Iteration 29, Training loss = 2.2469894886016846, Test loss = 2.2443130016326904\n","Iteration 30, Training loss = 2.245389699935913, Test loss = 2.242727518081665\n","Iteration 31, Training loss = 2.2437453269958496, Test loss = 2.2411017417907715\n","Iteration 32, Training loss = 2.242032527923584, Test loss = 2.2394113540649414\n","Iteration 33, Training loss = 2.2402243614196777, Test loss = 2.2376294136047363\n","Iteration 34, Training loss = 2.2382915019989014, Test loss = 2.2357301712036133\n","Iteration 35, Training loss = 2.2362115383148193, Test loss = 2.2336924076080322\n","Iteration 36, Training loss = 2.2339718341827393, Test loss = 2.2315094470977783\n","Iteration 37, Training loss = 2.231570243835449, Test loss = 2.229182243347168\n","Iteration 38, Training loss = 2.2290308475494385, Test loss = 2.226733922958374\n","Iteration 39, Training loss = 2.2263996601104736, Test loss = 2.2242040634155273\n","Iteration 40, Training loss = 2.223740816116333, Test loss = 2.221651077270508\n","Iteration 41, Training loss = 2.2211177349090576, Test loss = 2.219132900238037\n","Iteration 42, Training loss = 2.2185890674591064, Test loss = 2.216702461242676\n","Iteration 43, Training loss = 2.216195583343506, Test loss = 2.2143938541412354\n","Iteration 44, Training loss = 2.213958740234375, Test loss = 2.212223768234253\n","Iteration 45, Training loss = 2.211883783340454, Test loss = 2.210200309753418\n","Iteration 46, Training loss = 2.2099649906158447, Test loss = 2.208314895629883\n","Iteration 47, Training loss = 2.2081916332244873, Test loss = 2.2065625190734863\n","Iteration 48, Training loss = 2.2065489292144775, Test loss = 2.2049295902252197\n","Iteration 49, Training loss = 2.2050225734710693, Test loss = 2.203404664993286\n","Iteration 50, Training loss = 2.203598737716675, Test loss = 2.2019779682159424\n","Iteration 51, Training loss = 2.202265977859497, Test loss = 2.200634479522705\n","Iteration 52, Training loss = 2.2010135650634766, Test loss = 2.1993675231933594\n","Iteration 53, Training loss = 2.199831247329712, Test loss = 2.1981654167175293\n","Iteration 54, Training loss = 2.1987130641937256, Test loss = 2.197025775909424\n","Iteration 55, Training loss = 2.197650671005249, Test loss = 2.1959378719329834\n","Iteration 56, Training loss = 2.1966378688812256, Test loss = 2.1949007511138916\n","Iteration 57, Training loss = 2.1956698894500732, Test loss = 2.1939070224761963\n","Iteration 58, Training loss = 2.1947429180145264, Test loss = 2.192955732345581\n","Iteration 59, Training loss = 2.1938538551330566, Test loss = 2.192039728164673\n","Iteration 60, Training loss = 2.1929988861083984, Test loss = 2.191159725189209\n","Iteration 61, Training loss = 2.1921756267547607, Test loss = 2.190307855606079\n","Iteration 62, Training loss = 2.191380739212036, Test loss = 2.1894869804382324\n","Iteration 63, Training loss = 2.190613031387329, Test loss = 2.188689947128296\n","Iteration 64, Training loss = 2.1898696422576904, Test loss = 2.1879189014434814\n","Iteration 65, Training loss = 2.1891489028930664, Test loss = 2.1871705055236816\n","Iteration 66, Training loss = 2.188450336456299, Test loss = 2.186446189880371\n","Iteration 67, Training loss = 2.1877713203430176, Test loss = 2.185739278793335\n","Iteration 68, Training loss = 2.1871109008789062, Test loss = 2.185051679611206\n","Iteration 69, Training loss = 2.1864688396453857, Test loss = 2.184382438659668\n","Iteration 70, Training loss = 2.185842752456665, Test loss = 2.183732509613037\n","Iteration 71, Training loss = 2.1852328777313232, Test loss = 2.1830976009368896\n","Iteration 72, Training loss = 2.1846370697021484, Test loss = 2.182476282119751\n","Iteration 73, Training loss = 2.184056043624878, Test loss = 2.1818699836730957\n","Iteration 74, Training loss = 2.183488368988037, Test loss = 2.181276559829712\n","Iteration 75, Training loss = 2.1829335689544678, Test loss = 2.180696964263916\n","Iteration 76, Training loss = 2.1823904514312744, Test loss = 2.1801278591156006\n","Iteration 77, Training loss = 2.1818594932556152, Test loss = 2.179572343826294\n","Iteration 78, Training loss = 2.1813395023345947, Test loss = 2.1790289878845215\n","Iteration 79, Training loss = 2.180830478668213, Test loss = 2.178496837615967\n","Iteration 80, Training loss = 2.180331230163574, Test loss = 2.1779744625091553\n","Iteration 81, Training loss = 2.1798412799835205, Test loss = 2.177462339401245\n","Iteration 82, Training loss = 2.1793606281280518, Test loss = 2.1769585609436035\n","Iteration 83, Training loss = 2.178889274597168, Test loss = 2.176464319229126\n","Iteration 84, Training loss = 2.178426742553711, Test loss = 2.175981283187866\n","Iteration 85, Training loss = 2.1779725551605225, Test loss = 2.1755053997039795\n","Iteration 86, Training loss = 2.1775264739990234, Test loss = 2.175037384033203\n","Iteration 87, Training loss = 2.1770880222320557, Test loss = 2.1745779514312744\n","Iteration 88, Training loss = 2.17665696144104, Test loss = 2.1741256713867188\n","Iteration 89, Training loss = 2.1762330532073975, Test loss = 2.173682689666748\n","Iteration 90, Training loss = 2.1758155822753906, Test loss = 2.173245429992676\n","Iteration 91, Training loss = 2.1754050254821777, Test loss = 2.17281436920166\n","Iteration 92, Training loss = 2.1750009059906006, Test loss = 2.1723902225494385\n","Iteration 93, Training loss = 2.17460298538208, Test loss = 2.17197322845459\n","Iteration 94, Training loss = 2.174210786819458, Test loss = 2.1715619564056396\n","Iteration 95, Training loss = 2.1738245487213135, Test loss = 2.1711549758911133\n","Iteration 96, Training loss = 2.17344331741333, Test loss = 2.170755386352539\n","Iteration 97, Training loss = 2.173067331314087, Test loss = 2.1703598499298096\n","Iteration 98, Training loss = 2.172696828842163, Test loss = 2.1699702739715576\n","Iteration 99, Training loss = 2.172330617904663, Test loss = 2.169586658477783\n","Iteration 100, Training loss = 2.1719698905944824, Test loss = 2.1692066192626953\n","Iteration 101, Training loss = 2.171613931655884, Test loss = 2.1688337326049805\n","Iteration 102, Training loss = 2.171262264251709, Test loss = 2.1684648990631104\n","Iteration 103, Training loss = 2.1709156036376953, Test loss = 2.1681008338928223\n","Iteration 104, Training loss = 2.1705727577209473, Test loss = 2.1677403450012207\n","Iteration 105, Training loss = 2.170234441757202, Test loss = 2.1673874855041504\n","Iteration 106, Training loss = 2.1698997020721436, Test loss = 2.167036771774292\n","Iteration 107, Training loss = 2.169569492340088, Test loss = 2.1666903495788574\n","Iteration 108, Training loss = 2.1692428588867188, Test loss = 2.1663472652435303\n","Iteration 109, Training loss = 2.1689200401306152, Test loss = 2.1660079956054688\n","Iteration 110, Training loss = 2.168600559234619, Test loss = 2.165672540664673\n","Iteration 111, Training loss = 2.1682844161987305, Test loss = 2.1653409004211426\n","Iteration 112, Training loss = 2.1679718494415283, Test loss = 2.1650121212005615\n","Iteration 113, Training loss = 2.1676623821258545, Test loss = 2.1646885871887207\n","Iteration 114, Training loss = 2.167356252670288, Test loss = 2.1643664836883545\n","Iteration 115, Training loss = 2.167052984237671, Test loss = 2.164048433303833\n","Iteration 116, Training loss = 2.166752815246582, Test loss = 2.1637332439422607\n","Iteration 117, Training loss = 2.1664555072784424, Test loss = 2.1634206771850586\n","Iteration 118, Training loss = 2.16616153717041, Test loss = 2.163113594055176\n","Iteration 119, Training loss = 2.165869951248169, Test loss = 2.1628079414367676\n","Iteration 120, Training loss = 2.165581703186035, Test loss = 2.162506103515625\n","Iteration 121, Training loss = 2.1652963161468506, Test loss = 2.1622071266174316\n","Iteration 122, Training loss = 2.165013313293457, Test loss = 2.1619110107421875\n","Iteration 123, Training loss = 2.1647326946258545, Test loss = 2.1616172790527344\n","Iteration 124, Training loss = 2.1644551753997803, Test loss = 2.1613268852233887\n","Iteration 125, Training loss = 2.164180040359497, Test loss = 2.161038875579834\n","Iteration 126, Training loss = 2.163907051086426, Test loss = 2.160752058029175\n","Iteration 127, Training loss = 2.1636364459991455, Test loss = 2.1604692935943604\n","Iteration 128, Training loss = 2.1633682250976562, Test loss = 2.160188913345337\n","Iteration 129, Training loss = 2.163102149963379, Test loss = 2.1599113941192627\n","Iteration 130, Training loss = 2.1628382205963135, Test loss = 2.1596360206604004\n","Iteration 131, Training loss = 2.162576675415039, Test loss = 2.1593639850616455\n","Iteration 132, Training loss = 2.1623172760009766, Test loss = 2.159092664718628\n","Iteration 133, Training loss = 2.1620595455169678, Test loss = 2.1588239669799805\n","Iteration 134, Training loss = 2.16180419921875, Test loss = 2.1585569381713867\n","Iteration 135, Training loss = 2.161550521850586, Test loss = 2.158292293548584\n","Iteration 136, Training loss = 2.1612987518310547, Test loss = 2.158029794692993\n","Iteration 137, Training loss = 2.1610491275787354, Test loss = 2.1577696800231934\n","Iteration 138, Training loss = 2.160801410675049, Test loss = 2.157510995864868\n","Iteration 139, Training loss = 2.160555362701416, Test loss = 2.1572539806365967\n","Iteration 140, Training loss = 2.160311222076416, Test loss = 2.156999111175537\n","Iteration 141, Training loss = 2.1600685119628906, Test loss = 2.156745195388794\n","Iteration 142, Training loss = 2.159827709197998, Test loss = 2.1564929485321045\n","Iteration 143, Training loss = 2.159588575363159, Test loss = 2.156243324279785\n","Iteration 144, Training loss = 2.159350872039795, Test loss = 2.1559956073760986\n","Iteration 145, Training loss = 2.1591148376464844, Test loss = 2.1557483673095703\n","Iteration 146, Training loss = 2.1588802337646484, Test loss = 2.1555047035217285\n","Iteration 147, Training loss = 2.158647298812866, Test loss = 2.155261993408203\n","Iteration 148, Training loss = 2.1584155559539795, Test loss = 2.1550207138061523\n","Iteration 149, Training loss = 2.1581857204437256, Test loss = 2.1547813415527344\n","Iteration 150, Training loss = 2.157957077026367, Test loss = 2.1545448303222656\n","Iteration 151, Training loss = 2.1577303409576416, Test loss = 2.154308557510376\n","Iteration 152, Training loss = 2.1575047969818115, Test loss = 2.154073715209961\n","Iteration 153, Training loss = 2.157280683517456, Test loss = 2.153841495513916\n","Iteration 154, Training loss = 2.157057762145996, Test loss = 2.1536097526550293\n","Iteration 155, Training loss = 2.15683650970459, Test loss = 2.153380870819092\n","Iteration 156, Training loss = 2.156616687774658, Test loss = 2.1531522274017334\n","Iteration 157, Training loss = 2.156397819519043, Test loss = 2.1529252529144287\n","Iteration 158, Training loss = 2.1561810970306396, Test loss = 2.1526994705200195\n","Iteration 159, Training loss = 2.1559653282165527, Test loss = 2.1524760723114014\n","Iteration 160, Training loss = 2.1557505130767822, Test loss = 2.1522538661956787\n","Iteration 161, Training loss = 2.1555371284484863, Test loss = 2.1520326137542725\n","Iteration 162, Training loss = 2.155325174331665, Test loss = 2.1518123149871826\n","Iteration 163, Training loss = 2.15511417388916, Test loss = 2.15159273147583\n","Iteration 164, Training loss = 2.154904365539551, Test loss = 2.151376247406006\n","Iteration 165, Training loss = 2.154695510864258, Test loss = 2.1511595249176025\n","Iteration 166, Training loss = 2.1544880867004395, Test loss = 2.150944471359253\n","Iteration 167, Training loss = 2.1542816162109375, Test loss = 2.150730609893799\n","Iteration 168, Training loss = 2.154076337814331, Test loss = 2.1505181789398193\n","Iteration 169, Training loss = 2.153872489929199, Test loss = 2.1503069400787354\n","Iteration 170, Training loss = 2.153669595718384, Test loss = 2.150097370147705\n","Iteration 171, Training loss = 2.1534676551818848, Test loss = 2.149888753890991\n","Iteration 172, Training loss = 2.1532669067382812, Test loss = 2.1496803760528564\n","Iteration 173, Training loss = 2.153067111968994, Test loss = 2.1494741439819336\n","Iteration 174, Training loss = 2.1528682708740234, Test loss = 2.149268865585327\n","Iteration 175, Training loss = 2.1526706218719482, Test loss = 2.1490652561187744\n","Iteration 176, Training loss = 2.1524739265441895, Test loss = 2.1488630771636963\n","Iteration 177, Training loss = 2.152278184890747, Test loss = 2.1486616134643555\n","Iteration 178, Training loss = 2.1520836353302, Test loss = 2.148460865020752\n","Iteration 179, Training loss = 2.1518900394439697, Test loss = 2.1482620239257812\n","Iteration 180, Training loss = 2.1516971588134766, Test loss = 2.1480636596679688\n","Iteration 181, Training loss = 2.1515052318573, Test loss = 2.1478657722473145\n","Iteration 182, Training loss = 2.1513140201568604, Test loss = 2.1476690769195557\n","Iteration 183, Training loss = 2.1511240005493164, Test loss = 2.1474738121032715\n","Iteration 184, Training loss = 2.1509346961975098, Test loss = 2.147278308868408\n","Iteration 185, Training loss = 2.1507461071014404, Test loss = 2.1470844745635986\n","Iteration 186, Training loss = 2.1505587100982666, Test loss = 2.14689040184021\n","Iteration 187, Training loss = 2.150371551513672, Test loss = 2.146697759628296\n","Iteration 188, Training loss = 2.1501853466033936, Test loss = 2.1465072631835938\n","Iteration 189, Training loss = 2.15000057220459, Test loss = 2.1463162899017334\n","Iteration 190, Training loss = 2.1498160362243652, Test loss = 2.1461269855499268\n","Iteration 191, Training loss = 2.1496329307556152, Test loss = 2.1459386348724365\n","Iteration 192, Training loss = 2.1494503021240234, Test loss = 2.1457509994506836\n","Iteration 193, Training loss = 2.149268627166748, Test loss = 2.145564556121826\n","Iteration 194, Training loss = 2.149087429046631, Test loss = 2.145378351211548\n","Iteration 195, Training loss = 2.148906707763672, Test loss = 2.145192861557007\n","Iteration 196, Training loss = 2.1487274169921875, Test loss = 2.1450083255767822\n","Iteration 197, Training loss = 2.1485483646392822, Test loss = 2.144824504852295\n","Iteration 198, Training loss = 2.1483702659606934, Test loss = 2.144641399383545\n","Iteration 199, Training loss = 2.148192882537842, Test loss = 2.1444590091705322\n","Iteration 200, Training loss = 2.1480162143707275, Test loss = 2.1442782878875732\n","Iteration 201, Training loss = 2.1478400230407715, Test loss = 2.144096851348877\n","Iteration 202, Training loss = 2.147664785385132, Test loss = 2.1439170837402344\n","Iteration 203, Training loss = 2.1474897861480713, Test loss = 2.1437368392944336\n","Iteration 204, Training loss = 2.1473159790039062, Test loss = 2.1435582637786865\n","Iteration 205, Training loss = 2.1471426486968994, Test loss = 2.143380641937256\n","Iteration 206, Training loss = 2.146970272064209, Test loss = 2.1432039737701416\n","Iteration 207, Training loss = 2.1467983722686768, Test loss = 2.1430275440216064\n","Iteration 208, Training loss = 2.146627187728882, Test loss = 2.142852783203125\n","Iteration 209, Training loss = 2.146456718444824, Test loss = 2.14267897605896\n","Iteration 210, Training loss = 2.146286964416504, Test loss = 2.142505168914795\n","Iteration 211, Training loss = 2.1461172103881836, Test loss = 2.142331838607788\n","Iteration 212, Training loss = 2.145948648452759, Test loss = 2.1421587467193604\n","Iteration 213, Training loss = 2.145780563354492, Test loss = 2.141986608505249\n","Iteration 214, Training loss = 2.145612955093384, Test loss = 2.141815423965454\n","Iteration 215, Training loss = 2.1454458236694336, Test loss = 2.1416444778442383\n","Iteration 216, Training loss = 2.1452794075012207, Test loss = 2.1414740085601807\n","Iteration 217, Training loss = 2.145113468170166, Test loss = 2.1413049697875977\n","Iteration 218, Training loss = 2.1449482440948486, Test loss = 2.1411361694335938\n","Iteration 219, Training loss = 2.1447837352752686, Test loss = 2.1409671306610107\n","Iteration 220, Training loss = 2.1446192264556885, Test loss = 2.140799045562744\n","Iteration 221, Training loss = 2.144455671310425, Test loss = 2.1406307220458984\n","Iteration 222, Training loss = 2.144292116165161, Test loss = 2.140463352203369\n","Iteration 223, Training loss = 2.144129514694214, Test loss = 2.14029598236084\n","Iteration 224, Training loss = 2.1439671516418457, Test loss = 2.140129804611206\n","Iteration 225, Training loss = 2.1438050270080566, Test loss = 2.1399645805358887\n","Iteration 226, Training loss = 2.143643379211426, Test loss = 2.139799118041992\n","Iteration 227, Training loss = 2.1434824466705322, Test loss = 2.139633893966675\n","Iteration 228, Training loss = 2.143321990966797, Test loss = 2.139469623565674\n","Iteration 229, Training loss = 2.1431617736816406, Test loss = 2.139305353164673\n","Iteration 230, Training loss = 2.1430017948150635, Test loss = 2.1391420364379883\n","Iteration 231, Training loss = 2.1428425312042236, Test loss = 2.1389787197113037\n","Iteration 232, Training loss = 2.142683267593384, Test loss = 2.1388161182403564\n","Iteration 233, Training loss = 2.142524480819702, Test loss = 2.1386539936065674\n","Iteration 234, Training loss = 2.142366409301758, Test loss = 2.1384918689727783\n","Iteration 235, Training loss = 2.1422083377838135, Test loss = 2.1383304595947266\n","Iteration 236, Training loss = 2.1420509815216064, Test loss = 2.138170003890991\n","Iteration 237, Training loss = 2.1418938636779785, Test loss = 2.1380093097686768\n","Iteration 238, Training loss = 2.141737461090088, Test loss = 2.137850522994995\n","Iteration 239, Training loss = 2.1415815353393555, Test loss = 2.137691020965576\n","Iteration 240, Training loss = 2.141425609588623, Test loss = 2.1375319957733154\n","Iteration 241, Training loss = 2.141270160675049, Test loss = 2.1373729705810547\n","Iteration 242, Training loss = 2.141115188598633, Test loss = 2.137213706970215\n","Iteration 243, Training loss = 2.140960216522217, Test loss = 2.1370553970336914\n","Iteration 244, Training loss = 2.140805721282959, Test loss = 2.1368978023529053\n","Iteration 245, Training loss = 2.1406517028808594, Test loss = 2.136739730834961\n","Iteration 246, Training loss = 2.1404976844787598, Test loss = 2.136582374572754\n","Iteration 247, Training loss = 2.1403441429138184, Test loss = 2.136425495147705\n","Iteration 248, Training loss = 2.140190839767456, Test loss = 2.136268138885498\n","Iteration 249, Training loss = 2.140037775039673, Test loss = 2.1361122131347656\n","Iteration 250, Training loss = 2.139885187149048, Test loss = 2.1359546184539795\n","Iteration 251, Training loss = 2.1397323608398438, Test loss = 2.1357998847961426\n","Iteration 252, Training loss = 2.139580011367798, Test loss = 2.135643243789673\n","Iteration 253, Training loss = 2.139427900314331, Test loss = 2.1354875564575195\n","Iteration 254, Training loss = 2.1392757892608643, Test loss = 2.1353321075439453\n","Iteration 255, Training loss = 2.1391236782073975, Test loss = 2.135176658630371\n","Iteration 256, Training loss = 2.138972043991089, Test loss = 2.1350207328796387\n","Iteration 257, Training loss = 2.1388204097747803, Test loss = 2.1348657608032227\n","Iteration 258, Training loss = 2.1386687755584717, Test loss = 2.1347105503082275\n","Iteration 259, Training loss = 2.138517141342163, Test loss = 2.1345551013946533\n","Iteration 260, Training loss = 2.1383655071258545, Test loss = 2.1344001293182373\n","Iteration 261, Training loss = 2.138214111328125, Test loss = 2.134244918823242\n","Iteration 262, Training loss = 2.1380627155303955, Test loss = 2.134089946746826\n","Iteration 263, Training loss = 2.137911558151245, Test loss = 2.1339352130889893\n","Iteration 264, Training loss = 2.1377604007720947, Test loss = 2.1337807178497314\n","Iteration 265, Training loss = 2.1376090049743652, Test loss = 2.133626699447632\n","Iteration 266, Training loss = 2.137458324432373, Test loss = 2.1334710121154785\n","Iteration 267, Training loss = 2.1373069286346436, Test loss = 2.1333162784576416\n","Iteration 268, Training loss = 2.1371560096740723, Test loss = 2.133160352706909\n","Iteration 269, Training loss = 2.1370046138763428, Test loss = 2.1330056190490723\n","Iteration 270, Training loss = 2.1368534564971924, Test loss = 2.13284969329834\n","Iteration 271, Training loss = 2.136702060699463, Test loss = 2.132695198059082\n","Iteration 272, Training loss = 2.1365504264831543, Test loss = 2.132540225982666\n","Iteration 273, Training loss = 2.1363987922668457, Test loss = 2.1323838233947754\n","Iteration 274, Training loss = 2.136246919631958, Test loss = 2.132227659225464\n","Iteration 275, Training loss = 2.136094808578491, Test loss = 2.1320717334747314\n","Iteration 276, Training loss = 2.1359426975250244, Test loss = 2.1319146156311035\n","Iteration 277, Training loss = 2.1357898712158203, Test loss = 2.1317572593688965\n","Iteration 278, Training loss = 2.1356375217437744, Test loss = 2.1316003799438477\n","Iteration 279, Training loss = 2.135484218597412, Test loss = 2.131442070007324\n","Iteration 280, Training loss = 2.1353306770324707, Test loss = 2.131284475326538\n","Iteration 281, Training loss = 2.135176658630371, Test loss = 2.1311256885528564\n","Iteration 282, Training loss = 2.135021924972534, Test loss = 2.130967140197754\n","Iteration 283, Training loss = 2.134866952896118, Test loss = 2.1308071613311768\n","Iteration 284, Training loss = 2.134711742401123, Test loss = 2.130647897720337\n","Iteration 285, Training loss = 2.1345555782318115, Test loss = 2.1304872035980225\n","Iteration 286, Training loss = 2.134398937225342, Test loss = 2.1303255558013916\n","Iteration 287, Training loss = 2.1342415809631348, Test loss = 2.1301627159118652\n","Iteration 288, Training loss = 2.1340835094451904, Test loss = 2.130000114440918\n","Iteration 289, Training loss = 2.133924961090088, Test loss = 2.1298367977142334\n","Iteration 290, Training loss = 2.133765459060669, Test loss = 2.1296727657318115\n","Iteration 291, Training loss = 2.1336052417755127, Test loss = 2.129507541656494\n","Iteration 292, Training loss = 2.13344407081604, Test loss = 2.1293416023254395\n","Iteration 293, Training loss = 2.133281946182251, Test loss = 2.1291747093200684\n","Iteration 294, Training loss = 2.1331191062927246, Test loss = 2.1290066242218018\n","Iteration 295, Training loss = 2.1329550743103027, Test loss = 2.1288373470306396\n","Iteration 296, Training loss = 2.1327900886535645, Test loss = 2.1286680698394775\n","Iteration 297, Training loss = 2.1326234340667725, Test loss = 2.1284968852996826\n","Iteration 298, Training loss = 2.1324563026428223, Test loss = 2.128323793411255\n","Iteration 299, Training loss = 2.1322877407073975, Test loss = 2.1281492710113525\n","Iteration 300, Training loss = 2.132117748260498, Test loss = 2.1279728412628174\n","Iteration 301, Training loss = 2.131946086883545, Test loss = 2.1277952194213867\n","Iteration 302, Training loss = 2.1317732334136963, Test loss = 2.127614736557007\n","Iteration 303, Training loss = 2.131598711013794, Test loss = 2.127434492111206\n","Iteration 304, Training loss = 2.131422758102417, Test loss = 2.1272518634796143\n","Iteration 305, Training loss = 2.131244421005249, Test loss = 2.1270668506622314\n","Iteration 306, Training loss = 2.1310648918151855, Test loss = 2.1268808841705322\n","Iteration 307, Training loss = 2.130882978439331, Test loss = 2.1266911029815674\n","Iteration 308, Training loss = 2.1306991577148438, Test loss = 2.1265013217926025\n","Iteration 309, Training loss = 2.1305131912231445, Test loss = 2.126307725906372\n","Iteration 310, Training loss = 2.1303250789642334, Test loss = 2.126112222671509\n","Iteration 311, Training loss = 2.1301348209381104, Test loss = 2.125913381576538\n","Iteration 312, Training loss = 2.129941940307617, Test loss = 2.1257131099700928\n","Iteration 313, Training loss = 2.129746437072754, Test loss = 2.1255102157592773\n","Iteration 314, Training loss = 2.1295485496520996, Test loss = 2.125304698944092\n","Iteration 315, Training loss = 2.129348039627075, Test loss = 2.1250956058502197\n","Iteration 316, Training loss = 2.1291449069976807, Test loss = 2.1248841285705566\n","Iteration 317, Training loss = 2.1289384365081787, Test loss = 2.1246700286865234\n","Iteration 318, Training loss = 2.1287295818328857, Test loss = 2.1244523525238037\n","Iteration 319, Training loss = 2.1285173892974854, Test loss = 2.124232530593872\n","Iteration 320, Training loss = 2.128302574157715, Test loss = 2.124008893966675\n","Iteration 321, Training loss = 2.128084421157837, Test loss = 2.1237831115722656\n","Iteration 322, Training loss = 2.1278629302978516, Test loss = 2.123553514480591\n","Iteration 323, Training loss = 2.127638339996338, Test loss = 2.123319625854492\n","Iteration 324, Training loss = 2.1274101734161377, Test loss = 2.1230833530426025\n","Iteration 325, Training loss = 2.127178907394409, Test loss = 2.1228420734405518\n","Iteration 326, Training loss = 2.126944065093994, Test loss = 2.1225991249084473\n","Iteration 327, Training loss = 2.1267056465148926, Test loss = 2.122349500656128\n","Iteration 328, Training loss = 2.1264638900756836, Test loss = 2.122098445892334\n","Iteration 329, Training loss = 2.126218318939209, Test loss = 2.121840715408325\n","Iteration 330, Training loss = 2.125969648361206, Test loss = 2.1215827465057373\n","Iteration 331, Training loss = 2.1257166862487793, Test loss = 2.121316432952881\n","Iteration 332, Training loss = 2.125460386276245, Test loss = 2.121051073074341\n","Iteration 333, Training loss = 2.1252002716064453, Test loss = 2.120779275894165\n","Iteration 334, Training loss = 2.124936580657959, Test loss = 2.1205077171325684\n","Iteration 335, Training loss = 2.124669313430786, Test loss = 2.1202290058135986\n","Iteration 336, Training loss = 2.1243984699249268, Test loss = 2.11995005607605\n","Iteration 337, Training loss = 2.124124526977539, Test loss = 2.1196653842926025\n","Iteration 338, Training loss = 2.123847484588623, Test loss = 2.1193814277648926\n","Iteration 339, Training loss = 2.123567819595337, Test loss = 2.1190900802612305\n","Iteration 340, Training loss = 2.1232857704162598, Test loss = 2.1187987327575684\n","Iteration 341, Training loss = 2.1230010986328125, Test loss = 2.118501901626587\n","Iteration 342, Training loss = 2.1227145195007324, Test loss = 2.118206739425659\n","Iteration 343, Training loss = 2.1224255561828613, Test loss = 2.1179041862487793\n","Iteration 344, Training loss = 2.1221346855163574, Test loss = 2.1176059246063232\n","Iteration 345, Training loss = 2.1218416690826416, Test loss = 2.1172990798950195\n","Iteration 346, Training loss = 2.121547222137451, Test loss = 2.116999626159668\n","Iteration 347, Training loss = 2.1212515830993652, Test loss = 2.1166880130767822\n","Iteration 348, Training loss = 2.120954751968384, Test loss = 2.1163878440856934\n","Iteration 349, Training loss = 2.120656728744507, Test loss = 2.116072177886963\n","Iteration 350, Training loss = 2.1203575134277344, Test loss = 2.1157734394073486\n","Iteration 351, Training loss = 2.120058298110962, Test loss = 2.115449905395508\n","Iteration 352, Training loss = 2.1197586059570312, Test loss = 2.1151559352874756\n","Iteration 353, Training loss = 2.1194591522216797, Test loss = 2.1148264408111572\n","Iteration 354, Training loss = 2.1191599369049072, Test loss = 2.1145424842834473\n","Iteration 355, Training loss = 2.118861675262451, Test loss = 2.114201307296753\n","Iteration 356, Training loss = 2.1185643672943115, Test loss = 2.113938093185425\n","Iteration 357, Training loss = 2.118269205093384, Test loss = 2.1135809421539307\n","Iteration 358, Training loss = 2.1179769039154053, Test loss = 2.1133487224578857\n","Iteration 359, Training loss = 2.1176881790161133, Test loss = 2.1129684448242188\n","Iteration 360, Training loss = 2.1174075603485107, Test loss = 2.1127870082855225\n","Iteration 361, Training loss = 2.1171326637268066, Test loss = 2.1123757362365723\n","Iteration 362, Training loss = 2.1168646812438965, Test loss = 2.1122543811798096\n","Iteration 363, Training loss = 2.116598129272461, Test loss = 2.1118078231811523\n","Iteration 364, Training loss = 2.1163241863250732, Test loss = 2.1117193698883057\n","Iteration 365, Training loss = 2.1160385608673096, Test loss = 2.111233711242676\n","Iteration 366, Training loss = 2.115739583969116, Test loss = 2.1111176013946533\n","Iteration 367, Training loss = 2.1154375076293945, Test loss = 2.110637664794922\n","Iteration 368, Training loss = 2.115133285522461, Test loss = 2.1104824542999268\n","Iteration 369, Training loss = 2.114835023880005, Test loss = 2.1100497245788574\n","Iteration 370, Training loss = 2.1145429611206055, Test loss = 2.1098666191101074\n","Iteration 371, Training loss = 2.11425518989563, Test loss = 2.1094813346862793\n","Iteration 372, Training loss = 2.113973379135132, Test loss = 2.109276056289673\n","Iteration 373, Training loss = 2.113694906234741, Test loss = 2.108927011489868\n","Iteration 374, Training loss = 2.113419771194458, Test loss = 2.1087088584899902\n","Iteration 375, Training loss = 2.113147497177124, Test loss = 2.1083824634552\n","Iteration 376, Training loss = 2.1128764152526855, Test loss = 2.1081550121307373\n","Iteration 377, Training loss = 2.112607479095459, Test loss = 2.1078453063964844\n","Iteration 378, Training loss = 2.1123406887054443, Test loss = 2.1076109409332275\n","Iteration 379, Training loss = 2.1120758056640625, Test loss = 2.107316017150879\n","Iteration 380, Training loss = 2.1118125915527344, Test loss = 2.107076644897461\n","Iteration 381, Training loss = 2.11155104637146, Test loss = 2.10679292678833\n","Iteration 382, Training loss = 2.1112916469573975, Test loss = 2.1065523624420166\n","Iteration 383, Training loss = 2.1110334396362305, Test loss = 2.106276273727417\n","Iteration 384, Training loss = 2.110776901245117, Test loss = 2.1060352325439453\n","Iteration 385, Training loss = 2.110522508621216, Test loss = 2.1057653427124023\n","Iteration 386, Training loss = 2.110269069671631, Test loss = 2.105524778366089\n","Iteration 387, Training loss = 2.1100172996520996, Test loss = 2.1052608489990234\n","Iteration 388, Training loss = 2.109766960144043, Test loss = 2.1050219535827637\n","Iteration 389, Training loss = 2.10951828956604, Test loss = 2.1047630310058594\n","Iteration 390, Training loss = 2.1092708110809326, Test loss = 2.1045262813568115\n","Iteration 391, Training loss = 2.109025478363037, Test loss = 2.104271650314331\n","Iteration 392, Training loss = 2.108781337738037, Test loss = 2.1040382385253906\n","Iteration 393, Training loss = 2.1085386276245117, Test loss = 2.1037869453430176\n","Iteration 394, Training loss = 2.108297348022461, Test loss = 2.1035571098327637\n","Iteration 395, Training loss = 2.108057737350464, Test loss = 2.1033096313476562\n","Iteration 396, Training loss = 2.1078195571899414, Test loss = 2.103081703186035\n","Iteration 397, Training loss = 2.1075825691223145, Test loss = 2.1028378009796143\n","Iteration 398, Training loss = 2.107347249984741, Test loss = 2.1026113033294678\n","Iteration 399, Training loss = 2.1071133613586426, Test loss = 2.1023709774017334\n","Iteration 400, Training loss = 2.1068806648254395, Test loss = 2.1021482944488525\n","Iteration 401, Training loss = 2.10664963722229, Test loss = 2.10191011428833\n","Iteration 402, Training loss = 2.1064200401306152, Test loss = 2.1016902923583984\n","Iteration 403, Training loss = 2.106191873550415, Test loss = 2.101454734802246\n","Iteration 404, Training loss = 2.1059648990631104, Test loss = 2.101238489151001\n","Iteration 405, Training loss = 2.1057393550872803, Test loss = 2.1010046005249023\n","Iteration 406, Training loss = 2.1055150032043457, Test loss = 2.1007919311523438\n","Iteration 407, Training loss = 2.1052918434143066, Test loss = 2.100560188293457\n","Iteration 408, Training loss = 2.1050705909729004, Test loss = 2.1003520488739014\n","Iteration 409, Training loss = 2.1048505306243896, Test loss = 2.100120782852173\n","Iteration 410, Training loss = 2.1046314239501953, Test loss = 2.0999178886413574\n","Iteration 411, Training loss = 2.1044139862060547, Test loss = 2.099686861038208\n","Iteration 412, Training loss = 2.1041979789733887, Test loss = 2.0994889736175537\n","Iteration 413, Training loss = 2.103982925415039, Test loss = 2.099257707595825\n","Iteration 414, Training loss = 2.103769302368164, Test loss = 2.099066734313965\n","Iteration 415, Training loss = 2.1035571098327637, Test loss = 2.0988340377807617\n","Iteration 416, Training loss = 2.103346109390259, Test loss = 2.0986509323120117\n","Iteration 417, Training loss = 2.1031370162963867, Test loss = 2.0984139442443848\n","Iteration 418, Training loss = 2.102928876876831, Test loss = 2.0982425212860107\n","Iteration 419, Training loss = 2.1027231216430664, Test loss = 2.09799861907959\n","Iteration 420, Training loss = 2.102518081665039, Test loss = 2.097841501235962\n","Iteration 421, Training loss = 2.1023154258728027, Test loss = 2.0975887775421143\n","Iteration 422, Training loss = 2.102113723754883, Test loss = 2.0974504947662354\n","Iteration 423, Training loss = 2.1019136905670166, Test loss = 2.097184419631958\n","Iteration 424, Training loss = 2.1017158031463623, Test loss = 2.0970675945281982\n","Iteration 425, Training loss = 2.101520538330078, Test loss = 2.0967869758605957\n","Iteration 426, Training loss = 2.1013259887695312, Test loss = 2.096693515777588\n","Iteration 427, Training loss = 2.101135015487671, Test loss = 2.0963962078094482\n","Iteration 428, Training loss = 2.1009445190429688, Test loss = 2.096330165863037\n","Iteration 429, Training loss = 2.1007578372955322, Test loss = 2.096013307571411\n","Iteration 430, Training loss = 2.1005685329437256, Test loss = 2.0959699153900146\n","Iteration 431, Training loss = 2.100381851196289, Test loss = 2.095635414123535\n","Iteration 432, Training loss = 2.1001932621002197, Test loss = 2.0956075191497803\n","Iteration 433, Training loss = 2.100003957748413, Test loss = 2.0952608585357666\n","Iteration 434, Training loss = 2.0998129844665527, Test loss = 2.0952348709106445\n","Iteration 435, Training loss = 2.0996203422546387, Test loss = 2.0948877334594727\n","Iteration 436, Training loss = 2.0994277000427246, Test loss = 2.0948495864868164\n","Iteration 437, Training loss = 2.099235773086548, Test loss = 2.094517230987549\n","Iteration 438, Training loss = 2.099043607711792, Test loss = 2.0944626331329346\n","Iteration 439, Training loss = 2.098853826522827, Test loss = 2.094151735305786\n","Iteration 440, Training loss = 2.0986647605895996, Test loss = 2.0940799713134766\n","Iteration 441, Training loss = 2.098478317260742, Test loss = 2.093792676925659\n","Iteration 442, Training loss = 2.0982933044433594, Test loss = 2.0937070846557617\n","Iteration 443, Training loss = 2.0981101989746094, Test loss = 2.0934391021728516\n","Iteration 444, Training loss = 2.097928047180176, Test loss = 2.0933408737182617\n","Iteration 445, Training loss = 2.0977470874786377, Test loss = 2.093090772628784\n","Iteration 446, Training loss = 2.0975680351257324, Test loss = 2.092982053756714\n","Iteration 447, Training loss = 2.0973899364471436, Test loss = 2.0927469730377197\n","Iteration 448, Training loss = 2.0972132682800293, Test loss = 2.0926320552825928\n","Iteration 449, Training loss = 2.0970373153686523, Test loss = 2.0924062728881836\n","Iteration 450, Training loss = 2.09686279296875, Test loss = 2.092287540435791\n","Iteration 451, Training loss = 2.096688985824585, Test loss = 2.0920681953430176\n","Iteration 452, Training loss = 2.0965161323547363, Test loss = 2.091947555541992\n","Iteration 453, Training loss = 2.096343994140625, Test loss = 2.0917344093322754\n","Iteration 454, Training loss = 2.096172332763672, Test loss = 2.0916106700897217\n","Iteration 455, Training loss = 2.096001625061035, Test loss = 2.0914041996002197\n","Iteration 456, Training loss = 2.095832109451294, Test loss = 2.091277837753296\n","Iteration 457, Training loss = 2.095663070678711, Test loss = 2.0910768508911133\n","Iteration 458, Training loss = 2.0954952239990234, Test loss = 2.0909488201141357\n","Iteration 459, Training loss = 2.095327377319336, Test loss = 2.0907516479492188\n","Iteration 460, Training loss = 2.095160722732544, Test loss = 2.090623140335083\n","Iteration 461, Training loss = 2.0949950218200684, Test loss = 2.0904290676116943\n","Iteration 462, Training loss = 2.09483003616333, Test loss = 2.0903022289276123\n","Iteration 463, Training loss = 2.09466552734375, Test loss = 2.0901079177856445\n","Iteration 464, Training loss = 2.0945024490356445, Test loss = 2.0899851322174072\n","Iteration 465, Training loss = 2.094339370727539, Test loss = 2.089790105819702\n","Iteration 466, Training loss = 2.09417724609375, Test loss = 2.0896713733673096\n","Iteration 467, Training loss = 2.0940160751342773, Test loss = 2.0894737243652344\n","Iteration 468, Training loss = 2.093855381011963, Test loss = 2.0893609523773193\n","Iteration 469, Training loss = 2.093695878982544, Test loss = 2.0891590118408203\n","Iteration 470, Training loss = 2.093536376953125, Test loss = 2.0890538692474365\n","Iteration 471, Training loss = 2.0933780670166016, Test loss = 2.088848114013672\n","Iteration 472, Training loss = 2.0932199954986572, Test loss = 2.0887491703033447\n","Iteration 473, Training loss = 2.0930628776550293, Test loss = 2.0885393619537354\n","Iteration 474, Training loss = 2.0929064750671387, Test loss = 2.0884475708007812\n","Iteration 475, Training loss = 2.0927510261535645, Test loss = 2.0882327556610107\n","Iteration 476, Training loss = 2.0925960540771484, Test loss = 2.0881507396698\n","Iteration 477, Training loss = 2.092442512512207, Test loss = 2.0879290103912354\n","Iteration 478, Training loss = 2.092289686203003, Test loss = 2.087860345840454\n","Iteration 479, Training loss = 2.0921382904052734, Test loss = 2.0876269340515137\n","Iteration 480, Training loss = 2.091986656188965, Test loss = 2.0875728130340576\n","Iteration 481, Training loss = 2.091837167739868, Test loss = 2.0873281955718994\n","Iteration 482, Training loss = 2.0916872024536133, Test loss = 2.0872890949249268\n","Iteration 483, Training loss = 2.0915396213531494, Test loss = 2.0870320796966553\n","Iteration 484, Training loss = 2.091390371322632, Test loss = 2.087007999420166\n","Iteration 485, Training loss = 2.09124493598938, Test loss = 2.0867393016815186\n","Iteration 486, Training loss = 2.091097593307495, Test loss = 2.086731433868408\n","Iteration 487, Training loss = 2.0909526348114014, Test loss = 2.0864505767822266\n","Iteration 488, Training loss = 2.0908052921295166, Test loss = 2.0864522457122803\n","Iteration 489, Training loss = 2.090660572052002, Test loss = 2.0861639976501465\n","Iteration 490, Training loss = 2.090512752532959, Test loss = 2.0861706733703613\n","Iteration 491, Training loss = 2.090367555618286, Test loss = 2.0858798027038574\n","Iteration 492, Training loss = 2.090219497680664, Test loss = 2.0858864784240723\n","Iteration 493, Training loss = 2.0900728702545166, Test loss = 2.085597515106201\n","Iteration 494, Training loss = 2.0899243354797363, Test loss = 2.0855956077575684\n","Iteration 495, Training loss = 2.0897769927978516, Test loss = 2.0853168964385986\n","Iteration 496, Training loss = 2.0896284580230713, Test loss = 2.0853018760681152\n","Iteration 497, Training loss = 2.089482069015503, Test loss = 2.0850400924682617\n","Iteration 498, Training loss = 2.0893354415893555, Test loss = 2.085010528564453\n","Iteration 499, Training loss = 2.089190721511841, Test loss = 2.084765672683716\n","Iteration 500, Training loss = 2.089045763015747, Test loss = 2.084723711013794\n","Iteration 501, Training loss = 2.088902235031128, Test loss = 2.0844929218292236\n","Iteration 502, Training loss = 2.088759422302246, Test loss = 2.0844404697418213\n","Iteration 503, Training loss = 2.0886178016662598, Test loss = 2.084221601486206\n","Iteration 504, Training loss = 2.0884757041931152, Test loss = 2.084160566329956\n","Iteration 505, Training loss = 2.0883350372314453, Test loss = 2.0839529037475586\n","Iteration 506, Training loss = 2.0881950855255127, Test loss = 2.083885431289673\n","Iteration 507, Training loss = 2.088054895401001, Test loss = 2.0836849212646484\n","Iteration 508, Training loss = 2.0879154205322266, Test loss = 2.083611249923706\n","Iteration 509, Training loss = 2.0877768993377686, Test loss = 2.083418369293213\n","Iteration 510, Training loss = 2.0876383781433105, Test loss = 2.083340644836426\n","Iteration 511, Training loss = 2.087500810623169, Test loss = 2.08315372467041\n","Iteration 512, Training loss = 2.0873637199401855, Test loss = 2.0830745697021484\n","Iteration 513, Training loss = 2.0872271060943604, Test loss = 2.0828893184661865\n","Iteration 514, Training loss = 2.0870912075042725, Test loss = 2.082810878753662\n","Iteration 515, Training loss = 2.0869553089141846, Test loss = 2.0826263427734375\n","Iteration 516, Training loss = 2.086819887161255, Test loss = 2.0825493335723877\n","Iteration 517, Training loss = 2.086684465408325, Test loss = 2.082364559173584\n","Iteration 518, Training loss = 2.086549997329712, Test loss = 2.0822901725769043\n","Iteration 519, Training loss = 2.0864152908325195, Test loss = 2.082104444503784\n","Iteration 520, Training loss = 2.0862815380096436, Test loss = 2.0820326805114746\n","Iteration 521, Training loss = 2.0861480236053467, Test loss = 2.0818448066711426\n","Iteration 522, Training loss = 2.086014986038208, Test loss = 2.0817790031433105\n","Iteration 523, Training loss = 2.0858829021453857, Test loss = 2.0815858840942383\n","Iteration 524, Training loss = 2.0857510566711426, Test loss = 2.0815298557281494\n","Iteration 525, Training loss = 2.0856196880340576, Test loss = 2.0813286304473877\n","Iteration 526, Training loss = 2.08548903465271, Test loss = 2.081282377243042\n","Iteration 527, Training loss = 2.085359811782837, Test loss = 2.081071376800537\n","Iteration 528, Training loss = 2.085230588912964, Test loss = 2.0810420513153076\n","Iteration 529, Training loss = 2.0851023197174072, Test loss = 2.0808160305023193\n","Iteration 530, Training loss = 2.0849738121032715, Test loss = 2.080803394317627\n","Iteration 531, Training loss = 2.0848464965820312, Test loss = 2.0805625915527344\n","Iteration 532, Training loss = 2.084718704223633, Test loss = 2.080564022064209\n","Iteration 533, Training loss = 2.0845913887023926, Test loss = 2.080310821533203\n","Iteration 534, Training loss = 2.084462881088257, Test loss = 2.080322742462158\n","Iteration 535, Training loss = 2.0843353271484375, Test loss = 2.080061197280884\n","Iteration 536, Training loss = 2.0842068195343018, Test loss = 2.080077886581421\n","Iteration 537, Training loss = 2.084078311920166, Test loss = 2.079813241958618\n","Iteration 538, Training loss = 2.083948850631714, Test loss = 2.079829692840576\n","Iteration 539, Training loss = 2.0838208198547363, Test loss = 2.0795657634735107\n","Iteration 540, Training loss = 2.0836896896362305, Test loss = 2.079576253890991\n","Iteration 541, Training loss = 2.083561897277832, Test loss = 2.079319715499878\n","Iteration 542, Training loss = 2.0834314823150635, Test loss = 2.079324245452881\n","Iteration 543, Training loss = 2.083303213119507, Test loss = 2.0790746212005615\n","Iteration 544, Training loss = 2.083173990249634, Test loss = 2.0790724754333496\n","Iteration 545, Training loss = 2.083045482635498, Test loss = 2.0788307189941406\n","Iteration 546, Training loss = 2.0829169750213623, Test loss = 2.078819751739502\n","Iteration 547, Training loss = 2.0827889442443848, Test loss = 2.0785880088806152\n","Iteration 548, Training loss = 2.0826609134674072, Test loss = 2.0785696506500244\n","Iteration 549, Training loss = 2.082533836364746, Test loss = 2.078346014022827\n","Iteration 550, Training loss = 2.0824062824249268, Test loss = 2.078321695327759\n","Iteration 551, Training loss = 2.0822792053222656, Test loss = 2.078104019165039\n","Iteration 552, Training loss = 2.0821523666381836, Test loss = 2.0780749320983887\n","Iteration 553, Training loss = 2.082026481628418, Test loss = 2.07786226272583\n","Iteration 554, Training loss = 2.081899881362915, Test loss = 2.077829599380493\n","Iteration 555, Training loss = 2.0817744731903076, Test loss = 2.0776209831237793\n","Iteration 556, Training loss = 2.0816493034362793, Test loss = 2.0775883197784424\n","Iteration 557, Training loss = 2.081524133682251, Test loss = 2.077378749847412\n","Iteration 558, Training loss = 2.081400156021118, Test loss = 2.0773518085479736\n","Iteration 559, Training loss = 2.0812764167785645, Test loss = 2.077136993408203\n","Iteration 560, Training loss = 2.08115291595459, Test loss = 2.077116012573242\n","Iteration 561, Training loss = 2.0810294151306152, Test loss = 2.076894998550415\n","Iteration 562, Training loss = 2.0809061527252197, Test loss = 2.07688045501709\n","Iteration 563, Training loss = 2.0807831287384033, Test loss = 2.0766525268554688\n","Iteration 564, Training loss = 2.080659866333008, Test loss = 2.0766444206237793\n","Iteration 565, Training loss = 2.0805373191833496, Test loss = 2.076411247253418\n","Iteration 566, Training loss = 2.080414295196533, Test loss = 2.0764071941375732\n","Iteration 567, Training loss = 2.080291748046875, Test loss = 2.0761709213256836\n","Iteration 568, Training loss = 2.0801684856414795, Test loss = 2.0761702060699463\n","Iteration 569, Training loss = 2.080045461654663, Test loss = 2.075929641723633\n","Iteration 570, Training loss = 2.079922914505005, Test loss = 2.0759313106536865\n","Iteration 571, Training loss = 2.0798003673553467, Test loss = 2.075688362121582\n","Iteration 572, Training loss = 2.0796780586242676, Test loss = 2.075692892074585\n","Iteration 573, Training loss = 2.0795562267303467, Test loss = 2.0754482746124268\n","Iteration 574, Training loss = 2.079434633255005, Test loss = 2.075457811355591\n","Iteration 575, Training loss = 2.079312324523926, Test loss = 2.075209140777588\n","Iteration 576, Training loss = 2.079190254211426, Test loss = 2.0752205848693848\n","Iteration 577, Training loss = 2.0790672302246094, Test loss = 2.0749709606170654\n","Iteration 578, Training loss = 2.078944444656372, Test loss = 2.074979066848755\n","Iteration 579, Training loss = 2.078819990158081, Test loss = 2.0747323036193848\n","Iteration 580, Training loss = 2.0786972045898438, Test loss = 2.074735164642334\n","Iteration 581, Training loss = 2.078573703765869, Test loss = 2.074495315551758\n","Iteration 582, Training loss = 2.0784506797790527, Test loss = 2.0744926929473877\n","Iteration 583, Training loss = 2.0783274173736572, Test loss = 2.074258327484131\n","Iteration 584, Training loss = 2.0782039165496826, Test loss = 2.074248790740967\n","Iteration 585, Training loss = 2.0780820846557617, Test loss = 2.074021577835083\n","Iteration 586, Training loss = 2.077958822250366, Test loss = 2.0740087032318115\n","Iteration 587, Training loss = 2.0778374671936035, Test loss = 2.0737855434417725\n","Iteration 588, Training loss = 2.077714681625366, Test loss = 2.07376766204834\n","Iteration 589, Training loss = 2.0775933265686035, Test loss = 2.0735483169555664\n","Iteration 590, Training loss = 2.077470541000366, Test loss = 2.0735273361206055\n","Iteration 591, Training loss = 2.0773494243621826, Test loss = 2.0733115673065186\n","Iteration 592, Training loss = 2.077227830886841, Test loss = 2.073289394378662\n","Iteration 593, Training loss = 2.077106475830078, Test loss = 2.0730741024017334\n","Iteration 594, Training loss = 2.0769848823547363, Test loss = 2.073051691055298\n","Iteration 595, Training loss = 2.076864242553711, Test loss = 2.0728366374969482\n","Iteration 596, Training loss = 2.076742649078369, Test loss = 2.07281494140625\n","Iteration 597, Training loss = 2.076622486114502, Test loss = 2.0725998878479004\n","Iteration 598, Training loss = 2.0765016078948975, Test loss = 2.072580099105835\n","Iteration 599, Training loss = 2.076380968093872, Test loss = 2.0723628997802734\n","Iteration 600, Training loss = 2.0762603282928467, Test loss = 2.072345018386841\n","Iteration 601, Training loss = 2.0761404037475586, Test loss = 2.0721254348754883\n","Iteration 602, Training loss = 2.076019525527954, Test loss = 2.072110891342163\n","Iteration 603, Training loss = 2.075900077819824, Test loss = 2.071887969970703\n","Iteration 604, Training loss = 2.075779676437378, Test loss = 2.07187819480896\n","Iteration 605, Training loss = 2.0756587982177734, Test loss = 2.071650743484497\n","Iteration 606, Training loss = 2.075538396835327, Test loss = 2.071640729904175\n","Iteration 607, Training loss = 2.0754172801971436, Test loss = 2.07141375541687\n","Iteration 608, Training loss = 2.0752971172332764, Test loss = 2.0714025497436523\n","Iteration 609, Training loss = 2.0751760005950928, Test loss = 2.0711758136749268\n","Iteration 610, Training loss = 2.0750558376312256, Test loss = 2.07116436958313\n","Iteration 611, Training loss = 2.0749340057373047, Test loss = 2.070937395095825\n","Iteration 612, Training loss = 2.074812889099121, Test loss = 2.0709242820739746\n","Iteration 613, Training loss = 2.0746917724609375, Test loss = 2.0706984996795654\n","Iteration 614, Training loss = 2.074570655822754, Test loss = 2.0706841945648193\n","Iteration 615, Training loss = 2.0744495391845703, Test loss = 2.0704598426818848\n","Iteration 616, Training loss = 2.0743284225463867, Test loss = 2.070443868637085\n","Iteration 617, Training loss = 2.0742077827453613, Test loss = 2.0702202320098877\n","Iteration 618, Training loss = 2.0740859508514404, Test loss = 2.0702011585235596\n","Iteration 619, Training loss = 2.073965549468994, Test loss = 2.0699803829193115\n","Iteration 620, Training loss = 2.0738449096679688, Test loss = 2.0699622631073\n","Iteration 621, Training loss = 2.0737245082855225, Test loss = 2.0697402954101562\n","Iteration 622, Training loss = 2.073603630065918, Test loss = 2.0697202682495117\n","Iteration 623, Training loss = 2.0734832286834717, Test loss = 2.0694990158081055\n","Iteration 624, Training loss = 2.073361396789551, Test loss = 2.069476842880249\n","Iteration 625, Training loss = 2.0732412338256836, Test loss = 2.069258689880371\n","Iteration 626, Training loss = 2.0731201171875, Test loss = 2.06923508644104\n","Iteration 627, Training loss = 2.0729992389678955, Test loss = 2.069017171859741\n","Iteration 628, Training loss = 2.072877883911133, Test loss = 2.0689918994903564\n","Iteration 629, Training loss = 2.0727574825286865, Test loss = 2.0687758922576904\n","Iteration 630, Training loss = 2.072636604309082, Test loss = 2.068751096725464\n","Iteration 631, Training loss = 2.072517156600952, Test loss = 2.0685348510742188\n","Iteration 632, Training loss = 2.0723953247070312, Test loss = 2.068509578704834\n","Iteration 633, Training loss = 2.0722744464874268, Test loss = 2.068294048309326\n","Iteration 634, Training loss = 2.0721516609191895, Test loss = 2.0682642459869385\n","Iteration 635, Training loss = 2.0720303058624268, Test loss = 2.068052291870117\n","Iteration 636, Training loss = 2.0719070434570312, Test loss = 2.068018674850464\n","Iteration 637, Training loss = 2.0717852115631104, Test loss = 2.06781005859375\n","Iteration 638, Training loss = 2.0716617107391357, Test loss = 2.067772150039673\n","Iteration 639, Training loss = 2.0715389251708984, Test loss = 2.067565679550171\n","Iteration 640, Training loss = 2.0714163780212402, Test loss = 2.06752610206604\n","Iteration 641, Training loss = 2.0712943077087402, Test loss = 2.067321538925171\n","Iteration 642, Training loss = 2.0711724758148193, Test loss = 2.067284345626831\n","Iteration 643, Training loss = 2.0710506439208984, Test loss = 2.0670788288116455\n","Iteration 644, Training loss = 2.0709285736083984, Test loss = 2.067042350769043\n","Iteration 645, Training loss = 2.0708062648773193, Test loss = 2.0668370723724365\n","Iteration 646, Training loss = 2.0706841945648193, Test loss = 2.0667998790740967\n","Iteration 647, Training loss = 2.0705618858337402, Test loss = 2.066596269607544\n","Iteration 648, Training loss = 2.0704400539398193, Test loss = 2.0665581226348877\n","Iteration 649, Training loss = 2.0703179836273193, Test loss = 2.066354990005493\n","Iteration 650, Training loss = 2.0701961517333984, Test loss = 2.0663182735443115\n","Iteration 651, Training loss = 2.070073366165161, Test loss = 2.066115140914917\n","Iteration 652, Training loss = 2.069951295852661, Test loss = 2.066077470779419\n","Iteration 653, Training loss = 2.069828748703003, Test loss = 2.0658745765686035\n","Iteration 654, Training loss = 2.0697062015533447, Test loss = 2.0658373832702637\n","Iteration 655, Training loss = 2.0695834159851074, Test loss = 2.0656349658966064\n","Iteration 656, Training loss = 2.069460868835449, Test loss = 2.065596580505371\n","Iteration 657, Training loss = 2.069338798522949, Test loss = 2.065396785736084\n","Iteration 658, Training loss = 2.069216728210449, Test loss = 2.0653605461120605\n","Iteration 659, Training loss = 2.069093942642212, Test loss = 2.065160036087036\n","Iteration 660, Training loss = 2.068971633911133, Test loss = 2.0651233196258545\n","Iteration 661, Training loss = 2.0688488483428955, Test loss = 2.0649237632751465\n","Iteration 662, Training loss = 2.0687255859375, Test loss = 2.0648858547210693\n","Iteration 663, Training loss = 2.068603754043579, Test loss = 2.0646870136260986\n","Iteration 664, Training loss = 2.0684802532196045, Test loss = 2.064652442932129\n","Iteration 665, Training loss = 2.068359136581421, Test loss = 2.064450979232788\n","Iteration 666, Training loss = 2.0682363510131836, Test loss = 2.0644214153289795\n","Iteration 667, Training loss = 2.068115472793579, Test loss = 2.064215660095215\n","Iteration 668, Training loss = 2.067992925643921, Test loss = 2.0641918182373047\n","Iteration 669, Training loss = 2.0678722858428955, Test loss = 2.0639808177948\n","Iteration 670, Training loss = 2.0677499771118164, Test loss = 2.0639636516571045\n","Iteration 671, Training loss = 2.067629337310791, Test loss = 2.063749313354492\n","Iteration 672, Training loss = 2.067507266998291, Test loss = 2.0637364387512207\n","Iteration 673, Training loss = 2.0673868656158447, Test loss = 2.063518524169922\n","Iteration 674, Training loss = 2.0672647953033447, Test loss = 2.063506841659546\n","Iteration 675, Training loss = 2.0671439170837402, Test loss = 2.063290596008301\n","Iteration 676, Training loss = 2.0670225620269775, Test loss = 2.0632777214050293\n","Iteration 677, Training loss = 2.0669028759002686, Test loss = 2.0630602836608887\n","Iteration 678, Training loss = 2.0667810440063477, Test loss = 2.0630481243133545\n","Iteration 679, Training loss = 2.066659927368164, Test loss = 2.0628302097320557\n","Iteration 680, Training loss = 2.066537380218506, Test loss = 2.0628175735473633\n","Iteration 681, Training loss = 2.0664162635803223, Test loss = 2.0626022815704346\n","Iteration 682, Training loss = 2.066293716430664, Test loss = 2.062589168548584\n","Iteration 683, Training loss = 2.0661723613739014, Test loss = 2.0623738765716553\n","Iteration 684, Training loss = 2.0660502910614014, Test loss = 2.0623607635498047\n","Iteration 685, Training loss = 2.065929889678955, Test loss = 2.062145233154297\n","Iteration 686, Training loss = 2.065807819366455, Test loss = 2.0621328353881836\n","Iteration 687, Training loss = 2.065687894821167, Test loss = 2.061917781829834\n","Iteration 688, Training loss = 2.065566301345825, Test loss = 2.0619075298309326\n","Iteration 689, Training loss = 2.065446376800537, Test loss = 2.0616915225982666\n","Iteration 690, Training loss = 2.0653252601623535, Test loss = 2.0616824626922607\n","Iteration 691, Training loss = 2.0652055740356445, Test loss = 2.0614659786224365\n","Iteration 692, Training loss = 2.065084934234619, Test loss = 2.0614571571350098\n","Iteration 693, Training loss = 2.0649662017822266, Test loss = 2.0612399578094482\n","Iteration 694, Training loss = 2.0648460388183594, Test loss = 2.0612356662750244\n","Iteration 695, Training loss = 2.064725875854492, Test loss = 2.0610156059265137\n","Iteration 696, Training loss = 2.064605474472046, Test loss = 2.0610105991363525\n","Iteration 697, Training loss = 2.064486503601074, Test loss = 2.0607893466949463\n","Iteration 698, Training loss = 2.064366579055786, Test loss = 2.06078839302063\n","Iteration 699, Training loss = 2.064248561859131, Test loss = 2.0605647563934326\n","Iteration 700, Training loss = 2.0641281604766846, Test loss = 2.060565710067749\n","Iteration 701, Training loss = 2.0640101432800293, Test loss = 2.0603408813476562\n","Iteration 702, Training loss = 2.063889503479004, Test loss = 2.060343027114868\n","Iteration 703, Training loss = 2.063771963119507, Test loss = 2.060117244720459\n","Iteration 704, Training loss = 2.063652276992798, Test loss = 2.0601229667663574\n","Iteration 705, Training loss = 2.0635340213775635, Test loss = 2.059893846511841\n","Iteration 706, Training loss = 2.0634148120880127, Test loss = 2.059901475906372\n","Iteration 707, Training loss = 2.063296318054199, Test loss = 2.059671401977539\n","Iteration 708, Training loss = 2.0631768703460693, Test loss = 2.059678792953491\n","Iteration 709, Training loss = 2.0630593299865723, Test loss = 2.059447765350342\n","Iteration 710, Training loss = 2.0629398822784424, Test loss = 2.0594565868377686\n","Iteration 711, Training loss = 2.062821388244629, Test loss = 2.059225082397461\n","Iteration 712, Training loss = 2.0627012252807617, Test loss = 2.059231996536255\n","Iteration 713, Training loss = 2.062581777572632, Test loss = 2.0590016841888428\n","Iteration 714, Training loss = 2.0624611377716064, Test loss = 2.059002637863159\n","Iteration 715, Training loss = 2.0623409748077393, Test loss = 2.0587785243988037\n","Iteration 716, Training loss = 2.0622196197509766, Test loss = 2.0587730407714844\n","Iteration 717, Training loss = 2.062098741531372, Test loss = 2.05855655670166\n","Iteration 718, Training loss = 2.0619776248931885, Test loss = 2.0585412979125977\n","Iteration 719, Training loss = 2.061856746673584, Test loss = 2.0583338737487793\n","Iteration 720, Training loss = 2.061736822128296, Test loss = 2.0583102703094482\n","Iteration 721, Training loss = 2.0616157054901123, Test loss = 2.0581114292144775\n","Iteration 722, Training loss = 2.0614964962005615, Test loss = 2.0580806732177734\n","Iteration 723, Training loss = 2.0613765716552734, Test loss = 2.0578887462615967\n","Iteration 724, Training loss = 2.0612571239471436, Test loss = 2.057852268218994\n","Iteration 725, Training loss = 2.0611376762390137, Test loss = 2.057666301727295\n","Iteration 726, Training loss = 2.0610179901123047, Test loss = 2.057624578475952\n","Iteration 727, Training loss = 2.060898780822754, Test loss = 2.0574440956115723\n","Iteration 728, Training loss = 2.060778856277466, Test loss = 2.0573973655700684\n","Iteration 729, Training loss = 2.060659646987915, Test loss = 2.0572211742401123\n","Iteration 730, Training loss = 2.0605406761169434, Test loss = 2.057173252105713\n","Iteration 731, Training loss = 2.0604217052459717, Test loss = 2.0569989681243896\n","Iteration 732, Training loss = 2.060302257537842, Test loss = 2.056947708129883\n","Iteration 733, Training loss = 2.06018328666687, Test loss = 2.0567758083343506\n","Iteration 734, Training loss = 2.0600640773773193, Test loss = 2.0567243099212646\n","Iteration 735, Training loss = 2.0599453449249268, Test loss = 2.0565526485443115\n","Iteration 736, Training loss = 2.059825897216797, Test loss = 2.056499719619751\n","Iteration 737, Training loss = 2.0597071647644043, Test loss = 2.0563294887542725\n","Iteration 738, Training loss = 2.0595879554748535, Test loss = 2.056276559829712\n","Iteration 739, Training loss = 2.059469223022461, Test loss = 2.056105613708496\n","Iteration 740, Training loss = 2.0593504905700684, Test loss = 2.0560567378997803\n","Iteration 741, Training loss = 2.059232234954834, Test loss = 2.0558817386627197\n","Iteration 742, Training loss = 2.0591132640838623, Test loss = 2.0558366775512695\n","Iteration 743, Training loss = 2.058995008468628, Test loss = 2.0556576251983643\n","Iteration 744, Training loss = 2.0588762760162354, Test loss = 2.055617570877075\n","Iteration 745, Training loss = 2.058758020401001, Test loss = 2.055433988571167\n","Iteration 746, Training loss = 2.0586395263671875, Test loss = 2.0553982257843018\n","Iteration 747, Training loss = 2.0585219860076904, Test loss = 2.0552093982696533\n","Iteration 748, Training loss = 2.058403968811035, Test loss = 2.055183172225952\n","Iteration 749, Training loss = 2.058286666870117, Test loss = 2.054985761642456\n","Iteration 750, Training loss = 2.0581696033477783, Test loss = 2.054969549179077\n","Iteration 751, Training loss = 2.058053970336914, Test loss = 2.0547616481781006\n","Iteration 752, Training loss = 2.057938575744629, Test loss = 2.054762125015259\n","Iteration 753, Training loss = 2.0578222274780273, Test loss = 2.0545406341552734\n","Iteration 754, Training loss = 2.0577077865600586, Test loss = 2.054553985595703\n","Iteration 755, Training loss = 2.0575897693634033, Test loss = 2.054321050643921\n","Iteration 756, Training loss = 2.057474136352539, Test loss = 2.054340124130249\n","Iteration 757, Training loss = 2.05735445022583, Test loss = 2.054100751876831\n","Iteration 758, Training loss = 2.0572359561920166, Test loss = 2.0541210174560547\n","Iteration 759, Training loss = 2.057114839553833, Test loss = 2.053877830505371\n","Iteration 760, Training loss = 2.0569941997528076, Test loss = 2.0538928508758545\n","Iteration 761, Training loss = 2.0568718910217285, Test loss = 2.0536530017852783\n","Iteration 762, Training loss = 2.056748867034912, Test loss = 2.0536577701568604\n","Iteration 763, Training loss = 2.0566253662109375, Test loss = 2.05342698097229\n","Iteration 764, Training loss = 2.0565011501312256, Test loss = 2.0534164905548096\n","Iteration 765, Training loss = 2.0563769340515137, Test loss = 2.053199529647827\n","Iteration 766, Training loss = 2.0562527179718018, Test loss = 2.0531744956970215\n","Iteration 767, Training loss = 2.056128740310669, Test loss = 2.0529701709747314\n","Iteration 768, Training loss = 2.0560050010681152, Test loss = 2.052933692932129\n","Iteration 769, Training loss = 2.055881977081299, Test loss = 2.0527405738830566\n","Iteration 770, Training loss = 2.0557591915130615, Test loss = 2.0526959896087646\n","Iteration 771, Training loss = 2.0556366443634033, Test loss = 2.052511215209961\n","Iteration 772, Training loss = 2.055513858795166, Test loss = 2.0524582862854004\n","Iteration 773, Training loss = 2.055391311645508, Test loss = 2.0522823333740234\n","Iteration 774, Training loss = 2.055269241333008, Test loss = 2.0522239208221436\n","Iteration 775, Training loss = 2.055147171020508, Test loss = 2.0520522594451904\n","Iteration 776, Training loss = 2.0550243854522705, Test loss = 2.0519893169403076\n","Iteration 777, Training loss = 2.0549023151397705, Test loss = 2.0518219470977783\n","Iteration 778, Training loss = 2.0547800064086914, Test loss = 2.051757335662842\n","Iteration 779, Training loss = 2.054657459259033, Test loss = 2.051591634750366\n","Iteration 780, Training loss = 2.054534912109375, Test loss = 2.051525354385376\n","Iteration 781, Training loss = 2.054412364959717, Test loss = 2.051359176635742\n","Iteration 782, Training loss = 2.054290294647217, Test loss = 2.0512936115264893\n","Iteration 783, Training loss = 2.054168224334717, Test loss = 2.0511257648468018\n","Iteration 784, Training loss = 2.054046154022217, Test loss = 2.051063299179077\n","Iteration 785, Training loss = 2.053924560546875, Test loss = 2.0508930683135986\n","Iteration 786, Training loss = 2.053802490234375, Test loss = 2.0508341789245605\n","Iteration 787, Training loss = 2.0536816120147705, Test loss = 2.0506606101989746\n","Iteration 788, Training loss = 2.0535597801208496, Test loss = 2.050607204437256\n","Iteration 789, Training loss = 2.053438186645508, Test loss = 2.050429105758667\n","Iteration 790, Training loss = 2.0533177852630615, Test loss = 2.0503804683685303\n","Iteration 791, Training loss = 2.0531957149505615, Test loss = 2.0501973628997803\n","Iteration 792, Training loss = 2.053074598312378, Test loss = 2.05015230178833\n","Iteration 793, Training loss = 2.052952766418457, Test loss = 2.049966812133789\n","Iteration 794, Training loss = 2.0528314113616943, Test loss = 2.049924612045288\n","Iteration 795, Training loss = 2.0527100563049316, Test loss = 2.049736976623535\n","Iteration 796, Training loss = 2.052588701248169, Test loss = 2.0496981143951416\n","Iteration 797, Training loss = 2.0524659156799316, Test loss = 2.049506425857544\n","Iteration 798, Training loss = 2.05234432220459, Test loss = 2.0494680404663086\n","Iteration 799, Training loss = 2.0522212982177734, Test loss = 2.0492758750915527\n","Iteration 800, Training loss = 2.0520997047424316, Test loss = 2.049238681793213\n","Iteration 801, Training loss = 2.051976203918457, Test loss = 2.0490453243255615\n","Iteration 802, Training loss = 2.051853656768799, Test loss = 2.049006700515747\n","Iteration 803, Training loss = 2.051729917526245, Test loss = 2.048814296722412\n","Iteration 804, Training loss = 2.0516059398651123, Test loss = 2.0487706661224365\n","Iteration 805, Training loss = 2.051481008529663, Test loss = 2.0485827922821045\n","Iteration 806, Training loss = 2.0513570308685303, Test loss = 2.0485336780548096\n","Iteration 807, Training loss = 2.051231622695923, Test loss = 2.048351526260376\n","Iteration 808, Training loss = 2.0511064529418945, Test loss = 2.0482943058013916\n","Iteration 809, Training loss = 2.050981044769287, Test loss = 2.0481181144714355\n","Iteration 810, Training loss = 2.050855875015259, Test loss = 2.0480546951293945\n","Iteration 811, Training loss = 2.0507307052612305, Test loss = 2.047884464263916\n","Iteration 812, Training loss = 2.0506057739257812, Test loss = 2.047816753387451\n","Iteration 813, Training loss = 2.050481081008911, Test loss = 2.047651529312134\n","Iteration 814, Training loss = 2.050356149673462, Test loss = 2.047579765319824\n","Iteration 815, Training loss = 2.0502309799194336, Test loss = 2.047419786453247\n","Iteration 816, Training loss = 2.0501065254211426, Test loss = 2.0473430156707764\n","Iteration 817, Training loss = 2.0499825477600098, Test loss = 2.0471866130828857\n","Iteration 818, Training loss = 2.049858570098877, Test loss = 2.0471081733703613\n","Iteration 819, Training loss = 2.049734592437744, Test loss = 2.0469539165496826\n","Iteration 820, Training loss = 2.0496113300323486, Test loss = 2.0468757152557373\n","Iteration 821, Training loss = 2.049487590789795, Test loss = 2.046722173690796\n","Iteration 822, Training loss = 2.049363613128662, Test loss = 2.0466415882110596\n","Iteration 823, Training loss = 2.0492396354675293, Test loss = 2.0464909076690674\n","Iteration 824, Training loss = 2.0491156578063965, Test loss = 2.0464084148406982\n","Iteration 825, Training loss = 2.0489931106567383, Test loss = 2.046259641647339\n","Iteration 826, Training loss = 2.0488691329956055, Test loss = 2.0461761951446533\n","Iteration 827, Training loss = 2.0487465858459473, Test loss = 2.046027898788452\n","Iteration 828, Training loss = 2.0486230850219727, Test loss = 2.0459442138671875\n","Iteration 829, Training loss = 2.048499584197998, Test loss = 2.0457968711853027\n","Iteration 830, Training loss = 2.0483763217926025, Test loss = 2.0457119941711426\n","Iteration 831, Training loss = 2.048253059387207, Test loss = 2.0455667972564697\n","Iteration 832, Training loss = 2.048129081726074, Test loss = 2.045478343963623\n","Iteration 833, Training loss = 2.048006057739258, Test loss = 2.0453364849090576\n","Iteration 834, Training loss = 2.047881603240967, Test loss = 2.04524564743042\n","Iteration 835, Training loss = 2.047758102416992, Test loss = 2.0451059341430664\n","Iteration 836, Training loss = 2.047633409500122, Test loss = 2.045011043548584\n","Iteration 837, Training loss = 2.0475096702575684, Test loss = 2.044875144958496\n","Iteration 838, Training loss = 2.047384738922119, Test loss = 2.044776678085327\n","Iteration 839, Training loss = 2.0472609996795654, Test loss = 2.044642448425293\n","Iteration 840, Training loss = 2.047135829925537, Test loss = 2.044541835784912\n","Iteration 841, Training loss = 2.0470125675201416, Test loss = 2.044409990310669\n","Iteration 842, Training loss = 2.046887159347534, Test loss = 2.0443074703216553\n","Iteration 843, Training loss = 2.0467638969421387, Test loss = 2.044175624847412\n","Iteration 844, Training loss = 2.0466396808624268, Test loss = 2.0440735816955566\n","Iteration 845, Training loss = 2.0465166568756104, Test loss = 2.0439419746398926\n","Iteration 846, Training loss = 2.0463922023773193, Test loss = 2.0438406467437744\n","Iteration 847, Training loss = 2.0462677478790283, Test loss = 2.0437073707580566\n","Iteration 848, Training loss = 2.0461432933807373, Test loss = 2.0436058044433594\n","Iteration 849, Training loss = 2.046018123626709, Test loss = 2.043471574783325\n","Iteration 850, Training loss = 2.045893430709839, Test loss = 2.043368339538574\n","Iteration 851, Training loss = 2.045769453048706, Test loss = 2.0432353019714355\n","Iteration 852, Training loss = 2.0456435680389404, Test loss = 2.04313063621521\n","Iteration 853, Training loss = 2.0455191135406494, Test loss = 2.0430006980895996\n","Iteration 854, Training loss = 2.045393705368042, Test loss = 2.042894124984741\n","Iteration 855, Training loss = 2.045269250869751, Test loss = 2.0427634716033936\n","Iteration 856, Training loss = 2.0451433658599854, Test loss = 2.0426552295684814\n","Iteration 857, Training loss = 2.0450186729431152, Test loss = 2.042525291442871\n","Iteration 858, Training loss = 2.0448927879333496, Test loss = 2.042417287826538\n","Iteration 859, Training loss = 2.044767379760742, Test loss = 2.0422863960266113\n","Iteration 860, Training loss = 2.0446407794952393, Test loss = 2.042177677154541\n","Iteration 861, Training loss = 2.0445151329040527, Test loss = 2.042046070098877\n","Iteration 862, Training loss = 2.044388771057129, Test loss = 2.041937828063965\n","Iteration 863, Training loss = 2.0442631244659424, Test loss = 2.041804790496826\n","Iteration 864, Training loss = 2.0441360473632812, Test loss = 2.041696071624756\n","Iteration 865, Training loss = 2.0440101623535156, Test loss = 2.041562795639038\n","Iteration 866, Training loss = 2.0438833236694336, Test loss = 2.041454315185547\n","Iteration 867, Training loss = 2.043757200241089, Test loss = 2.041321277618408\n","Iteration 868, Training loss = 2.0436296463012695, Test loss = 2.0412137508392334\n","Iteration 869, Training loss = 2.0435030460357666, Test loss = 2.041078567504883\n","Iteration 870, Training loss = 2.04337477684021, Test loss = 2.0409722328186035\n","Iteration 871, Training loss = 2.043246269226074, Test loss = 2.040834665298462\n","Iteration 872, Training loss = 2.0431177616119385, Test loss = 2.0407280921936035\n","Iteration 873, Training loss = 2.0429880619049072, Test loss = 2.04058837890625\n","Iteration 874, Training loss = 2.042858123779297, Test loss = 2.040480613708496\n","Iteration 875, Training loss = 2.0427286624908447, Test loss = 2.0403411388397217\n","Iteration 876, Training loss = 2.042598247528076, Test loss = 2.0402345657348633\n","Iteration 877, Training loss = 2.042468786239624, Test loss = 2.0400936603546143\n","Iteration 878, Training loss = 2.0423388481140137, Test loss = 2.039990186691284\n","Iteration 879, Training loss = 2.042208671569824, Test loss = 2.0398452281951904\n","Iteration 880, Training loss = 2.0420777797698975, Test loss = 2.039743185043335\n","Iteration 881, Training loss = 2.0419466495513916, Test loss = 2.039597511291504\n","Iteration 882, Training loss = 2.0418152809143066, Test loss = 2.0394952297210693\n","Iteration 883, Training loss = 2.0416834354400635, Test loss = 2.039348840713501\n","Iteration 884, Training loss = 2.041550874710083, Test loss = 2.039245367050171\n","Iteration 885, Training loss = 2.0414187908172607, Test loss = 2.0390994548797607\n","Iteration 886, Training loss = 2.0412864685058594, Test loss = 2.038996696472168\n","Iteration 887, Training loss = 2.0411536693573, Test loss = 2.0388500690460205\n","Iteration 888, Training loss = 2.041020393371582, Test loss = 2.038745164871216\n","Iteration 889, Training loss = 2.040886163711548, Test loss = 2.0385987758636475\n","Iteration 890, Training loss = 2.0407514572143555, Test loss = 2.038489818572998\n","Iteration 891, Training loss = 2.0406181812286377, Test loss = 2.038346290588379\n","Iteration 892, Training loss = 2.040482521057129, Test loss = 2.0382351875305176\n","Iteration 893, Training loss = 2.0403482913970947, Test loss = 2.0380918979644775\n","Iteration 894, Training loss = 2.040212392807007, Test loss = 2.0379812717437744\n","Iteration 895, Training loss = 2.0400776863098145, Test loss = 2.0378363132476807\n","Iteration 896, Training loss = 2.0399415493011475, Test loss = 2.037726640701294\n","Iteration 897, Training loss = 2.0398054122924805, Test loss = 2.0375795364379883\n","Iteration 898, Training loss = 2.039668321609497, Test loss = 2.037471055984497\n","Iteration 899, Training loss = 2.039530038833618, Test loss = 2.037322759628296\n","Iteration 900, Training loss = 2.0393922328948975, Test loss = 2.03721284866333\n","Iteration 901, Training loss = 2.0392541885375977, Test loss = 2.0370633602142334\n","Iteration 902, Training loss = 2.0391154289245605, Test loss = 2.0369553565979004\n","Iteration 903, Training loss = 2.0389766693115234, Test loss = 2.0368025302886963\n","Iteration 904, Training loss = 2.038839101791382, Test loss = 2.0367002487182617\n","Iteration 905, Training loss = 2.038701057434082, Test loss = 2.0365405082702637\n","Iteration 906, Training loss = 2.0385634899139404, Test loss = 2.0364437103271484\n","Iteration 907, Training loss = 2.038424491882324, Test loss = 2.036278009414673\n","Iteration 908, Training loss = 2.0382862091064453, Test loss = 2.036184549331665\n","Iteration 909, Training loss = 2.0381479263305664, Test loss = 2.036013603210449\n","Iteration 910, Training loss = 2.03800892829895, Test loss = 2.0359270572662354\n","Iteration 911, Training loss = 2.037869691848755, Test loss = 2.0357508659362793\n","Iteration 912, Training loss = 2.037729263305664, Test loss = 2.035665273666382\n","Iteration 913, Training loss = 2.037588119506836, Test loss = 2.035487174987793\n","Iteration 914, Training loss = 2.0374462604522705, Test loss = 2.035402297973633\n","Iteration 915, Training loss = 2.0373053550720215, Test loss = 2.0352210998535156\n","Iteration 916, Training loss = 2.037161350250244, Test loss = 2.0351386070251465\n","Iteration 917, Training loss = 2.0370192527770996, Test loss = 2.03495192527771\n","Iteration 918, Training loss = 2.036874771118164, Test loss = 2.034869432449341\n","Iteration 919, Training loss = 2.0367307662963867, Test loss = 2.0346813201904297\n","Iteration 920, Training loss = 2.0365843772888184, Test loss = 2.0345964431762695\n","Iteration 921, Training loss = 2.0364391803741455, Test loss = 2.034407138824463\n","Iteration 922, Training loss = 2.036292314529419, Test loss = 2.0343217849731445\n","Iteration 923, Training loss = 2.036147356033325, Test loss = 2.0341296195983887\n","Iteration 924, Training loss = 2.0359973907470703, Test loss = 2.0340416431427\n","Iteration 925, Training loss = 2.03585147857666, Test loss = 2.0338499546051025\n","Iteration 926, Training loss = 2.0357017517089844, Test loss = 2.0337605476379395\n","Iteration 927, Training loss = 2.035554885864258, Test loss = 2.033566474914551\n","Iteration 928, Training loss = 2.035405158996582, Test loss = 2.033475875854492\n","Iteration 929, Training loss = 2.0352590084075928, Test loss = 2.0332837104797363\n","Iteration 930, Training loss = 2.035109281539917, Test loss = 2.033191204071045\n","Iteration 931, Training loss = 2.0349624156951904, Test loss = 2.0329976081848145\n","Iteration 932, Training loss = 2.0348124504089355, Test loss = 2.0329031944274902\n","Iteration 933, Training loss = 2.0346646308898926, Test loss = 2.032708168029785\n","Iteration 934, Training loss = 2.0345137119293213, Test loss = 2.0326106548309326\n","Iteration 935, Training loss = 2.034364700317383, Test loss = 2.032416582107544\n","Iteration 936, Training loss = 2.0342135429382324, Test loss = 2.032317876815796\n","Iteration 937, Training loss = 2.0340631008148193, Test loss = 2.032123327255249\n","Iteration 938, Training loss = 2.0339112281799316, Test loss = 2.0320191383361816\n","Iteration 939, Training loss = 2.0337607860565186, Test loss = 2.0318281650543213\n","Iteration 940, Training loss = 2.033608913421631, Test loss = 2.031719207763672\n","Iteration 941, Training loss = 2.033459186553955, Test loss = 2.031529188156128\n","Iteration 942, Training loss = 2.033306837081909, Test loss = 2.031419277191162\n","Iteration 943, Training loss = 2.033156156539917, Test loss = 2.031226634979248\n","Iteration 944, Training loss = 2.033003807067871, Test loss = 2.0311169624328613\n","Iteration 945, Training loss = 2.032851457595825, Test loss = 2.03092098236084\n","Iteration 946, Training loss = 2.0326976776123047, Test loss = 2.030808687210083\n","Iteration 947, Training loss = 2.0325450897216797, Test loss = 2.030611276626587\n","Iteration 948, Training loss = 2.03239107131958, Test loss = 2.0304999351501465\n","Iteration 949, Training loss = 2.0322370529174805, Test loss = 2.0302958488464355\n","Iteration 950, Training loss = 2.0320818424224854, Test loss = 2.030186414718628\n","Iteration 951, Training loss = 2.0319273471832275, Test loss = 2.029975414276123\n","Iteration 952, Training loss = 2.0317704677581787, Test loss = 2.0298688411712646\n","Iteration 953, Training loss = 2.0316152572631836, Test loss = 2.0296528339385986\n","Iteration 954, Training loss = 2.031458854675293, Test loss = 2.029550313949585\n","Iteration 955, Training loss = 2.0313034057617188, Test loss = 2.0293281078338623\n","Iteration 956, Training loss = 2.0311481952667236, Test loss = 2.029235601425171\n","Iteration 957, Training loss = 2.0309906005859375, Test loss = 2.0290017127990723\n","Iteration 958, Training loss = 2.030834674835205, Test loss = 2.028917074203491\n","Iteration 959, Training loss = 2.030677080154419, Test loss = 2.028672933578491\n","Iteration 960, Training loss = 2.0305187702178955, Test loss = 2.0285959243774414\n","Iteration 961, Training loss = 2.030359983444214, Test loss = 2.0283401012420654\n","Iteration 962, Training loss = 2.030198335647583, Test loss = 2.028268337249756\n","Iteration 963, Training loss = 2.0300393104553223, Test loss = 2.028005361557007\n","Iteration 964, Training loss = 2.0298779010772705, Test loss = 2.0279412269592285\n","Iteration 965, Training loss = 2.0297179222106934, Test loss = 2.0276694297790527\n","Iteration 966, Training loss = 2.0295560359954834, Test loss = 2.0276131629943848\n","Iteration 967, Training loss = 2.0293943881988525, Test loss = 2.0273313522338867\n","Iteration 968, Training loss = 2.0292296409606934, Test loss = 2.02728009223938\n","Iteration 969, Training loss = 2.029066562652588, Test loss = 2.026989459991455\n","Iteration 970, Training loss = 2.0289015769958496, Test loss = 2.0269429683685303\n","Iteration 971, Training loss = 2.0287373065948486, Test loss = 2.0266432762145996\n","Iteration 972, Training loss = 2.028571367263794, Test loss = 2.026601552963257\n","Iteration 973, Training loss = 2.0284059047698975, Test loss = 2.026296377182007\n","Iteration 974, Training loss = 2.0282390117645264, Test loss = 2.0262532234191895\n","Iteration 975, Training loss = 2.0280725955963135, Test loss = 2.025946855545044\n","Iteration 976, Training loss = 2.027905225753784, Test loss = 2.0259039402008057\n","Iteration 977, Training loss = 2.027737617492676, Test loss = 2.025594711303711\n","Iteration 978, Training loss = 2.0275697708129883, Test loss = 2.02555251121521\n","Iteration 979, Training loss = 2.0274014472961426, Test loss = 2.025243043899536\n","Iteration 980, Training loss = 2.0272340774536133, Test loss = 2.0252013206481934\n","Iteration 981, Training loss = 2.027064800262451, Test loss = 2.024890184402466\n","Iteration 982, Training loss = 2.0268969535827637, Test loss = 2.024848222732544\n","Iteration 983, Training loss = 2.026726722717285, Test loss = 2.02453875541687\n","Iteration 984, Training loss = 2.0265581607818604, Test loss = 2.024491310119629\n","Iteration 985, Training loss = 2.0263872146606445, Test loss = 2.0241854190826416\n","Iteration 986, Training loss = 2.026216983795166, Test loss = 2.0241315364837646\n","Iteration 987, Training loss = 2.0260446071624756, Test loss = 2.023831605911255\n","Iteration 988, Training loss = 2.0258731842041016, Test loss = 2.0237679481506348\n","Iteration 989, Training loss = 2.025700330734253, Test loss = 2.0234758853912354\n","Iteration 990, Training loss = 2.0255284309387207, Test loss = 2.023404598236084\n","Iteration 991, Training loss = 2.0253570079803467, Test loss = 2.023116111755371\n","Iteration 992, Training loss = 2.02518630027771, Test loss = 2.023045063018799\n","Iteration 993, Training loss = 2.025015115737915, Test loss = 2.022754192352295\n","Iteration 994, Training loss = 2.02484393119812, Test loss = 2.022686004638672\n","Iteration 995, Training loss = 2.024672746658325, Test loss = 2.022393226623535\n","Iteration 996, Training loss = 2.0245025157928467, Test loss = 2.0223283767700195\n","Iteration 997, Training loss = 2.02433180809021, Test loss = 2.022029161453247\n","Iteration 998, Training loss = 2.024162769317627, Test loss = 2.02197527885437\n","Iteration 999, Training loss = 2.023991584777832, Test loss = 2.0216662883758545\n","Iteration 1000, Training loss = 2.023822546005249, Test loss = 2.0216197967529297\n"]}]},{"cell_type":"markdown","source":["---\n","\n","The following sequence of code snippets are mostly derived from Andrej Karpathy's superb tutorial on *Let's build GPT: from scratch, in code, spelled out* available via https://youtu.be/kCc8FmEb1nY?feature=shared\n","\n","Additional modifications are done to elaborate on the details from Karpathy's tutorial\n","\n","---"],"metadata":{"id":"de1ASiTGTDOe"}},{"cell_type":"markdown","source":["---\n","\n","Load, print, and look at the Shakespeare dataset\n","\n","---"],"metadata":{"id":"RfkgQKIMTYMe"}},{"cell_type":"code","source":["# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"],"metadata":{"id":"EakRJEwsTd2u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657153207,"user_tz":-330,"elapsed":804,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"18dbf452-643e-42ea-b6d0-f4dc7ee05943"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-12-08 11:25:52--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: input.txt\n","\n","input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n","\n","2024-12-08 11:25:52 (46.2 MB/s) - input.txt saved [1115394/1115394]\n","\n"]}]},{"cell_type":"code","source":["# Read it in to inspect it\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()"],"metadata":{"id":"d1oPbDa6Tq9R","executionInfo":{"status":"ok","timestamp":1733657395673,"user_tz":-330,"elapsed":1007,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":134,"outputs":[]},{"cell_type":"code","source":["print(\"length of dataset in characters: \", len(text))"],"metadata":{"id":"rnC0K17iTxIh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657396864,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"0a9b1e26-e456-48e9-cdc5-d91631410766"},"execution_count":135,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters:  1115394\n"]}]},{"cell_type":"code","source":["# Let's look at the first 1000 characters\n","print(text[:1000])"],"metadata":{"id":"mDv8dANRTyyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657397481,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"2a60a6ef-800a-42e0-c5a0-e19062b54b35"},"execution_count":136,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n","Is't a verdict?\n","\n","All:\n","No more talking on't; let it be done: away, away!\n","\n","Second Citizen:\n","One word, good citizens.\n","\n","First Citizen:\n","We are accounted poor citizens, the patricians good.\n","What authority surfeits on would relieve us: if they\n","would yield us but the superfluity, while it were\n","wholesome, we might guess they relieved us humanely;\n","but they think we are too dear: the leanness that\n","afflicts us, the object of our misery, is as an\n","inventory to particularise their abundance; our\n","sufferance is a gain to them Let us revenge this with\n","our pikes, ere we become rakes: for the gods know I\n","speak this in hunger for bread, not in thirst for revenge.\n","\n","\n"]}]},{"cell_type":"code","source":["# Here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"id":"rOTE1Ur1T2TP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657423899,"user_tz":-330,"elapsed":528,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"a46237e2-dc54-44d1-f412-670d7259e45b"},"execution_count":137,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","65\n"]}]},{"cell_type":"code","source":["# Create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"hii there\"))\n","print(decode(encode(\"hii there\")))"],"metadata":{"id":"GCwf-Ry9T7VY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657426852,"user_tz":-330,"elapsed":486,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"4d1251b2-e0ce-4299-bbaf-5a8b64e5b9e7"},"execution_count":138,"outputs":[{"output_type":"stream","name":"stdout","text":["[46, 47, 47, 1, 58, 46, 43, 56, 43]\n","hii there\n"]}]},{"cell_type":"markdown","source":["---\n","\n","\n","ChatGPT's tiktoken library\n","\n","---"],"metadata":{"id":"SFPIE9yVT_2E"}},{"cell_type":"code","source":["# ChatGPTs tiktoken library (codebook size 50257)\n","!pip install tiktoken\n","import tiktoken"],"metadata":{"id":"uwDpzfgwUE2G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657433507,"user_tz":-330,"elapsed":4158,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"8c531199-6233-4dcb-8a19-a6f0127989b3"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n","Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[91m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.8.0\n"]}]},{"cell_type":"code","source":["# Using ChatGPTs tiktoken to tokenize\n","enc =tiktoken.get_encoding('gpt2')\n","print(enc.n_vocab)\n","enc.encode(\"hii there\")"],"metadata":{"id":"4L4nf5dWUHmT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657436806,"user_tz":-330,"elapsed":3305,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"05469cb1-7785-4124-ad49-4d27d7b1f86e"},"execution_count":140,"outputs":[{"output_type":"stream","name":"stdout","text":["50257\n"]},{"output_type":"execute_result","data":{"text/plain":["[71, 4178, 612]"]},"metadata":{},"execution_count":140}]},{"cell_type":"code","source":["# Let's now encode the entire Shakespeare dataset and store it into a torch.Tensor\n","data = torch.tensor(encode(text), dtype = torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"],"metadata":{"id":"4xntJrdiULJR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657436807,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"ffd3f48b-5734-4276-845b-027b1b283648"},"execution_count":141,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1115394]) torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n","         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n","        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n","         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n","        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n","         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n","        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n","        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n","         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n","         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n","        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n","        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n","         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n","        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n","        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n","        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n","        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n","        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n","        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n","        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n","         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n","         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n","         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n","        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n","        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n","        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n","        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n","        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n","        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n","        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n","         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n","         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n","        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n","        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n","        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n","         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n","        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n","        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n","         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n","        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n","        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n","        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n","        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n","        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n","        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n","        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n","        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n","        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n","         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n","        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n","        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n","        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n","        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n","        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n","        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"]}]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"Pm5HdXYDUXQV","executionInfo":{"status":"ok","timestamp":1733657436807,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":142,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"id":"wGZ6cW60Uaif","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657436807,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"bd6b734e-cf08-4a70-f66e-0a8b97f92754"},"execution_count":143,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"]},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"when input is {context} the target: {target}\")"],"metadata":{"id":"bo9P6UiwUdD5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733657437327,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"58ae9da2-bc6e-4d65-f57e-c61396eba4bc"},"execution_count":144,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([18]) the target: 47\n","when input is tensor([18, 47]) the target: 56\n","when input is tensor([18, 47, 56]) the target: 57\n","when input is tensor([18, 47, 56, 57]) the target: 58\n","when input is tensor([18, 47, 56, 57, 58]) the target: 1\n","when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n","when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n","when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"id":"HMMIFNDpUfSN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733659547473,"user_tz":-330,"elapsed":487,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"93286423-e127-4464-ca32-2357734db95e"},"execution_count":165,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])\n","----\n","when input is [24] the target: 43\n","when input is [24, 43] the target: 58\n","when input is [24, 43, 58] the target: 5\n","when input is [24, 43, 58, 5] the target: 57\n","when input is [24, 43, 58, 5, 57] the target: 1\n","when input is [24, 43, 58, 5, 57, 1] the target: 46\n","when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n","when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n","when input is [44] the target: 53\n","when input is [44, 53] the target: 56\n","when input is [44, 53, 56] the target: 1\n","when input is [44, 53, 56, 1] the target: 58\n","when input is [44, 53, 56, 1, 58] the target: 46\n","when input is [44, 53, 56, 1, 58, 46] the target: 39\n","when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n","when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n","when input is [52] the target: 58\n","when input is [52, 58] the target: 1\n","when input is [52, 58, 1] the target: 58\n","when input is [52, 58, 1, 58] the target: 46\n","when input is [52, 58, 1, 58, 46] the target: 39\n","when input is [52, 58, 1, 58, 46, 39] the target: 58\n","when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n","when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n","when input is [25] the target: 17\n","when input is [25, 17] the target: 27\n","when input is [25, 17, 27] the target: 10\n","when input is [25, 17, 27, 10] the target: 0\n","when input is [25, 17, 27, 10, 0] the target: 21\n","when input is [25, 17, 27, 10, 0, 21] the target: 1\n","when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n","when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"]}]},{"cell_type":"markdown","source":["---\n","\n","A quick introduction to embeddings in PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","\n","---"],"metadata":{"id":"uzM9Bwp1Uiq4"}},{"cell_type":"code","source":["emb = torch.nn.Embedding(10,4)\n","emb\n","emb.weight\n","input = torch.LongTensor([[1,3],[5,5]])\n","print(input)\n","emb(input)"],"metadata":{"id":"4G2kMugXUqlx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733659524486,"user_tz":-330,"elapsed":463,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"2794ca92-9f6a-4fd3-d373-20909c66e972"},"execution_count":163,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 3],\n","        [5, 5]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.9420, -0.1418,  2.5431, -1.0991],\n","         [-1.3211, -0.3692, -0.0046,  1.9581]],\n","\n","        [[-0.9410, -0.1522,  0.1742, -1.4833],\n","         [-0.9410, -0.1522,  0.1742, -1.4833]]], grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":163}]},{"cell_type":"markdown","source":["---\n","\n","Build a simple Bigram model class\n","\n","---"],"metadata":{"id":"DIZU0rJVUsC6"}},{"cell_type":"code","source":["emb= torch.nn.Embedding(65,65)\n","emb(xb).shape\n","# emb(xb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JgiNzQZJkxV","executionInfo":{"status":"ok","timestamp":1733659854698,"user_tz":-330,"elapsed":481,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"48a8d053-bc63-40fb-83b3-6851aea99814"},"execution_count":172,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 65])"]},"metadata":{},"execution_count":172}]},{"cell_type":"code","source":["xb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unQW4KniLigP","executionInfo":{"status":"ok","timestamp":1733659867691,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"d71d0e50-2017-464d-a36d-d27dce8630cf"},"execution_count":174,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])"]},"metadata":{},"execution_count":174}]},{"cell_type":"code","source":["yb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tymHB2P2LkC1","executionInfo":{"status":"ok","timestamp":1733659872308,"user_tz":-330,"elapsed":625,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"aceee8ae-603f-493b-a750-2b77b005af55"},"execution_count":175,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])"]},"metadata":{},"execution_count":175}]},{"cell_type":"code","source":["emb= torch.nn.Embedding(65,65)\n","T = emb(xb)\n","T[0,0,:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qn0E2vdrKyr9","executionInfo":{"status":"ok","timestamp":1733659857692,"user_tz":-330,"elapsed":473,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"a1a46a34-a440-48f6-b35d-0aad3217e110"},"execution_count":173,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.4288,  0.9174,  0.7843,  0.9944, -0.2693, -0.9706,  0.6682, -1.2371,\n","        -0.1279, -0.5237,  0.8821, -1.0130, -1.3017,  0.5728, -1.0267,  0.5128,\n","         2.8340, -0.8991,  1.9791,  0.1187, -0.8696,  0.3010, -0.8921,  1.0314,\n","         1.1101,  0.2019,  1.1272, -0.1387, -1.2324, -0.3435,  1.1828, -1.1266,\n","         0.1003,  0.9391, -0.4941,  1.0841, -1.4850,  2.2035, -0.2034,  0.9101,\n","        -2.4893,  1.7413,  0.5102, -2.6755, -0.2316,  1.2472,  0.1332,  1.3809,\n","        -1.4396,  0.0324, -0.6490,  1.0231, -0.5780, -1.5587, -1.8142, -0.1949,\n","        -0.1895, -0.4669,  0.0115,  0.2671,  1.5299, -0.6339,  1.6854, -1.2475,\n","        -0.1250], grad_fn=<SliceBackward0>)"]},"metadata":{},"execution_count":173}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","\n","class BigramLanguageModel(torch.nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = torch.nn.functional.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"],"metadata":{"id":"mfpGkNztU0TL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733660666531,"user_tz":-330,"elapsed":492,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"2ebf9906-899c-48dd-9b0d-add7b4152fd7"},"execution_count":178,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 65])\n","tensor(4.8786, grad_fn=<NllLossBackward0>)\n","\n","Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"]}]},{"cell_type":"markdown","source":["---\n","\n","\n","Train the bigram model\n","\n","---"],"metadata":{"id":"zy984wO9VCNi"}},{"cell_type":"code","source":["# Optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n","\n","batch_size = 32\n","for steps in range(10000): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())"],"metadata":{"id":"BVh6HIDmVELI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733658744985,"user_tz":-330,"elapsed":22208,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"aff8cdc1-6abc-4b31-95d2-9b826c820d76"},"execution_count":147,"outputs":[{"output_type":"stream","name":"stdout","text":["2.5727508068084717\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Generate next set of tokens using the trained Bigram model\n","\n","---"],"metadata":{"id":"NMDLEYORVZL7"}},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"id":"_NgmH2oLVVeA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733658744986,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"b638d6fe-e009-45b1-ef3f-bca4144b96fa"},"execution_count":148,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Iyoteng h hasbe pave pirance\n","Rie hicomyonthar's\n","Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n","KIN d pe wither vouprrouthercc.\n","hathe; d!\n","My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n","h hay.JUCle n prids, r loncave w hollular s O:\n","HIs; ht anjx?\n","\n","DUThinqunt.\n","\n","LaZAnde.\n","athave l.\n","KEONH:\n","ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n","PlorseelapinghiybHen yof GLUCEN t l-t E:\n","I hisgothers je are!-e!\n","QLYotouciullle'z\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Basics of the self attention model\n","\n","---"],"metadata":{"id":"_d1-4ERdgNys"}},{"cell_type":"code","source":["torch.manual_seed(1337)\n","B,T,C = 4,8,2 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"id":"uYAfKEccgRS4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733658751816,"user_tz":-330,"elapsed":551,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"ad6794d2-b3cc-4937-afa1-bbe09f71b20b"},"execution_count":149,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":149}]},{"cell_type":"markdown","source":["---\n","\n","Attention using averages\n","\n","---"],"metadata":{"id":"PoJxegPag-mI"}},{"cell_type":"code","source":["xbow = torch.zeros((B,T,C))\n","for b in range(B):\n","    for t in range(T):\n","        xprev = x[b,:t+1] # (t,C)\n","        xbow[b,t] = torch.mean(xprev, 0)\n","xbow.shape"],"metadata":{"id":"e72ttnUGhCX5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733658752315,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"73aeff2a-c727-43ba-88e1-46ac65b76368"},"execution_count":150,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":150}]},{"cell_type":"markdown","source":["---\n","\n","Weighted averages using matrix multiplication\n","\n","---"],"metadata":{"id":"Bb82KWsgjJEE"}},{"cell_type":"code","source":["wei = torch.tril(torch.ones(T, T))\n","wei = wei / wei.sum(1, keepdim=True)\n","xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n","xbow2.shape"],"metadata":{"id":"bZIjk5UzjMUl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733658754934,"user_tz":-330,"elapsed":536,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"91b63d11-0e73-41ba-eddd-ebe39291ff61"},"execution_count":151,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":151}]},{"cell_type":"markdown","source":["---\n","\n","Weighted averages using softmax\n","\n","---"],"metadata":{"id":"Wp23cb0TjoNK"}},{"cell_type":"code","source":["tril = torch.tril(torch.ones(T, T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = torch.nn.functional.softmax(wei, dim=-1)\n","xbow3 = wei @ x\n","xbow3.shape"],"metadata":{"id":"2Mx32Yryjrcj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733658756488,"user_tz":-330,"elapsed":1026,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"d15f096a-1faf-430b-8adf-cd34a090cb58"},"execution_count":152,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":152}]},{"cell_type":"markdown","source":["---\n","\n","The self attention block\n","\n","---"],"metadata":{"id":"BQjNYGK-kHxL"}},{"cell_type":"code","source":["torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = torch.nn.Linear(C, head_size, bias=False)\n","query = torch.nn.Linear(C, head_size, bias=False)\n","value = torch.nn.Linear(C, head_size, bias=False)\n","k = key(x)   # (B, T, 16)\n","q = query(x) # (B, T, 16)\n","wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T, T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = torch.nn.functional.softmax(wei, dim=-1)\n","\n","v = value(x)\n","out = wei @ v\n","out = wei @ x\n","\n","out.shape"],"metadata":{"id":"wnsj4xvgkJz_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733658759202,"user_tz":-330,"elapsed":498,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"b158558a-bdfe-414a-feff-d777352aba1d"},"execution_count":153,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 32])"]},"metadata":{},"execution_count":153}]}]}