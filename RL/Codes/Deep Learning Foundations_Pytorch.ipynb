{"cells":[{"cell_type":"markdown","metadata":{"id":"M7g7bxFCHxGP"},"source":["$${\\color{yellow}{\\text{Deep Learning for Foundations Using PyTorch}}}$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0_BbyTKXQflD"},"source":["---\n","\n","Load essential libraries\n","\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"20W0d4ruQjE4","executionInfo":{"status":"ok","timestamp":1735969231285,"user_tz":-330,"elapsed":12364,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import matplotlib.pyplot as plt\n","plt.style.use('dark_background')\n","%matplotlib inline\n","import sys\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n","from sklearn.metrics import confusion_matrix\n","import gensim.downloader\n","import nltk\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"sfYXkqmLiVLM"},"source":["---\n","\n","Mount Google Drive folder if running Google Colab\n","\n","---"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VYzBBBxqiaGa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735969360416,"user_tz":-330,"elapsed":89940,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"a19d631b-cd15-4a2c-8a6c-418935c791d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## Mount Google drive folder if running in Colab\n","if('google.colab' in sys.modules):\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount = True)\n","    DIR = '/content/drive/MyDrive/Colab Notebooks/EvenSemester2024/RL/Codes'\n","    DATA_DIR = DIR+'/Data/'\n","else:\n","    DATA_DIR = 'Data/'"]},{"cell_type":"markdown","metadata":{"id":"avVZ6D1ZgEUT"},"source":["---\n","\n","**We will now use Pytorch to create tensors**\n","\n","The patient data matrix:\n","\n","![patient data matrix](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=1000)\n","\n","**Notation**:\n","\n","Zeroth patient vector $\\mathbf{x}^{(0)}= \\begin{bmatrix}72\\\\120\\\\37.3\\\\104\\\\32.5\\end{bmatrix}$ and zeroth feature (heart rate vector) $\\mathbf{x}_0 = \\begin{bmatrix}72\\\\85\\\\68\\\\90\\\\84\\\\78\\end{bmatrix}.$\n","\n","---\n","\n"]},{"cell_type":"code","source":["X = torch.Tensor(6,5)\n","print(X.dtype)   #flot32 is the default datatype in pytorch\n","torch.get_default_dtype()\n","\n","#Y = torch.tensor(6,5)  Gives error cuz .torch is a method and not a class\n","Y = torch.tensor([6.0,5])\n","print(Y.shape)\n","print(Y.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fH9KCFNJ-Z0B","executionInfo":{"status":"ok","timestamp":1735972061823,"user_tz":-330,"elapsed":539,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"e2a7163a-5b1e-4e3b-d9db-eae45caa87f2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.float32\n","torch.Size([2])\n","torch.float32\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zrPnepAEvr0O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735971426135,"user_tz":-330,"elapsed":797,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"45d9d27e-9f8b-40eb-98fe-3d1121cfe32c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n","        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n","        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n","        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n","        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n","        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]])\n","torch.Size([6, 5])\n","tensor([ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000])\n","tensor(37.3000)\n"]}],"source":["## Create a patient data matrix as a constant tensor\n","X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n","                 [85, 130, 37.0, 110, 14],\n","                 [68, 110, 38.5, 125, 34],\n","                 [90, 140, 38.0, 130, 26],\n","                 [84, 132, 38.3, 146, 30],\n","                 [78, 128, 37.2, 102, 12]])\n","print(X)\n","print(X.shape)\n","# X is a rank-2 tensor which is similar to a numpy 2D array\n","print(X[0]) # this is patient-0 info which is a rank-1 tensor\n","print(X[0, 2])"]},{"cell_type":"markdown","metadata":{"id":"cevtn_b4gek5"},"source":["---\n","\n","**Convert a PyTorch object into a numpy array**\n","\n","---"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JrYQ2moygfPu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735971426686,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"f11c618d-3119-442e-8967-a99ad88d090c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 72.  120.   37.3 104.   32.5]\n"," [ 85.  130.   37.  110.   14. ]\n"," [ 68.  110.   38.5 125.   34. ]\n"," [ 90.  140.   38.  130.   26. ]\n"," [ 84.  132.   38.3 146.   30. ]\n"," [ 78.  128.   37.2 102.   12. ]]\n","<class 'numpy.ndarray'>\n","(6, 5)\n"]}],"source":["X_numpy = X.numpy()\n","print(X_numpy)\n","print(type(X_numpy))\n","print(X_numpy.shape)"]},{"cell_type":"markdown","metadata":{"id":"QS3MmzwsgkWU"},"source":["---\n","\n","**Addition and subtraction of vectors, scalar multiplication (apply operation componentwise)**\n","\n","![vector addition](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NokBAAAAAZLAaAoWwhtn8Vk26NotALo?width=256)\n","\n","![vector subtracton](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3M4kBAAAAAU_n_mAEv006QFZm_sUj2Dc?width=256)\n","\n","![vector multiplication](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NIkBAAAAAa_qL04bLT4kWoNeHcrR9LQ?width=256)\n","\n","![vector geometry1](https://1drv.ms/i/c/37720f927b6ddc34/IQSGNMr5z3SSRry7LSKL7LybAcGYuzgw5smabV8-6DudXIs?width=230)\n","\n","![vector geometry2](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=192)\n","\n","\n","---"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TgPtJP0sglQP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735971447121,"user_tz":-330,"elapsed":433,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"9e4a30a3-41a5-4c52-f4ba-778e72b8639c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([153.0000, 240.0000,  75.5000, 235.0000,  48.0000])\n","tensor([ 17.0000,  20.0000,  -1.5000, -15.0000, -20.0000])\n","tensor([37.3000, 37.0000, 38.5000, 38.0000, 38.3000, 37.2000])\n","tensor([ 99.1400,  98.6000, 101.3000, 100.4000, 100.9400,  98.9600])\n","tensor([ 79.5000, 126.6667,  37.7167, 119.5000,  24.7500])\n","tensor([ 79.5000, 126.6667,  37.7167, 119.5000,  24.7500])\n"]}],"source":["# Vector addition\n","print(X[1, :] + X[2, :])\n","\n","# Vector subtraction\n","print(X[1, :] - X[2, :]) # how different patient-1 and patient-2 are\n","\n","# Scalar-vector multiplication\n","print(X[:, 2])\n","print((9/5)*X[:, 2] + 32)\n","\n","# Average patient\n","print((1/6)*(X[0, :] + X[1, :] + X[2, :] + X[3, :] + X[4, :] + X[5, :]))\n","print(torch.mean(X, dim = 0)) # dim = 0 means top-to-bottom operation or each row is an element"]},{"cell_type":"markdown","metadata":{"id":"1t_qXrlCROKA"},"source":["---\n","\n","Application of vector subtraction in natural language processing (NLP): download the word embedding model trained on Wikipedia articles.\n","\n","---"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_e13FnW0RUwy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735971547468,"user_tz":-330,"elapsed":50870,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"7f8e5631-6ff1-49d4-f7a2-80d2f399ab28"},"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n"]}],"source":["model = gensim.downloader.load('glove-wiki-gigaword-50')"]},{"cell_type":"markdown","metadata":{"id":"7YRVJferRlK5"},"source":["---\n","\n","Now we will see what embedding vector comes as a result of applying the model for the words *cricket* and *football*.\n","\n","Next, we will do an *intuitive* subtraction of word embeddings as in\n","\n","1. Cricket without Tendulkar\n","2. Football without Messi\n","\n","Note that the embedding vectors have 50 components corresponding to the 50-dimensional embedding of model suggested by the name '**glove-wiki-gigaword-50**'\n","\n","---"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"HVVFzeQyR3Wb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972168726,"user_tz":-330,"elapsed":574,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"d307d31a-6769-40a2-96ca-d2d0f33f6b55"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.2233    -0.28292   -1.7429     0.56621   -0.13787   -0.88088\n"," -0.26887    0.41893    0.94977   -0.61332    0.0092522  1.0045\n"," -0.89004   -0.55148    0.61202    0.42256    0.92935    0.83307\n"," -1.5568     0.084345  -0.018224   0.84803    0.68321    0.97059\n","  0.26561   -1.0538     0.40724   -0.45079   -0.89013    0.94153\n","  2.2055     0.75363    0.5166     0.47977    0.86824    0.57228\n","  0.81846   -0.070934  -0.9391    -0.81628   -0.35486   -0.010408\n"," -0.83316    1.1001    -0.087408   1.8452    -0.83112    0.43732\n","  0.63007   -0.81023  ]\n","[-1.8209    0.70094  -1.1403    0.34363  -0.42266  -0.92479  -1.3942\n","  0.28512  -0.78416  -0.52579   0.89627   0.35899  -0.80087  -0.34636\n","  1.0854   -0.087046  0.63411   1.1429   -1.6264    0.41326  -1.1283\n"," -0.16645   0.17424   0.99585  -0.81838  -1.7724    0.078281  0.13382\n"," -0.59779  -0.45068   2.5474    1.0693   -0.27017  -0.75646   0.24757\n","  1.0261    0.11329   0.17668  -0.23257  -1.1561   -0.10665  -0.25377\n"," -0.65102   0.32393  -0.58262   0.88137  -0.13465   0.96903  -0.076259\n"," -0.59909 ]\n","[-0.7716      0.41267997 -1.725968   -0.10445005 -1.1475699  -0.854661\n"," -1.089      -0.08342999  0.62349    -1.67822    -0.2488078  -0.49199998\n","  0.18756002 -1.67098     0.6117872   0.42784432  1.05656     0.91583097\n"," -0.03299999 -0.04422501  0.200326   -0.33737004  0.31068     1.37842\n"," -1.13689    -0.57445    -0.70685995  0.41552    -0.28937     0.54485\n","  1.0492998   0.62732    -0.8105     -1.27723    -0.02612001  0.53963\n"," -0.14065999 -0.738244   -0.30487    -1.18129     0.05651999 -0.993618\n"," -0.911399   -0.09289992  0.535432    0.26259995 -0.63031     0.64473\n","  0.77843     0.15099996]\n","[-2.06898     0.66804904 -1.077512    0.79964995 -0.27109998 -0.26289004\n"," -0.881       0.377503   -0.10869002 -2.47329    -0.23453003 -0.58438\n","  0.10404003 -0.52671003 -0.03030002  0.237764    0.19168997  1.60344\n"," -0.42980003  0.59058     0.59800005 -0.67075     0.45888     1.4538\n"," -1.15642    -1.63534    -1.1248189  -0.20879    -0.00812     0.25545004\n","  1.92044     0.30049008  0.19949001 -0.675167   -0.15230002  0.13278002\n"," -0.29492003 -0.55414    -0.30988902 -0.34549004 -0.72603    -1.20504\n"," -0.45038998  0.51834     0.12448996  0.787596   -1.13398     0.91365004\n"," -0.280479    0.76741004]\n"]}],"source":["print(model['cricket'])\n","print(model['football'])\n","a = model['cricket'] - model['tendulkar']\n","b = model['football'] - model['messi']\n","print(a)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"8VPICS8ggvvg"},"source":["---\n","\n","A tensor of rank 3 corresponding to 4 time stamps (hourly), 3 samples (patients), 2 features (HR and BP)\n","\n","---"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"qn6KT_pBgwUe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972365774,"user_tz":-330,"elapsed":436,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"248c2c76-7ddf-4992-9864-9a1b8d4b2e23"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 74., 128.],\n","         [ 79., 116.],\n","         [ 71., 116.]],\n","\n","        [[ 78., 118.],\n","         [ 82., 124.],\n","         [ 72., 128.]],\n","\n","        [[ 84., 138.],\n","         [ 84., 130.],\n","         [ 74., 120.]],\n","\n","        [[ 82., 126.],\n","         [ 76., 156.],\n","         [ 82., 132.]]])\n","torch.Size([4, 3, 2])\n"]}],"source":["# A rank-3 patient tensor with shape (4, 3, 2)\n","# with meaning for\n","# dimension-0 as 4 hourly timestamps,\n","# dimension-1 as 3 patients, and\n","# dimension-2 as 2 features (HR and BP)\n","T = torch.tensor([[[74., 128], [79, 116], [71, 116]],\n","                 [[78, 118], [82, 124], [72, 128]],\n","                 [[84, 138], [84, 130], [74, 120]],\n","                 [[82, 126], [76, 156], [82, 132]]])\n","print(T)\n","print(T.shape)"]},{"cell_type":"markdown","metadata":{"id":"JV0fpSojg2EZ"},"source":["---\n","\n","**Accessing elements of a tensor**\n","\n","---"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"1GbZuDYqg22n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972368538,"user_tz":-330,"elapsed":430,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"5d2610de-e736-4a4e-8efd-91a3cd8d3b7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(132.)\n","tensor([[ 74., 128.],\n","        [ 79., 116.],\n","        [ 71., 116.]])\n","tensor([ 82., 132.])\n"]}],"source":["## Accessing elements of a tensor\n","# Rank-3 tensor T has axes order (timestamps, patients, features)\n","\n","# Element of T at postion 3 w.r.t. axis-0, position 2 w.r.t. axis-1,\n","# position-1 w.r.t axis-2\n","print(T[3, 2, 1]) # 3rd timestamp, 2nd patient, 1st feature (BP)\n","\n","print(T[0]) # element-0 of object T which is also the info for all patients at admission time 9AM\n","\n","print(T[3, 2]) # patient-2 info at 12PM"]},{"cell_type":"markdown","metadata":{"id":"0o6kEXfCpDzo"},"source":["---\n","\n","**Exercise**: interpret $\\texttt{T[:, -1, :]}$\n","\n","---"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"X6lEPZEWo6wo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972575050,"user_tz":-330,"elapsed":481,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"b43b7443-fea8-41ce-bdcd-534ef36242af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 71., 116.],\n","        [ 72., 128.],\n","        [ 74., 120.],\n","        [ 82., 132.]])"]},"metadata":{},"execution_count":21}],"source":["T[:, -1, :] # information at the last patient at all timestamps"]},{"cell_type":"markdown","metadata":{"id":"gc9EJuZQhD9i"},"source":["---\n","\n","$l_2$ norm or the geometric length of a vector denoted as $\\lVert \\mathbf{a}\\rVert$ tells us how long a vector is. In 2-dimensions, $$\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2}$$ and in $n$-dimensions, $$\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}.$$\n","\n","![vector norm](https://1drv.ms/i/c/37720f927b6ddc34/IQT817WmpQjlRqZ1R0d5Cfv6AUW6c4robL-gk06i9wmCaFU?width=500)\n","\n","---"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"OM65UP4_hEso","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972609938,"user_tz":-330,"elapsed":442,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"83d9232c-60dd-4e60-ef97-3a872d1036ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 76., 124.])\n","tensor(145.4373)\n"]}],"source":["## l2 norm of a vector\n","x = torch.tensor([76., 124])\n","print(x)\n","print(torch.norm(x)) # sqrt(76^2+124^2)"]},{"cell_type":"markdown","metadata":{"id":"SRbanrUmwLX7"},"source":["\n","---\n","\n","**Dot Product of Vectors**\n","\n","A scalar resulting from an elementwise multiplication and addition: $$\\mathbf{a}{\\color{cyan}\\cdot}\\mathbf{b} = {\\color{red}{a_1b_1}}+{\\color{green}{a_2b_2}}+\\cdots+{\\color{magenta}{a_nb_n}}$$\n","\n","The <font color=\"cyan\">dot</font> ${\\color{cyan}\\cdot}$ represents the computation of the dot product.\n","\n","\n","---"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"s91XY1JZwU2w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972612174,"user_tz":-330,"elapsed":414,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"c9f94c60-f178-4926-93c5-507c6b3db474"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(32.)\n"]}],"source":["## Dot product of vectors\n","a = torch.tensor([1., 2, 3])\n","b = torch.tensor([4., 5, 6])\n","print(torch.dot(a, b)) # elementwise product followed by a summation"]},{"cell_type":"markdown","metadata":{"id":"2-b90m-QXyFp"},"source":["---\n","\n","The dot product is a measure of similarity between vectors (or, how aligned they are geometrically).\n","\n","![dot product](https://1drv.ms/i/c/37720f927b6ddc34/IQTbcGSjdbhSTJ7J39d5BCWAAWS6-y5U6J87vHuDWeAqGwM?width=6000)\n","---"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"3GxZ95uXXz3P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972615284,"user_tz":-330,"elapsed":400,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"395ace20-2b40-4a38-b66d-8b00838fe982"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.)\n","tensor(0.)\n","tensor(-5.)\n"]}],"source":["a = torch.tensor([1.0, 2.0])\n","b = torch.tensor([2.0, 4.0])  # b is exactly aligned with a\n","c = torch.tensor([-2.0, 1.0]) # c is perpendicular or orthogonal to a\n","d = torch.tensor([-1.0, -2.0])  # d is anti-aligned with a\n","print(torch.dot(a, b))\n","print(torch.dot(a, c))\n","print(torch.dot(a, d))"]},{"cell_type":"markdown","metadata":{"id":"U6CS4_8byCs8"},"source":["---\n","\n","Cauchy-Schwarz inequality $-1\\leq\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\leq1.$\n","\n","This is a normalized measure of similarity (or extent of alignment) between vectors.\n","\n","Angle between vectors $\\mathbf{x}$ and $\\mathbf{y} = \\cos^{-1}\\left(\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\right).$\n","\n","![angle](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=400)\n","\n","\n","---"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"q4UhBnPUx7TV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972621297,"user_tz":-330,"elapsed":489,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"c74688fa-42e9-4561-e93a-1b0d4f484c21"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.8000)\n","tensor(0.6435)\n","tensor(36.8699)\n"]}],"source":["x = torch.tensor([1.0, 2.0])\n","y = torch.tensor([2.0, 1.0])\n","print(torch.dot(x, y) / (torch.norm(x) * torch.norm(y))) # normalized similarity measure\n","print(torch.acos(torch.dot(x, y) / (torch.norm(x) * torch.norm(y)))) # angle in radians\n","print((180/torch.pi)*torch.acos(torch.dot(x, y) / (torch.norm(x) * torch.norm(y)))) # angle in degrees"]},{"cell_type":"markdown","metadata":{"id":"1bnmEkg3Tctx"},"source":["---\n","\n","Application of the Cauchy-Schwarz inequality: is \"Cricket without Tendulkar\" same as \"Football without Messi\"?\n","\n","---"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"KrmCknO5TkNZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972768331,"user_tz":-330,"elapsed":1708,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"3c35563e-fb2d-48ba-d645-8699798fd845"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.7371284\n","42.51263077162803\n","4.2349043\n"]}],"source":["a = model['cricket'] - model['tendulkar']\n","b = model['football'] - model['messi']\n","print(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))) # normalized similarity\n","print((180/np.pi)*np.arccos(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)))) # angular difference in degrees\n","print(np.linalg.norm(a-b)) # linear difference"]},{"cell_type":"markdown","metadata":{"id":"ayzM_0_synRF"},"source":["\n","---\n","\n","**Hadamard Product of Vectors**\n","\n","A vector resulting from an elementwise multiplication: $$\\mathbf{a}{\\color{cyan}\\otimes}\\mathbf{b} = \\begin{bmatrix}{\\color{red}{a_1\\times b_1}}\\\\{\\color{green}{a_2\\times b_2}}\\\\\\vdots\\\\{\\color{magenta}{a_n\\times b_n}}\\end{bmatrix}.$$\n","\n","The <font color=\"cyan\">$\\otimes$</font> represents the computation of the Hadamard product.\n","\n","---"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"UPojS0rIzR8p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972768331,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"e356e537-a7d7-4c50-8d44-39b75aabaa91"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 4., 10., 18.])\n","tensor([ 4., 10., 18.])\n"]}],"source":["## Hadamard product\n","a = torch.tensor([1.0, 2.0, 3.0])\n","b = torch.tensor([4.0, 5.0, 6.0])\n","\n","# Element-wise multiplication (Hadamard product)\n","print(a * b)  # Using the * operator\n","print(torch.mul(a, b))  # Using torch.mul function"]},{"cell_type":"markdown","metadata":{"id":"oruyV_EjhqCR"},"source":["---\n","\n","A matrix-vector product is simply a sequence of dot products of the rows of matrix (seen as vectors) with the vector\n","\n","![matvec product](https://1drv.ms/i/c/37720f927b6ddc34/IQQ1cQ8fZdFmS4cnGkBlsZbAAaL2zMtzWdjHe-HCMt4UTA0?width=700)\n","\n","---"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"A_IScSWzhpi7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972791416,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"fd267c96-d1e7-4861-e771-89250e8ee7fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.,  2.,  4.],\n","        [ 2., -1.,  3.]])\n","tensor([ 4.,  2., -2.])\n","tensor([0., 0.])\n"]}],"source":["## Matrix-vector product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","x = torch.tensor([4.0, 2.0, -2.0])\n","\n","# Matrix-vector multiplication\n","print(A)\n","print(x)\n","print(torch.matmul(A, x))"]},{"cell_type":"markdown","metadata":{"id":"uTnGSJ3vT4EN"},"source":["---\n","\n","Here we create a simple sentence in English and tokenize it\n","\n","---"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"pQ73kkevT5L3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972834849,"user_tz":-330,"elapsed":510,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"bc6b5f94-d257-4bee-cafb-7839e7e26210"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["12\n","['i', 'swam', 'quickly', 'across', 'the', 'river', 'to', 'get', 'to', 'the', 'other', 'bank']\n"]}],"source":["sentence = 'i swam quickly across the river to get to the other bank'\n","nltk.download('punkt_tab')\n","tokens = word_tokenize(sentence)\n","print(len(tokens))\n","print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"M40pqI8UUbX4"},"source":["---\n","\n","Generate the word embeddings for the tokens and store them in a matrix $\\mathbf{X}$ such that each row of the matrix corresponds to a token.\n","\n","---"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"1mKKVRyxUh5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972838649,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"772dd81e-1298-426f-94e5-61240a380622"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n","         -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04,\n","         -1.5043e-01,  8.3770e-01, -1.0797e+00, -5.1460e-01,  1.3188e+00,\n","          6.2007e-01,  1.3779e-01,  4.7108e-01, -7.2874e-02, -7.2675e-01,\n","         -7.4116e-01,  7.5263e-01,  8.8180e-01,  2.9561e-01,  1.3548e+00,\n","         -2.5701e+00, -1.3523e+00,  4.5880e-01,  1.0068e+00, -1.1856e+00,\n","          3.4737e+00,  7.7898e-01, -7.2929e-01,  2.5102e-01, -2.6156e-01,\n","         -3.4684e-01,  5.5841e-01,  7.5098e-01,  4.9830e-01, -2.6823e-01,\n","         -2.7443e-03, -1.8298e-02, -2.8096e-01,  5.5318e-01,  3.7706e-02,\n","          1.8555e-01, -1.5025e-01, -5.7512e-01, -2.6671e-01,  9.2121e-01],\n","        [-3.5303e-01,  3.6953e-01,  4.7266e-01,  1.3832e-02, -1.6484e-01,\n","         -5.2687e-01, -7.3986e-01,  1.2058e+00,  1.1147e+00, -4.6772e-01,\n","          1.7928e-01, -5.9239e-01,  2.5257e-01,  4.3449e-01,  6.7023e-01,\n","         -2.8594e-01,  7.3105e-01,  3.1828e-01, -1.5825e+00, -3.5711e-01,\n","         -1.2347e-01,  1.3207e+00,  9.0049e-01,  1.2197e-01,  1.4916e+00,\n","         -5.4105e-01,  1.2264e+00,  3.3936e-01, -3.3048e-01, -3.5457e-01,\n","         -9.6055e-02,  5.0327e-02,  1.2475e-01, -2.2816e-02, -3.5395e-01,\n","         -4.3186e-01,  3.7120e-02,  7.1439e-01, -1.5775e-02, -4.6025e-01,\n","         -3.8091e-01, -6.5412e-01,  1.2373e+00,  7.7759e-01,  6.3607e-02,\n","         -1.4376e+00,  2.4166e-01, -1.5705e+00, -1.0645e-01, -8.2711e-01],\n","        [ 5.6036e-01, -1.1257e+00,  5.5507e-01, -7.1007e-01,  5.9383e-02,\n","         -2.9997e-01, -7.8756e-01,  7.0269e-01, -2.8640e-01, -1.5299e-01,\n","          3.6951e-01,  2.8308e-01, -6.1024e-01,  5.8911e-03,  2.2412e-01,\n","          6.4999e-01, -2.3358e-01, -4.7678e-01, -1.2418e-01, -7.5310e-01,\n","          2.6407e-01, -4.9331e-02,  5.9658e-01, -2.3219e-01,  5.5754e-01,\n","         -1.5649e+00,  2.1960e-01,  2.6784e-01,  7.2161e-01, -1.3073e-01,\n","          3.0699e+00,  2.1293e-01, -1.4069e-01, -8.7782e-01, -3.6846e-01,\n","          1.1815e-01, -2.7351e-01,  5.1589e-01, -4.3990e-06, -5.4707e-01,\n","         -2.6419e-01, -2.3358e-01, -2.7178e-01, -1.6913e-01,  7.4022e-02,\n","          1.0568e-01,  8.5594e-02, -5.5750e-01, -2.7034e-01, -2.5920e-01],\n","        [ 5.2360e-01, -2.0293e-02,  2.6881e-01, -4.8425e-01, -5.4396e-01,\n","         -4.6181e-01, -9.0864e-01, -3.2993e-01,  6.2731e-01, -6.8066e-01,\n","         -5.4416e-01, -1.0720e+00,  3.9323e-02,  2.9368e-02, -4.9019e-01,\n","         -5.9847e-02,  2.3170e-01, -1.7236e-01, -6.2349e-01, -6.9779e-01,\n","          4.8163e-01,  2.1039e-01,  3.0509e-01,  5.0297e-01,  1.3997e-01,\n","         -1.2732e+00,  8.5410e-02,  7.0401e-01,  2.0331e-01, -6.5306e-01,\n","          3.6790e+00,  5.5571e-01,  5.1758e-01, -4.6839e-01, -6.0765e-01,\n","          8.2281e-02, -9.4349e-01, -3.8319e-01, -3.8270e-01,  7.0752e-01,\n","         -5.6429e-01,  4.8173e-01,  3.8864e-01,  3.8322e-02, -2.1097e-01,\n","          1.4094e-01,  1.4637e-01, -9.1432e-01, -6.1570e-01, -1.3112e+00],\n","        [ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n","         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n","          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n","          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n","         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n","         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n","          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n","          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n","         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n","          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n","        [ 7.3109e-01,  1.0242e+00, -2.6714e-01, -3.3449e-01, -1.4873e+00,\n","         -3.7840e-01, -1.2873e+00,  7.1923e-01,  1.1322e+00, -9.0059e-01,\n","         -1.2594e-01,  7.2664e-01,  7.2766e-01, -1.0212e+00, -1.6100e+00,\n","         -2.3452e-01,  1.2707e+00, -6.4512e-02, -6.8331e-01,  2.6957e-01,\n","         -2.1492e-01, -1.0354e+00,  7.7650e-01,  1.5299e-04,  6.2976e-01,\n","         -1.3818e+00, -1.9453e-01,  1.3069e+00, -3.0326e-01, -1.4769e+00,\n","          2.9696e+00, -5.6795e-01,  3.3913e-01,  3.8007e-01,  1.8625e-03,\n","         -2.8222e-01, -8.7329e-01, -7.9432e-01,  1.3823e+00,  7.9638e-01,\n","         -7.1245e-01, -2.8103e-01,  5.6754e-02, -9.7843e-01, -1.1465e+00,\n","         -6.2487e-01,  2.6281e-01, -1.4434e+00,  4.0201e-01, -1.3707e+00],\n","        [ 6.8047e-01, -3.9263e-02,  3.0186e-01, -1.7792e-01,  4.2962e-01,\n","          3.2246e-02, -4.1376e-01,  1.3228e-01, -2.9847e-01, -8.5253e-02,\n","          1.7118e-01,  2.2419e-01, -1.0046e-01, -4.3653e-01,  3.3418e-01,\n","          6.7846e-01,  5.7204e-02, -3.4448e-01, -4.2785e-01, -4.3275e-01,\n","          5.5963e-01,  1.0032e-01,  1.8677e-01, -2.6854e-01,  3.7334e-02,\n","         -2.0932e+00,  2.2171e-01, -3.9868e-01,  2.0912e-01, -5.5725e-01,\n","          3.8826e+00,  4.7466e-01, -9.5658e-01, -3.7788e-01,  2.0869e-01,\n","         -3.2752e-01,  1.2751e-01,  8.8359e-02,  1.6351e-01, -2.1634e-01,\n","         -9.4375e-02,  1.8324e-02,  2.1048e-01, -3.0880e-02, -1.9722e-01,\n","          8.2279e-02, -9.4340e-02, -7.3297e-02, -6.4699e-02, -2.6044e-01],\n","        [ 1.5910e-01, -2.1428e-01,  6.3099e-01, -5.9950e-01,  3.1248e-01,\n","         -1.6615e-01, -9.0548e-01,  4.5115e-01,  5.1568e-02,  2.5910e-01,\n","         -3.2882e-01,  4.8155e-01, -3.4982e-01,  1.2905e-01,  1.0758e+00,\n","          4.8690e-01,  5.3420e-01,  5.9762e-02,  2.1660e-01, -1.1059e+00,\n","         -2.5591e-01,  5.7462e-01,  5.4562e-01,  3.1043e-01,  3.7765e-01,\n","         -2.0337e+00, -2.2496e-01,  1.8447e-01,  8.2587e-01, -1.1991e+00,\n","          3.6042e+00,  1.1605e+00, -5.9787e-01,  1.3000e-01,  1.5678e-01,\n","          1.3166e-01,  1.8510e-01,  3.6308e-01,  5.7538e-01, -8.9593e-01,\n","         -3.6366e-01,  2.8397e-01,  4.8614e-02,  7.8780e-01, -8.7311e-02,\n","         -2.3394e-01, -1.4237e-01,  2.1215e-02, -1.4219e-01,  6.6955e-01],\n","        [ 6.8047e-01, -3.9263e-02,  3.0186e-01, -1.7792e-01,  4.2962e-01,\n","          3.2246e-02, -4.1376e-01,  1.3228e-01, -2.9847e-01, -8.5253e-02,\n","          1.7118e-01,  2.2419e-01, -1.0046e-01, -4.3653e-01,  3.3418e-01,\n","          6.7846e-01,  5.7204e-02, -3.4448e-01, -4.2785e-01, -4.3275e-01,\n","          5.5963e-01,  1.0032e-01,  1.8677e-01, -2.6854e-01,  3.7334e-02,\n","         -2.0932e+00,  2.2171e-01, -3.9868e-01,  2.0912e-01, -5.5725e-01,\n","          3.8826e+00,  4.7466e-01, -9.5658e-01, -3.7788e-01,  2.0869e-01,\n","         -3.2752e-01,  1.2751e-01,  8.8359e-02,  1.6351e-01, -2.1634e-01,\n","         -9.4375e-02,  1.8324e-02,  2.1048e-01, -3.0880e-02, -1.9722e-01,\n","          8.2279e-02, -9.4340e-02, -7.3297e-02, -6.4699e-02, -2.6044e-01],\n","        [ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n","         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n","          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n","          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n","         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n","         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n","          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n","          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n","         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n","          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n","        [ 6.4756e-01,  1.6000e-01,  2.9191e-02,  3.5118e-01,  8.9119e-02,\n","          6.1115e-01, -6.6362e-01, -5.1724e-01, -4.6521e-01, -8.8450e-02,\n","          5.0200e-02,  2.6329e-01,  1.2407e-01,  4.3832e-02,  1.7283e-01,\n","          1.3170e-02,  1.4168e-01, -1.5827e-01, -1.0427e-01, -9.3070e-01,\n","          2.1646e-01, -1.0753e-01,  6.2087e-01,  3.6761e-01, -4.8144e-01,\n","         -1.2800e+00, -5.5152e-01, -7.2023e-01, -1.7097e-01, -4.7993e-01,\n","          4.0165e+00,  4.7054e-01,  9.3614e-02, -8.6341e-01,  5.0881e-01,\n","          3.3353e-01, -3.5962e-01, -1.6648e-01, -3.1803e-01,  4.9003e-01,\n","         -3.6697e-01,  3.2051e-01,  7.0932e-01,  6.2878e-01,  7.0128e-01,\n","          1.3020e-01, -7.3769e-01,  1.0325e-01, -3.0964e-01, -4.4213e-01],\n","        [ 6.6488e-01, -1.1391e-01,  6.7844e-01,  1.7951e-01,  6.8280e-01,\n","         -4.7787e-01, -3.0761e-01,  1.7489e-01, -7.0512e-01, -5.5022e-01,\n","          1.5140e-01,  1.0214e-01, -4.5063e-01, -3.3069e-01,  5.6133e-02,\n","          1.2271e+00,  5.5607e-01, -6.8297e-01,  3.7364e-02,  7.0266e-01,\n","          1.9093e+00, -6.1483e-01, -8.3329e-01, -3.0230e-01, -1.1118e+00,\n","         -1.5500e+00,  2.6040e-01,  2.2957e-01, -1.0375e+00, -3.1789e-01,\n","          3.5091e+00, -2.5871e-01,  1.0151e+00,  6.5927e-01, -1.8231e-01,\n","         -7.5859e-01, -3.0927e-01, -9.1678e-01,  1.0633e+00, -6.6761e-01,\n","         -3.7464e-01, -2.9143e-01,  6.5606e-01, -4.4642e-01, -7.5495e-02,\n","         -1.0552e+00, -6.0501e-01,  7.3582e-01,  1.0139e+00, -2.7749e-01]])\n","torch.Size([12, 50])\n","tensor([-0.3530,  0.3695,  0.4727,  0.0138, -0.1648, -0.5269, -0.7399,  1.2058,\n","         1.1147, -0.4677,  0.1793, -0.5924,  0.2526,  0.4345,  0.6702, -0.2859,\n","         0.7311,  0.3183, -1.5825, -0.3571, -0.1235,  1.3207,  0.9005,  0.1220,\n","         1.4916, -0.5411,  1.2264,  0.3394, -0.3305, -0.3546, -0.0961,  0.0503,\n","         0.1248, -0.0228, -0.3539, -0.4319,  0.0371,  0.7144, -0.0158, -0.4602,\n","        -0.3809, -0.6541,  1.2373,  0.7776,  0.0636, -1.4376,  0.2417, -1.5705,\n","        -0.1064, -0.8271])\n"]}],"source":["X_word = torch.tensor(model[tokens])\n","np.set_printoptions(precision=3, suppress=True)\n","print(X_word)\n","print(X_word.shape)\n","print(X_word[1]) # embedding vector for the word \"swam\""]},{"cell_type":"markdown","metadata":{"id":"0Z0pZQisxtY-"},"source":["---\n","\n","A matrix-matrix product is simply a sequence of matrix-vector products.\n","\n","![matmatprod](https://1drv.ms/i/c/37720f927b6ddc34/IQQ-B3z7tbWHQqBrW9k2ElDVAUc5fWzM24txLkgBK7f8Yac?width=550)\n","\n","\n","---"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"YSg1brJ9yKnM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972857136,"user_tz":-330,"elapsed":413,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"b0c947a3-40f7-4ab7-fc15-cdd96f28a234"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0., 11.],\n","        [ 0.,  7.]])"]},"metadata":{},"execution_count":31}],"source":["## Matrix-matrix product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","B = torch.tensor([[4.0, -1.0],\n","                  [2.0, 0.0],\n","                  [-2.0, 3.0]])\n","torch.matmul(A, B)"]},{"cell_type":"markdown","metadata":{"id":"mVoJRc6kUtI2"},"source":["---\n","\n","The similarity between each pair of words represented in the word embeddings matrix $\\mathbf{X}_\\mathrm{word}$ is the matrix-matrix product $\\mathbf{X}_\\mathrm{word}\\mathbf{X}_\\mathrm{word}^\\mathrm{T}.$\n","\n","---"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"ms9Qg5AoVJy_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972887456,"user_tz":-330,"elapsed":538,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"5ccd082c-6929-4814-f52b-c56da8e98aad"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[39.0197,  4.5574, 20.2995, 14.5481, 19.4420, 12.0143, 22.6571, 29.8528,\n","         22.6571, 19.4420, 17.8858, 11.2500],\n","        [ 4.5574, 24.7064,  4.2668,  7.2292,  0.9357,  8.8437,  2.3160,  5.4718,\n","          2.3160,  0.9357, -0.6112, -1.5239],\n","        [20.2995,  4.2668, 21.2841, 16.5796, 16.1261, 12.0684, 18.8431, 19.4523,\n","         18.8431, 16.1261, 15.2439, 12.9733],\n","        [14.5481,  7.2292, 16.5796, 28.8804, 19.3706, 22.9543, 17.9298, 16.6827,\n","         17.9298, 19.3706, 19.6938, 14.1935],\n","        [19.4420,  0.9357, 16.1261, 19.3706, 24.6793, 18.2549, 21.1141, 17.5502,\n","         21.1141, 24.6793, 19.8692, 18.0800],\n","        [12.0143,  8.8437, 12.0684, 22.9543, 18.2549, 44.2210, 14.7193, 12.0669,\n","         14.7193, 18.2549, 12.5996, 16.6162],\n","        [22.6571,  2.3160, 18.8431, 17.9298, 21.1141, 14.7193, 24.5706, 22.2478,\n","         24.5706, 21.1141, 20.6623, 19.2235],\n","        [29.8528,  5.4718, 19.4523, 16.6827, 17.5502, 12.0669, 22.2478, 30.1930,\n","         22.2478, 17.5502, 19.3905, 14.3078],\n","        [22.6571,  2.3160, 18.8431, 17.9298, 21.1141, 14.7193, 24.5706, 22.2478,\n","         24.5706, 21.1141, 20.6623, 19.2235],\n","        [19.4420,  0.9357, 16.1261, 19.3706, 24.6793, 18.2549, 21.1141, 17.5502,\n","         21.1141, 24.6793, 19.8692, 18.0800],\n","        [17.8858, -0.6112, 15.2439, 19.6938, 19.8692, 12.5996, 20.6623, 19.3905,\n","         20.6623, 19.8692, 26.9230, 15.7850],\n","        [11.2500, -1.5239, 12.9733, 14.1935, 18.0800, 16.6162, 19.2235, 14.3078,\n","         19.2235, 18.0800, 15.7850, 36.3920]])\n","torch.Size([12, 12])\n"]}],"source":["S = torch.matmul(X_word, X_word.T)\n","print(S)\n","print(S.shape)"]},{"cell_type":"markdown","metadata":{"id":"QH_sW6XW0MDT"},"source":["---\n","\n","Matrix-matrix product using q patient data matrix and a weights matrix:\n","\n","![Patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hscharGu916tjWNzZQ?embed=1&width=660)\n","\n","---"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"s0RFtdhxhkvZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972999045,"user_tz":-330,"elapsed":421,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"ab3da5a6-c68e-4869-d350-1725bde320b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 72.0000, 120.0000,  36.5000],\n","        [ 85.0000, 130.0000,  37.0000],\n","        [ 68.0000, 110.0000,  38.5000],\n","        [ 90.0000, 140.0000,  38.0000]])\n","tensor([[ 0.5000,  0.3000, -0.6000],\n","        [ 0.9000,  0.3000, -0.2500],\n","        [-1.5000,  0.4000,  0.1000]])\n","tensor([[ 89.2500,  72.2000, -69.5500],\n","        [104.0000,  79.3000, -79.8000],\n","        [ 75.2500,  68.8000, -64.4500],\n","        [114.0000,  84.2000, -85.2000]])\n"]}],"source":["# Patients data matrix\n","X = torch.tensor([[72, 120, 36.5],\n","                  [85, 130, 37.0],\n","                  [68, 110, 38.5],\n","                  [90, 140, 38.0]])\n","print(X)\n","\n","# Weights matrix\n","W = torch.tensor([[0.5, 0.3, -0.6],\n","                  [0.9, 0.3, -0.25],\n","                  [-1.5, 0.4, 0.1]])\n","print(W)\n","\n","# Raw scores matrix (matrix-matrix multiplication)\n","Z = torch.matmul(X, W) # PyTorch matmul() also does matrix-matrix multiplication\n","print(Z)\n","\n","# The raw scores are also referred to as the logits"]},{"cell_type":"markdown","metadata":{"id":"sOnZvS5Vjjrd"},"source":["---\n","\n","The softmax function\n","\n","![softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hscmdol7J2G4GDo5WQ?embed=1&width=660)\n","\n","---"]},{"cell_type":"code","source":["mysoftmax = torch.nn.Softmax(dim = 1)\n","type(mysoftmax)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"_BH8LevIFid4","executionInfo":{"status":"ok","timestamp":1735973577924,"user_tz":-330,"elapsed":395,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"90442613-2cf3-4c0b-c6ac-a91fbabc04e3"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.nn.modules.activation.Softmax"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.nn.modules.activation.Softmax</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py</a>Applies the Softmax function to an n-dimensional input Tensor.\n","\n","Rescales them so that the elements of the n-dimensional output Tensor\n","lie in the range [0,1] and sum to 1.\n","\n","Softmax is defined as:\n","\n",".. math::\n","    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n","\n","When the input Tensor is a sparse tensor then the unspecified\n","values are treated as ``-inf``.\n","\n","Shape:\n","    - Input: :math:`(*)` where `*` means, any number of additional\n","      dimensions\n","    - Output: :math:`(*)`, same shape as the input\n","\n","Returns:\n","    a Tensor of the same dimension and shape as the input with\n","    values in the range [0, 1]\n","\n","Args:\n","    dim (int): A dimension along which Softmax will be computed (so every slice\n","        along dim will sum to 1).\n","\n",".. note::\n","    This module doesn&#x27;t work directly with NLLLoss,\n","    which expects the Log to be computed between the Softmax and itself.\n","    Use `LogSoftmax` instead (it&#x27;s faster and has better numerical properties).\n","\n","Examples::\n","\n","    &gt;&gt;&gt; m = nn.Softmax(dim=1)\n","    &gt;&gt;&gt; input = torch.randn(2, 3)\n","    &gt;&gt;&gt; output = m(input)</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 1614);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["#mysoftmax = torch.nn.functional.softmax(dim=1)  Error cuz its a function and not a class\n","#type(mysoftmax)"],"metadata":{"id":"5Ac9ZR9MFrmG","executionInfo":{"status":"ok","timestamp":1735973673072,"user_tz":-330,"elapsed":423,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","execution_count":34,"metadata":{"id":"TDkPV0qXVVyp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972943207,"user_tz":-330,"elapsed":437,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"7088ef65-a3b6-45cc-ee9f-6d6a056278fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[9.9990e-01, 1.0794e-15, 7.4108e-09, 2.3555e-11, 3.1439e-09, 1.8692e-12,\n","         7.8300e-08, 1.0443e-04, 7.8300e-08, 3.1439e-09, 6.6318e-10, 8.7044e-13],\n","        [1.7758e-09, 1.0000e+00, 1.3280e-09, 2.5688e-08, 4.7482e-11, 1.2910e-07,\n","         1.8879e-10, 4.4313e-09, 1.8879e-10, 4.7482e-11, 1.0109e-11, 4.0579e-12],\n","        [2.1580e-01, 2.3505e-08, 5.7765e-01, 5.2304e-03, 3.3235e-03, 5.7455e-05,\n","         5.0300e-02, 9.2497e-02, 5.0300e-02, 3.3235e-03, 1.3755e-03, 1.4202e-04],\n","        [5.9469e-07, 3.9420e-10, 4.5349e-06, 9.9704e-01, 7.3902e-05, 2.6611e-03,\n","         1.7496e-05, 5.0274e-06, 1.7496e-05, 7.3902e-05, 1.0210e-04, 4.1714e-07],\n","        [2.5563e-03, 2.3467e-11, 9.2803e-05, 2.3801e-03, 4.8101e-01, 7.7993e-04,\n","         1.3608e-02, 3.8551e-04, 1.3608e-02, 4.8101e-01, 3.9187e-03, 6.5480e-04],\n","        [1.0299e-14, 4.3238e-16, 1.0872e-14, 5.8078e-10, 5.2853e-12, 1.0000e+00,\n","         1.5403e-13, 1.0855e-14, 1.5403e-13, 5.2853e-12, 1.8493e-14, 1.0266e-12],\n","        [6.3112e-02, 9.2493e-11, 1.3923e-03, 5.5856e-04, 1.3490e-02, 2.2532e-05,\n","         4.2770e-01, 4.1917e-02, 4.2770e-01, 1.3490e-02, 8.5862e-03, 2.0366e-03],\n","        [4.1558e-01, 1.0719e-11, 1.2640e-05, 7.9241e-07, 1.8868e-06, 7.8399e-09,\n","         2.0694e-04, 5.8397e-01, 2.0694e-04, 1.8868e-06, 1.1883e-05, 7.3716e-08],\n","        [6.3112e-02, 9.2493e-11, 1.3923e-03, 5.5856e-04, 1.3490e-02, 2.2532e-05,\n","         4.2770e-01, 4.1917e-02, 4.2770e-01, 1.3490e-02, 8.5862e-03, 2.0366e-03],\n","        [2.5563e-03, 2.3467e-11, 9.2803e-05, 2.3801e-03, 4.8101e-01, 7.7993e-04,\n","         1.3608e-02, 3.8551e-04, 1.3608e-02, 4.8101e-01, 3.9187e-03, 6.5480e-04],\n","        [1.1808e-04, 1.0940e-12, 8.4105e-06, 7.2006e-04, 8.5813e-04, 5.9757e-07,\n","         1.8967e-03, 5.3167e-04, 1.8967e-03, 8.5813e-04, 9.9310e-01, 1.4448e-05],\n","        [1.2049e-11, 3.4143e-17, 6.7511e-11, 2.2871e-10, 1.1148e-08, 2.5790e-09,\n","         3.4977e-08, 2.5642e-10, 3.4977e-08, 1.1148e-08, 1.1233e-09, 1.0000e+00]])\n"]}],"source":["## In-built softmax function in PyTorch (dim = 1 corresponds to applying row-by-row)\n","## applied to the word embeddings similarity matrix\n","S_softmax = torch.nn.functional.softmax(S, dim = 1)\n","print(S_softmax)"]},{"cell_type":"markdown","metadata":{"id":"_OVqwridV5T_"},"source":["---\n","\n","Transform the word embeddings using the softmax-normalized similarity matrix.\n","\n","---"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"SF731yR3V3E7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735973707279,"user_tz":-330,"elapsed":437,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"7aef26fe-a16b-403f-a762-ed02c6747ada"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.1891e-01,  1.5251e-01, -8.1998e-02, -7.4142e-01,  7.5912e-01,\n","         -4.8325e-01, -3.1015e-01,  5.1475e-01, -9.8697e-01,  6.4455e-04,\n","         -1.5045e-01,  8.3766e-01, -1.0796e+00, -5.1453e-01,  1.3188e+00,\n","          6.2006e-01,  1.3783e-01,  4.7104e-01, -7.2844e-02, -7.2679e-01,\n","         -7.4111e-01,  7.5261e-01,  8.8176e-01,  2.9561e-01,  1.3547e+00,\n","         -2.5700e+00, -1.3522e+00,  4.5877e-01,  1.0068e+00, -1.1856e+00,\n","          3.4737e+00,  7.7902e-01, -7.2928e-01,  2.5101e-01, -2.6152e-01,\n","         -3.4679e-01,  5.5837e-01,  7.5094e-01,  4.9831e-01, -2.6830e-01,\n","         -2.7820e-03, -1.8266e-02, -2.8093e-01,  5.5320e-01,  3.7693e-02,\n","          1.8551e-01, -1.5025e-01, -5.7506e-01, -2.6670e-01,  9.2118e-01],\n","        [-3.5303e-01,  3.6953e-01,  4.7266e-01,  1.3832e-02, -1.6484e-01,\n","         -5.2687e-01, -7.3986e-01,  1.2058e+00,  1.1147e+00, -4.6772e-01,\n","          1.7928e-01, -5.9239e-01,  2.5257e-01,  4.3449e-01,  6.7023e-01,\n","         -2.8594e-01,  7.3105e-01,  3.1828e-01, -1.5825e+00, -3.5711e-01,\n","         -1.2347e-01,  1.3207e+00,  9.0049e-01,  1.2197e-01,  1.4916e+00,\n","         -5.4105e-01,  1.2264e+00,  3.3936e-01, -3.3048e-01, -3.5457e-01,\n","         -9.6054e-02,  5.0327e-02,  1.2475e-01, -2.2816e-02, -3.5395e-01,\n","         -4.3186e-01,  3.7120e-02,  7.1439e-01, -1.5775e-02, -4.6025e-01,\n","         -3.8091e-01, -6.5412e-01,  1.2373e+00,  7.7759e-01,  6.3607e-02,\n","         -1.4376e+00,  2.4166e-01, -1.5705e+00, -1.0645e-01, -8.2711e-01],\n","        [ 4.3907e-01, -6.3930e-01,  3.9044e-01, -6.4476e-01,  2.6984e-01,\n","         -2.9165e-01, -6.5632e-01,  5.6847e-01, -4.0110e-01, -8.1029e-02,\n","          1.6688e-01,  4.0522e-01, -6.3131e-01, -1.3854e-01,  5.4471e-01,\n","          6.2251e-01, -4.7790e-02, -2.0495e-01, -1.1952e-01, -7.4331e-01,\n","          2.8192e-02,  1.9576e-01,  6.0551e-01, -6.7079e-02,  6.5179e-01,\n","         -1.8785e+00, -1.6886e-01,  2.3414e-01,  7.2940e-01, -5.0384e-01,\n","          3.2990e+00,  4.4844e-01, -3.9066e-01, -4.8452e-01, -2.3629e-01,\n","         -2.6554e-02, -1.1883e-02,  4.9907e-01,  1.7507e-01, -4.7457e-01,\n","         -2.0187e-01, -1.0891e-01, -1.9116e-01,  9.2081e-02,  1.9823e-02,\n","          8.9705e-02, -5.9408e-03, -4.5740e-01, -2.3762e-01,  7.1990e-02],\n","        [ 5.2415e-01, -1.7462e-02,  2.6726e-01, -4.8367e-01, -5.4623e-01,\n","         -4.6140e-01, -9.0954e-01, -3.2711e-01,  6.2841e-01, -6.8115e-01,\n","         -5.4283e-01, -1.0669e+00,  4.1065e-02,  2.6575e-02, -4.9299e-01,\n","         -6.0261e-02,  2.3443e-01, -1.7207e-01, -6.2361e-01, -6.9515e-01,\n","          4.7967e-01,  2.0696e-01,  3.0631e-01,  5.0148e-01,  1.4116e-01,\n","         -1.2736e+00,  8.4477e-02,  7.0534e-01,  2.0184e-01, -6.5516e-01,\n","          3.6772e+00,  5.5260e-01,  5.1685e-01, -4.6614e-01, -6.0579e-01,\n","          8.1311e-02, -9.4303e-01, -3.8420e-01, -3.7791e-01,  7.0757e-01,\n","         -5.6461e-01,  4.7957e-01,  3.8767e-01,  3.5665e-02, -2.1340e-01,\n","          1.3890e-01,  1.4656e-01, -9.1548e-01, -6.1286e-01, -1.3111e+00],\n","        [ 4.2585e-01,  2.4063e-01, -3.8746e-01,  1.1012e-01,  3.4426e-01,\n","         -4.2530e-02, -4.9645e-01, -1.6882e-01, -1.1197e-02, -6.3692e-01,\n","          2.7094e-01, -1.3449e-01, -5.4045e-01,  1.2709e-01,  2.0455e-03,\n","          3.2044e-02,  1.0272e-01, -1.3278e-01, -8.2639e-01, -1.3595e-01,\n","          1.7693e-04, -3.1641e-01, -1.3848e-01, -2.2654e-01, -1.8164e-01,\n","         -1.8854e+00, -7.3774e-01,  8.5729e-02, -3.9770e-01, -2.1131e-01,\n","          4.0002e+00, -1.6095e-01, -5.2862e-01, -3.1822e-01,  6.0369e-03,\n","         -1.7909e-03,  1.7143e-01, -1.5120e-01,  1.7147e-02, -5.5345e-02,\n","         -2.9369e-01, -1.4898e-01, -3.2519e-01, -4.1542e-02, -4.2970e-01,\n","          1.8302e-01, -3.0532e-03, -1.8304e-01, -1.1499e-01, -7.6656e-01],\n","        [ 7.3109e-01,  1.0242e+00, -2.6714e-01, -3.3449e-01, -1.4873e+00,\n","         -3.7840e-01, -1.2873e+00,  7.1923e-01,  1.1322e+00, -9.0059e-01,\n","         -1.2594e-01,  7.2664e-01,  7.2766e-01, -1.0212e+00, -1.6100e+00,\n","         -2.3452e-01,  1.2707e+00, -6.4512e-02, -6.8331e-01,  2.6957e-01,\n","         -2.1492e-01, -1.0354e+00,  7.7650e-01,  1.5299e-04,  6.2976e-01,\n","         -1.3818e+00, -1.9453e-01,  1.3069e+00, -3.0326e-01, -1.4769e+00,\n","          2.9696e+00, -5.6795e-01,  3.3913e-01,  3.8007e-01,  1.8625e-03,\n","         -2.8222e-01, -8.7329e-01, -7.9432e-01,  1.3823e+00,  7.9638e-01,\n","         -7.1245e-01, -2.8103e-01,  5.6754e-02, -9.7843e-01, -1.1465e+00,\n","         -6.2487e-01,  2.6281e-01, -1.4434e+00,  4.0201e-01, -1.3707e+00],\n","        [ 6.1553e-01, -2.6617e-02,  2.7090e-01, -2.1872e-01,  4.3972e-01,\n","         -7.4917e-03, -4.3282e-01,  1.5646e-01, -3.2092e-01, -8.2234e-02,\n","          1.3161e-01,  2.6312e-01, -1.8442e-01, -3.9681e-01,  4.1553e-01,\n","          6.4369e-01,  8.4955e-02, -2.6939e-01, -3.8564e-01, -4.7367e-01,\n","          4.2713e-01,  1.4627e-01,  2.3875e-01, -2.0178e-01,  1.2257e-01,\n","         -2.1058e+00,  7.0311e-02, -3.0659e-01,  2.6320e-01, -6.1237e-01,\n","          3.8476e+00,  5.0292e-01, -9.0048e-01, -3.1804e-01,  1.7174e-01,\n","         -2.9481e-01,  1.5222e-01,  1.3110e-01,  1.9501e-01, -2.3850e-01,\n","         -1.0882e-01,  2.4269e-02,  1.6224e-01,  4.4542e-02, -1.7609e-01,\n","          7.6534e-02, -1.0343e-01, -1.0201e-01, -8.2547e-02, -1.6326e-01],\n","        [ 1.4263e-01, -6.1765e-02,  3.3451e-01, -6.5830e-01,  4.9816e-01,\n","         -2.9785e-01, -6.5784e-01,  4.7744e-01, -3.8023e-01,  1.5152e-01,\n","         -2.5446e-01,  6.2944e-01, -6.5304e-01, -1.3868e-01,  1.1765e+00,\n","          5.4232e-01,  3.6925e-01,  2.3052e-01,  9.6020e-02, -9.4804e-01,\n","         -4.5722e-01,  6.4838e-01,  6.8518e-01,  3.0402e-01,  7.8358e-01,\n","         -2.2566e+00, -6.9328e-01,  2.9823e-01,  9.0079e-01, -1.1932e+00,\n","          3.5501e+00,  1.0016e+00, -6.5262e-01,  1.8006e-01, -1.7057e-02,\n","         -6.7384e-02,  3.4020e-01,  5.2416e-01,  5.4316e-01, -6.3476e-01,\n","         -2.1356e-01,  1.5824e-01, -8.8281e-02,  6.8994e-01, -3.5392e-02,\n","         -5.9466e-02, -1.4563e-01, -2.2666e-01, -1.9391e-01,  7.7372e-01],\n","        [ 6.1553e-01, -2.6617e-02,  2.7090e-01, -2.1872e-01,  4.3972e-01,\n","         -7.4917e-03, -4.3282e-01,  1.5646e-01, -3.2092e-01, -8.2234e-02,\n","          1.3161e-01,  2.6312e-01, -1.8442e-01, -3.9681e-01,  4.1553e-01,\n","          6.4369e-01,  8.4955e-02, -2.6939e-01, -3.8564e-01, -4.7367e-01,\n","          4.2713e-01,  1.4627e-01,  2.3875e-01, -2.0178e-01,  1.2257e-01,\n","         -2.1058e+00,  7.0311e-02, -3.0659e-01,  2.6320e-01, -6.1237e-01,\n","          3.8476e+00,  5.0292e-01, -9.0048e-01, -3.1804e-01,  1.7174e-01,\n","         -2.9481e-01,  1.5222e-01,  1.3110e-01,  1.9501e-01, -2.3850e-01,\n","         -1.0882e-01,  2.4269e-02,  1.6224e-01,  4.4542e-02, -1.7609e-01,\n","          7.6534e-02, -1.0343e-01, -1.0201e-01, -8.2547e-02, -1.6326e-01],\n","        [ 4.2585e-01,  2.4063e-01, -3.8746e-01,  1.1012e-01,  3.4426e-01,\n","         -4.2530e-02, -4.9645e-01, -1.6882e-01, -1.1197e-02, -6.3692e-01,\n","          2.7094e-01, -1.3449e-01, -5.4045e-01,  1.2709e-01,  2.0455e-03,\n","          3.2044e-02,  1.0272e-01, -1.3278e-01, -8.2639e-01, -1.3595e-01,\n","          1.7693e-04, -3.1641e-01, -1.3848e-01, -2.2654e-01, -1.8164e-01,\n","         -1.8854e+00, -7.3774e-01,  8.5729e-02, -3.9770e-01, -2.1131e-01,\n","          4.0002e+00, -1.6095e-01, -5.2862e-01, -3.1822e-01,  6.0369e-03,\n","         -1.7909e-03,  1.7143e-01, -1.5120e-01,  1.7147e-02, -5.5345e-02,\n","         -2.9369e-01, -1.4898e-01, -3.2519e-01, -4.1542e-02, -4.2970e-01,\n","          1.8302e-01, -3.0532e-03, -1.8304e-01, -1.1499e-01, -7.6656e-01],\n","        [ 6.4688e-01,  1.5905e-01,  2.9960e-02,  3.4753e-01,  9.0600e-02,\n","          6.0649e-01, -6.6265e-01, -5.1340e-01, -4.6278e-01, -8.9652e-02,\n","          5.0401e-02,  2.6166e-01,  1.2158e-01,  4.2149e-02,  1.7326e-01,\n","          1.5985e-02,  1.4157e-01, -1.5875e-01, -1.0697e-01, -9.2730e-01,\n","          2.1721e-01, -1.0644e-01,  6.1763e-01,  3.6421e-01, -4.7785e-01,\n","         -1.2847e+00, -5.4840e-01, -7.1594e-01, -1.6902e-01, -4.8032e-01,\n","          4.0154e+00,  4.6988e-01,  8.8424e-02, -8.5966e-01,  5.0570e-01,\n","          3.3008e-01, -3.5687e-01, -1.6527e-01, -3.1509e-01,  4.8572e-01,\n","         -3.6592e-01,  3.1859e-01,  7.0491e-01,  6.2475e-01,  6.9474e-01,\n","          1.2992e-01, -7.3295e-01,  1.0123e-01, -3.0848e-01, -4.4190e-01],\n","        [ 6.6488e-01, -1.1391e-01,  6.7844e-01,  1.7951e-01,  6.8280e-01,\n","         -4.7787e-01, -3.0761e-01,  1.7489e-01, -7.0512e-01, -5.5022e-01,\n","          1.5140e-01,  1.0214e-01, -4.5063e-01, -3.3069e-01,  5.6133e-02,\n","          1.2271e+00,  5.5607e-01, -6.8297e-01,  3.7364e-02,  7.0266e-01,\n","          1.9093e+00, -6.1483e-01, -8.3329e-01, -3.0230e-01, -1.1118e+00,\n","         -1.5500e+00,  2.6040e-01,  2.2957e-01, -1.0375e+00, -3.1789e-01,\n","          3.5091e+00, -2.5871e-01,  1.0151e+00,  6.5927e-01, -1.8231e-01,\n","         -7.5859e-01, -3.0927e-01, -9.1678e-01,  1.0633e+00, -6.6761e-01,\n","         -3.7464e-01, -2.9143e-01,  6.5606e-01, -4.4642e-01, -7.5495e-02,\n","         -1.0552e+00, -6.0501e-01,  7.3582e-01,  1.0139e+00, -2.7749e-01]])\n"]}],"source":["X_word = torch.tensor(model[tokens])\n","Y = torch.matmul(S_softmax, X_word)\n","print(Y)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"0EG_bqPgjlV5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735973382613,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"4ce0cfde-7793-4a19-fad4-013020843816"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000e+00, 3.9380e-08, 0.0000e+00],\n","        [1.0000e+00, 1.8747e-11, 0.0000e+00],\n","        [9.9842e-01, 1.5780e-03, 0.0000e+00],\n","        [1.0000e+00, 1.1429e-13, 0.0000e+00]])\n"]}],"source":["## In-built softmax function in PyTorch (dim = 1 corresponds to row-by-row)\n","## applied to the toy patient data matrix\n","softmax_scores = torch.nn.functional.softmax(Z, dim = 1)\n","print(softmax_scores)"]},{"cell_type":"markdown","metadata":{"id":"6_8cPXFFkiUZ"},"source":["---\n","\n","A toy data matrix with output labels and an initial weights matrix for the softmax classifier:\n","\n","![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n","\n","---"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"MJ3U-JCukmIG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974795307,"user_tz":-330,"elapsed":420,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"7452f6f9-3dec-4b0e-d044-5b14fb3fabdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.9799, -0.7019, -0.7238, -0.9871,  0.8920],\n","        [ 0.7186,  0.3509, -1.2449, -0.6050, -1.2374],\n","        [-1.5025, -1.7547,  1.3607,  0.3503,  1.0647],\n","        [ 1.3718,  1.4037,  0.4922,  0.6687,  0.1439],\n","        [ 0.5879,  0.5615,  1.0133,  1.6876,  0.6043],\n","        [-0.1960,  0.1404, -0.8975, -1.1144, -1.4676]])\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]])\n","tensor([[-0.1000,  0.5000,  0.3000],\n","        [ 0.9000,  0.3000,  0.5000],\n","        [-1.5000,  0.4000,  0.1000],\n","        [ 0.1000,  0.1000, -1.0000],\n","        [-1.2000,  0.5000, -0.8000]])\n"]}],"source":["# Create the data matrix (read from a file typically)\n","X = np.array([[72, 120, 37.3, 104, 32.5],\n","              [85, 130, 37.0, 110, 14],\n","              [68, 110, 38.5, 125, 34],\n","              [90, 140, 38.0, 130, 26],\n","              [84, 132, 38.3, 146, 30],\n","              [78, 128, 37.2, 102, 12]])\n","\n","# Standardize the data matrix\n","sc = StandardScaler()\n","X_S = sc.fit_transform(X)  # fit(), fit_transform(), transform()\n","\n","# Convert to a PyTorch tensor\n","X_S = torch.tensor(X_S, dtype = torch.float32)\n","\n","# Get the number of samples and features\n","num_samples, num_features = X_S.shape\n","\n","# Create the output labels vector (also read from a file typically)\n","y = np.array(['non-diabetic',\n","              'diabetic',\n","              'non-diabetic',\n","              'pre-diabetic',\n","              'diabetic',\n","              'pre-diabetic'])\n","\n","# One-hot encoding of output labels using scikit-learn\n","ohe = OneHotEncoder(sparse_output = False)  # Use `sparse_output=False` for dense array\n","Y = ohe.fit_transform(y.reshape(-1, 1))\n","\n","# Convert to a PyTorch tensor\n","Y = torch.tensor(Y, dtype = torch.float32)\n","\n","# Get the number of labels\n","num_labels = Y.shape[1]\n","\n","# Create the weights matrix\n","W = torch.tensor([[-0.1, 0.5, 0.3],\n","                  [0.9, 0.3, 0.5],\n","                  [-1.5, 0.4, 0.1],\n","                  [0.1, 0.1, -1.0],\n","                  [-1.2, 0.5, -0.8]], dtype = torch.float32)\n","\n","print(X_S)\n","print(Y)\n","print(W)"]},{"cell_type":"markdown","metadata":{"id":"1cQDyu7llDo9"},"source":["---\n","\n","Bias trick to absorb the bias into the weights matrix\n","\n","![bias trick](https://1drv.ms/i/c/37720f927b6ddc34/IQR8NDbhvaddQa3W3F_46q4nATD7WBNgnwGJ7QC6HDL6g14?width=850)\n","\n","---"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"DlviiS0tlH7p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974797458,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"d691bec2-2c3e-4d96-934b-73cae23dea69"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.9799, -0.7019, -0.7238, -0.9871,  0.8920,  1.0000],\n","        [ 0.7186,  0.3509, -1.2449, -0.6050, -1.2374,  1.0000],\n","        [-1.5025, -1.7547,  1.3607,  0.3503,  1.0647,  1.0000],\n","        [ 1.3718,  1.4037,  0.4922,  0.6687,  0.1439,  1.0000],\n","        [ 0.5879,  0.5615,  1.0133,  1.6876,  0.6043,  1.0000],\n","        [-0.1960,  0.1404, -0.8975, -1.1144, -1.4676,  1.0000]])\n","tensor([[-0.1000,  0.5000,  0.3000],\n","        [ 0.9000,  0.3000,  0.5000],\n","        [-1.5000,  0.4000,  0.1000],\n","        [ 0.1000,  0.1000, -1.0000],\n","        [-1.2000,  0.5000, -0.8000],\n","        [ 0.1000,  0.1000,  0.1000]])\n"]}],"source":["## Bias trick to absorb the bias into the weights matrix\n","# Concatenate a column of ones to X_S (bias term)\n","X_B = torch.cat([X_S, torch.ones((num_samples, 1))], dim = 1)\n","\n","# Create the bias vector `b`\n","b = 0.1 * torch.ones((1, num_labels))\n","\n","# Concatenate the weights matrix `W` with the bias vector `b`\n","W_B = torch.cat([W, b], dim = 0)\n","\n","print(X_B)\n","print(W_B)"]},{"cell_type":"markdown","metadata":{"id":"5rZkNr8d1gAw"},"source":["---\n","\n","Forward propagation for the toy patient dataset: $$\\textbf{bias-added input }\\mathbf{X}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{raw scores }\\mathbf{Z}=\\mathbf{X}_B\\textbf{W}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{softmax activated scores }\\mathbf{A}=\\text{softmax}(\\mathbf{Z}).$$\n","\n","---"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"A5g-D7NLlZBl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974803455,"user_tz":-330,"elapsed":410,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"bfe77af3-b549-41ef-e0cb-76e20bb87471"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.5171, -0.5427, -0.3438],\n","        [ 3.6357, -0.6126,  1.9614],\n","        [-4.6127, -0.0660, -2.2940],\n","        [ 0.3821,  1.5427,  0.4789],\n","        [-1.5298,  1.4386, -1.5126],\n","        [ 3.2418, -1.1601,  2.3101]])\n","tensor([[0.3161, 0.3081, 0.3759],\n","        [0.8321, 0.0119, 0.1560],\n","        [0.0095, 0.8942, 0.0963],\n","        [0.1889, 0.6030, 0.2081],\n","        [0.0466, 0.9061, 0.0474],\n","        [0.7112, 0.0087, 0.2801]])\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]])\n"]}],"source":["# Raw scores matrix\n","Z = torch.matmul(X_B, W_B) # also referred to as logits\n","print(Z)\n","\n","# Softmax activated scores\n","A = torch.nn.functional.softmax(Z, dim = 1)\n","\n","# Predicted probabilities for each sample\n","print(A)\n","\n","# True output label for each sample\n","print(Y)"]},{"cell_type":"markdown","metadata":{"id":"dgtOD11HljXv"},"source":["---\n","\n","Loss for each sample can be quantified using the categorical crossentropy (CCE) loss function which is defined as $$\\color{yellow}{-\\log(\\text{predicted probability that a sample belongs its correct class})}$$\n","\n","For example, consider a sample with\n","\n","- true_label = [$\\color{yellow}{1}$ 0 0]\n","- predicted_label = [$\\color{yellow}{0.05}$, 0.99, 0.05]\n","\n","categorical crossentropy loss = $-\\log(\\color{yellow}{0.05}).$\n","\n","Here, we calculate the average CCE loss for all all samples and average them out.\n","\n","---"]},{"cell_type":"code","source":["loss = -torch.mean(torch.log(torch.sum(Y * A, dim = 1)))\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7UEwj-L5KJ5t","executionInfo":{"status":"ok","timestamp":1735974816172,"user_tz":-330,"elapsed":450,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"5c9cbae8-9e7c-4ccf-d2da-897bc4b4eaab"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.2304)\n"]}]},{"cell_type":"code","execution_count":56,"metadata":{"id":"aiCY-GvwlpDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974826626,"user_tz":-330,"elapsed":396,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"820fdcd6-bf33-429f-e985-9bbdab4204c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.2304)\n","tensor(1.2304)\n"]}],"source":["## Calculate average CCE loss\n","loss = torch.mean(-torch.log(torch.sum(Y * A, dim = 1)))\n","print(loss)\n","\n","# Using the PyTorch in-built function for CCE loss\n","loss_fn = torch.nn.CrossEntropyLoss()\n","loss = loss_fn(Z, torch.argmax(Y, dim = 1)) # Note that the arguemnts are logits and correct labels\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"Iro8wo7tnv2g"},"source":["---\n","\n","Applying the gradient descent method with\n","\n","- a maximum number of iterations equal to 1000\n","- a stopping tolerance equal to $10^{-6}$\n","- a learning rate of 0.01\n","\n"," to minimize $$L(\\mathbf{w}) = (w_1-2)^2+(w_2+3)^2$$ starting from $\\mathbf{w} = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$\n","\n","---"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"qcM3YnJmnwrY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974863093,"user_tz":-330,"elapsed":1406,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"3e994a14-4aa2-46a3-e625-9f46c8acd2f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1: ||grad|| = 7.211102485656738\n","Iteration 2: ||grad|| = 7.066880702972412\n","Iteration 3: ||grad|| = 6.925542831420898\n","Iteration 4: ||grad|| = 6.787032127380371\n","Iteration 5: ||grad|| = 6.6512908935546875\n","Iteration 6: ||grad|| = 6.518265724182129\n","Iteration 7: ||grad|| = 6.387900352478027\n","Iteration 8: ||grad|| = 6.2601423263549805\n","Iteration 9: ||grad|| = 6.134939193725586\n","Iteration 10: ||grad|| = 6.012240409851074\n","Iteration 11: ||grad|| = 5.891995906829834\n","Iteration 12: ||grad|| = 5.774156093597412\n","Iteration 13: ||grad|| = 5.65867280960083\n","Iteration 14: ||grad|| = 5.545499324798584\n","Iteration 15: ||grad|| = 5.434589385986328\n","Iteration 16: ||grad|| = 5.325897693634033\n","Iteration 17: ||grad|| = 5.219379901885986\n","Iteration 18: ||grad|| = 5.114992141723633\n","Iteration 19: ||grad|| = 5.012691974639893\n","Iteration 20: ||grad|| = 4.91243839263916\n","Iteration 21: ||grad|| = 4.814189434051514\n","Iteration 22: ||grad|| = 4.717905521392822\n","Iteration 23: ||grad|| = 4.623547554016113\n","Iteration 24: ||grad|| = 4.531076431274414\n","Iteration 25: ||grad|| = 4.440454959869385\n","Iteration 26: ||grad|| = 4.3516459465026855\n","Iteration 27: ||grad|| = 4.264613151550293\n","Iteration 28: ||grad|| = 4.179320812225342\n","Iteration 29: ||grad|| = 4.095734119415283\n","Iteration 30: ||grad|| = 4.013819694519043\n","Iteration 31: ||grad|| = 3.9335432052612305\n","Iteration 32: ||grad|| = 3.854872465133667\n","Iteration 33: ||grad|| = 3.7777750492095947\n","Iteration 34: ||grad|| = 3.7022194862365723\n","Iteration 35: ||grad|| = 3.6281752586364746\n","Iteration 36: ||grad|| = 3.5556118488311768\n","Iteration 37: ||grad|| = 3.484499454498291\n","Iteration 38: ||grad|| = 3.4148097038269043\n","Iteration 39: ||grad|| = 3.346513509750366\n","Iteration 40: ||grad|| = 3.279583215713501\n","Iteration 41: ||grad|| = 3.213991641998291\n","Iteration 42: ||grad|| = 3.149711847305298\n","Iteration 43: ||grad|| = 3.0867176055908203\n","Iteration 44: ||grad|| = 3.0249834060668945\n","Iteration 45: ||grad|| = 2.9644837379455566\n","Iteration 46: ||grad|| = 2.905194044113159\n","Iteration 47: ||grad|| = 2.847090482711792\n","Iteration 48: ||grad|| = 2.7901482582092285\n","Iteration 49: ||grad|| = 2.7343454360961914\n","Iteration 50: ||grad|| = 2.6796586513519287\n","Iteration 51: ||grad|| = 2.626065492630005\n","Iteration 52: ||grad|| = 2.5735440254211426\n","Iteration 53: ||grad|| = 2.522073268890381\n","Iteration 54: ||grad|| = 2.4716317653656006\n","Iteration 55: ||grad|| = 2.422199010848999\n","Iteration 56: ||grad|| = 2.3737549781799316\n","Iteration 57: ||grad|| = 2.326280117034912\n","Iteration 58: ||grad|| = 2.279754400253296\n","Iteration 59: ||grad|| = 2.234158992767334\n","Iteration 60: ||grad|| = 2.1894760131835938\n","Iteration 61: ||grad|| = 2.145686388015747\n","Iteration 62: ||grad|| = 2.1027727127075195\n","Iteration 63: ||grad|| = 2.0607173442840576\n","Iteration 64: ||grad|| = 2.019503116607666\n","Iteration 65: ||grad|| = 1.979112982749939\n","Iteration 66: ||grad|| = 1.9395304918289185\n","Iteration 67: ||grad|| = 1.9007399082183838\n","Iteration 68: ||grad|| = 1.8627251386642456\n","Iteration 69: ||grad|| = 1.8254705667495728\n","Iteration 70: ||grad|| = 1.7889609336853027\n","Iteration 71: ||grad|| = 1.7531819343566895\n","Iteration 72: ||grad|| = 1.7181183099746704\n","Iteration 73: ||grad|| = 1.6837559938430786\n","Iteration 74: ||grad|| = 1.6500808000564575\n","Iteration 75: ||grad|| = 1.6170790195465088\n","Iteration 76: ||grad|| = 1.5847375392913818\n","Iteration 77: ||grad|| = 1.553043007850647\n","Iteration 78: ||grad|| = 1.521982192993164\n","Iteration 79: ||grad|| = 1.4915425777435303\n","Iteration 80: ||grad|| = 1.4617117643356323\n","Iteration 81: ||grad|| = 1.432477355003357\n","Iteration 82: ||grad|| = 1.4038276672363281\n","Iteration 83: ||grad|| = 1.37575101852417\n","Iteration 84: ||grad|| = 1.3482359647750854\n","Iteration 85: ||grad|| = 1.3212710618972778\n","Iteration 86: ||grad|| = 1.2948455810546875\n","Iteration 87: ||grad|| = 1.2689487934112549\n","Iteration 88: ||grad|| = 1.2435698509216309\n","Iteration 89: ||grad|| = 1.218698501586914\n","Iteration 90: ||grad|| = 1.1943244934082031\n","Iteration 91: ||grad|| = 1.1704381704330444\n","Iteration 92: ||grad|| = 1.1470293998718262\n","Iteration 93: ||grad|| = 1.1240887641906738\n","Iteration 94: ||grad|| = 1.1016069650650024\n","Iteration 95: ||grad|| = 1.079574704170227\n","Iteration 96: ||grad|| = 1.057983160018921\n","Iteration 97: ||grad|| = 1.0368235111236572\n","Iteration 98: ||grad|| = 1.0160870552062988\n","Iteration 99: ||grad|| = 0.9957653284072876\n","Iteration 100: ||grad|| = 0.9758499264717102\n","Iteration 101: ||grad|| = 0.9563328623771667\n","Iteration 102: ||grad|| = 0.9372060894966125\n","Iteration 103: ||grad|| = 0.9184620380401611\n","Iteration 104: ||grad|| = 0.9000928401947021\n","Iteration 105: ||grad|| = 0.8820909857749939\n","Iteration 106: ||grad|| = 0.8644490838050842\n","Iteration 107: ||grad|| = 0.8471602201461792\n","Iteration 108: ||grad|| = 0.8302169442176819\n","Iteration 109: ||grad|| = 0.8136128187179565\n","Iteration 110: ||grad|| = 0.7973405718803406\n","Iteration 111: ||grad|| = 0.7813937067985535\n","Iteration 112: ||grad|| = 0.7657656669616699\n","Iteration 113: ||grad|| = 0.7504504919052124\n","Iteration 114: ||grad|| = 0.7354413270950317\n","Iteration 115: ||grad|| = 0.7207325100898743\n","Iteration 116: ||grad|| = 0.7063177227973938\n","Iteration 117: ||grad|| = 0.6921911835670471\n","Iteration 118: ||grad|| = 0.6783471703529358\n","Iteration 119: ||grad|| = 0.6647803783416748\n","Iteration 120: ||grad|| = 0.6514847874641418\n","Iteration 121: ||grad|| = 0.6384550333023071\n","Iteration 122: ||grad|| = 0.6256861090660095\n","Iteration 123: ||grad|| = 0.6131722927093506\n","Iteration 124: ||grad|| = 0.6009087562561035\n","Iteration 125: ||grad|| = 0.5888906121253967\n","Iteration 126: ||grad|| = 0.57711261510849\n","Iteration 127: ||grad|| = 0.5655705332756042\n","Iteration 128: ||grad|| = 0.5542590022087097\n","Iteration 129: ||grad|| = 0.5431737303733826\n","Iteration 130: ||grad|| = 0.5323102474212646\n","Iteration 131: ||grad|| = 0.5216640830039978\n","Iteration 132: ||grad|| = 0.5112309455871582\n","Iteration 133: ||grad|| = 0.5010064840316772\n","Iteration 134: ||grad|| = 0.49098649621009827\n","Iteration 135: ||grad|| = 0.4811667501926422\n","Iteration 136: ||grad|| = 0.4715435206890106\n","Iteration 137: ||grad|| = 0.462112694978714\n","Iteration 138: ||grad|| = 0.4528704881668091\n","Iteration 139: ||grad|| = 0.44381290674209595\n","Iteration 140: ||grad|| = 0.4349364936351776\n","Iteration 141: ||grad|| = 0.42623767256736755\n","Iteration 142: ||grad|| = 0.417712926864624\n","Iteration 143: ||grad|| = 0.40935850143432617\n","Iteration 144: ||grad|| = 0.40117138624191284\n","Iteration 145: ||grad|| = 0.39314812421798706\n","Iteration 146: ||grad|| = 0.38528528809547424\n","Iteration 147: ||grad|| = 0.37757956981658936\n","Iteration 148: ||grad|| = 0.3700280487537384\n","Iteration 149: ||grad|| = 0.36262768507003784\n","Iteration 150: ||grad|| = 0.35537517070770264\n","Iteration 151: ||grad|| = 0.3482677638530731\n","Iteration 152: ||grad|| = 0.3413025140762329\n","Iteration 153: ||grad|| = 0.334476500749588\n","Iteration 154: ||grad|| = 0.3277868628501892\n","Iteration 155: ||grad|| = 0.32123130559921265\n","Iteration 156: ||grad|| = 0.31480658054351807\n","Iteration 157: ||grad|| = 0.3085106313228607\n","Iteration 158: ||grad|| = 0.30234020948410034\n","Iteration 159: ||grad|| = 0.29629358649253845\n","Iteration 160: ||grad|| = 0.29036781191825867\n","Iteration 161: ||grad|| = 0.2845606803894043\n","Iteration 162: ||grad|| = 0.2788693904876709\n","Iteration 163: ||grad|| = 0.2732921242713928\n","Iteration 164: ||grad|| = 0.26782605051994324\n","Iteration 165: ||grad|| = 0.26246950030326843\n","Iteration 166: ||grad|| = 0.257220059633255\n","Iteration 167: ||grad|| = 0.2520754933357239\n","Iteration 168: ||grad|| = 0.2470339685678482\n","Iteration 169: ||grad|| = 0.24209333956241608\n","Iteration 170: ||grad|| = 0.2372513711452484\n","Iteration 171: ||grad|| = 0.23250623047351837\n","Iteration 172: ||grad|| = 0.22785614430904388\n","Iteration 173: ||grad|| = 0.22329892218112946\n","Iteration 174: ||grad|| = 0.21883293986320496\n","Iteration 175: ||grad|| = 0.2144562304019928\n","Iteration 176: ||grad|| = 0.21016719937324524\n","Iteration 177: ||grad|| = 0.20596374571323395\n","Iteration 178: ||grad|| = 0.20184439420700073\n","Iteration 179: ||grad|| = 0.19780756533145905\n","Iteration 180: ||grad|| = 0.1938515454530716\n","Iteration 181: ||grad|| = 0.1899746209383011\n","Iteration 182: ||grad|| = 0.18617506325244904\n","Iteration 183: ||grad|| = 0.18245166540145874\n","Iteration 184: ||grad|| = 0.1788027286529541\n","Iteration 185: ||grad|| = 0.17522667348384857\n","Iteration 186: ||grad|| = 0.1717221438884735\n","Iteration 187: ||grad|| = 0.16828759014606476\n","Iteration 188: ||grad|| = 0.1649218201637268\n","Iteration 189: ||grad|| = 0.16162320971488953\n","Iteration 190: ||grad|| = 0.1583908647298813\n","Iteration 191: ||grad|| = 0.15522293746471405\n","Iteration 192: ||grad|| = 0.1521184742450714\n","Iteration 193: ||grad|| = 0.14907604455947876\n","Iteration 194: ||grad|| = 0.14609432220458984\n","Iteration 195: ||grad|| = 0.1431722342967987\n","Iteration 196: ||grad|| = 0.14030861854553223\n","Iteration 197: ||grad|| = 0.1375022530555725\n","Iteration 198: ||grad|| = 0.134752094745636\n","Iteration 199: ||grad|| = 0.13205695152282715\n","Iteration 200: ||grad|| = 0.12941564619541168\n","Iteration 201: ||grad|| = 0.12682749330997467\n","Iteration 202: ||grad|| = 0.12429092079401016\n","Iteration 203: ||grad|| = 0.12180500477552414\n","Iteration 204: ||grad|| = 0.11936880648136139\n","Iteration 205: ||grad|| = 0.11698141694068909\n","Iteration 206: ||grad|| = 0.11464203149080276\n","Iteration 207: ||grad|| = 0.11234906315803528\n","Iteration 208: ||grad|| = 0.11010199040174484\n","Iteration 209: ||grad|| = 0.10790015012025833\n","Iteration 210: ||grad|| = 0.105741947889328\n","Iteration 211: ||grad|| = 0.10362725704908371\n","Iteration 212: ||grad|| = 0.10155488550662994\n","Iteration 213: ||grad|| = 0.09952377527952194\n","Iteration 214: ||grad|| = 0.09753340482711792\n","Iteration 215: ||grad|| = 0.09558270126581192\n","Iteration 216: ||grad|| = 0.09367088973522186\n","Iteration 217: ||grad|| = 0.09179741889238358\n","Iteration 218: ||grad|| = 0.08996125310659409\n","Iteration 219: ||grad|| = 0.08816184103488922\n","Iteration 220: ||grad|| = 0.08639854192733765\n","Iteration 221: ||grad|| = 0.08467068523168564\n","Iteration 222: ||grad|| = 0.08297721296548843\n","Iteration 223: ||grad|| = 0.08131759613752365\n","Iteration 224: ||grad|| = 0.07969117909669876\n","Iteration 225: ||grad|| = 0.07809742540121078\n","Iteration 226: ||grad|| = 0.0765356719493866\n","Iteration 227: ||grad|| = 0.07500500231981277\n","Iteration 228: ||grad|| = 0.07350475341081619\n","Iteration 229: ||grad|| = 0.07203477621078491\n","Iteration 230: ||grad|| = 0.07059403508901596\n","Iteration 231: ||grad|| = 0.06918199360370636\n","Iteration 232: ||grad|| = 0.06779851764440536\n","Iteration 233: ||grad|| = 0.06644255667924881\n","Iteration 234: ||grad|| = 0.06511356681585312\n","Iteration 235: ||grad|| = 0.06381142139434814\n","Iteration 236: ||grad|| = 0.06253520399332047\n","Iteration 237: ||grad|| = 0.061284638941287994\n","Iteration 238: ||grad|| = 0.06005880609154701\n","Iteration 239: ||grad|| = 0.058857571333646774\n","Iteration 240: ||grad|| = 0.0576804094016552\n","Iteration 241: ||grad|| = 0.0565267838537693\n","Iteration 242: ||grad|| = 0.055396173149347305\n","Iteration 243: ||grad|| = 0.05428830534219742\n","Iteration 244: ||grad|| = 0.053202394396066666\n","Iteration 245: ||grad|| = 0.05213817209005356\n","Iteration 246: ||grad|| = 0.05109523981809616\n","Iteration 247: ||grad|| = 0.05007333680987358\n","Iteration 248: ||grad|| = 0.04907206818461418\n","Iteration 249: ||grad|| = 0.04809050261974335\n","Iteration 250: ||grad|| = 0.04712877422571182\n","Iteration 251: ||grad|| = 0.046186089515686035\n","Iteration 252: ||grad|| = 0.04526231810450554\n","Iteration 253: ||grad|| = 0.04435693100094795\n","Iteration 254: ||grad|| = 0.04346979036927223\n","Iteration 255: ||grad|| = 0.042600374668836594\n","Iteration 256: ||grad|| = 0.0417482815682888\n","Iteration 257: ||grad|| = 0.04091325029730797\n","Iteration 258: ||grad|| = 0.04009488224983215\n","Iteration 259: ||grad|| = 0.03929304704070091\n","Iteration 260: ||grad|| = 0.03850734233856201\n","Iteration 261: ||grad|| = 0.03773711249232292\n","Iteration 262: ||grad|| = 0.03698235750198364\n","Iteration 263: ||grad|| = 0.03624254837632179\n","Iteration 264: ||grad|| = 0.03551768139004707\n","Iteration 265: ||grad|| = 0.034807492047548294\n","Iteration 266: ||grad|| = 0.03411119431257248\n","Iteration 267: ||grad|| = 0.03342917561531067\n","Iteration 268: ||grad|| = 0.0327603854238987\n","Iteration 269: ||grad|| = 0.03210534527897835\n","Iteration 270: ||grad|| = 0.03146339952945709\n","Iteration 271: ||grad|| = 0.030834149569272995\n","Iteration 272: ||grad|| = 0.03021746501326561\n","Iteration 273: ||grad|| = 0.029612945392727852\n","Iteration 274: ||grad|| = 0.02902085706591606\n","Iteration 275: ||grad|| = 0.028440410271286964\n","Iteration 276: ||grad|| = 0.02787146531045437\n","Iteration 277: ||grad|| = 0.027313897386193275\n","Iteration 278: ||grad|| = 0.026767700910568237\n","Iteration 279: ||grad|| = 0.026232348755002022\n","Iteration 280: ||grad|| = 0.025707842782139778\n","Iteration 281: ||grad|| = 0.02519378252327442\n","Iteration 282: ||grad|| = 0.024690039455890656\n","Iteration 283: ||grad|| = 0.02419608272612095\n","Iteration 284: ||grad|| = 0.023712310940027237\n","Iteration 285: ||grad|| = 0.0232379250228405\n","Iteration 286: ||grad|| = 0.022773196920752525\n","Iteration 287: ||grad|| = 0.022317592054605484\n","Iteration 288: ||grad|| = 0.021871114149689674\n","Iteration 289: ||grad|| = 0.021433759480714798\n","Iteration 290: ||grad|| = 0.021005135029554367\n","Iteration 291: ||grad|| = 0.020585106685757637\n","Iteration 292: ||grad|| = 0.020173542201519012\n","Iteration 293: ||grad|| = 0.019770044833421707\n","Iteration 294: ||grad|| = 0.01937461458146572\n","Iteration 295: ||grad|| = 0.01898711919784546\n","Iteration 296: ||grad|| = 0.018607163801789284\n","Iteration 297: ||grad|| = 0.018235141411423683\n","Iteration 298: ||grad|| = 0.017870524898171425\n","Iteration 299: ||grad|| = 0.017512919381260872\n","Iteration 300: ||grad|| = 0.01716271974146366\n","Iteration 301: ||grad|| = 0.01681939698755741\n","Iteration 302: ||grad|| = 0.016482951119542122\n","Iteration 303: ||grad|| = 0.016153382137417793\n","Iteration 304: ||grad|| = 0.015830159187316895\n","Iteration 305: ||grad|| = 0.015513683669269085\n","Iteration 306: ||grad|| = 0.015203556045889854\n","Iteration 307: ||grad|| = 0.014899378642439842\n","Iteration 308: ||grad|| = 0.014601417817175388\n","Iteration 309: ||grad|| = 0.014309275895357132\n","Iteration 310: ||grad|| = 0.014023217372596264\n","Iteration 311: ||grad|| = 0.013742845505475998\n","Iteration 312: ||grad|| = 0.013467895798385143\n","Iteration 313: ||grad|| = 0.013198500499129295\n","Iteration 314: ||grad|| = 0.012934396043419838\n","Iteration 315: ||grad|| = 0.012675845995545387\n","Iteration 316: ||grad|| = 0.012422452680766582\n","Iteration 317: ||grad|| = 0.012173821218311787\n","Iteration 318: ||grad|| = 0.011930347420275211\n","Iteration 319: ||grad|| = 0.011691899970173836\n","Iteration 320: ||grad|| = 0.011458080261945724\n","Iteration 321: ||grad|| = 0.011228889226913452\n","Iteration 322: ||grad|| = 0.011004326865077019\n","Iteration 323: ||grad|| = 0.010784261859953403\n","Iteration 324: ||grad|| = 0.010568693280220032\n","Iteration 325: ||grad|| = 0.010357224382460117\n","Iteration 326: ||grad|| = 0.010150250978767872\n","Iteration 327: ||grad|| = 0.009947378188371658\n","Iteration 328: ||grad|| = 0.009748473763465881\n","Iteration 329: ||grad|| = 0.009553535841405392\n","Iteration 330: ||grad|| = 0.009362565353512764\n","Iteration 331: ||grad|| = 0.009175166487693787\n","Iteration 332: ||grad|| = 0.008991734124720097\n","Iteration 333: ||grad|| = 0.008811873383820057\n","Iteration 334: ||grad|| = 0.008635450154542923\n","Iteration 335: ||grad|| = 0.00846286304295063\n","Iteration 336: ||grad|| = 0.008293714374303818\n","Iteration 337: ||grad|| = 0.008128004148602486\n","Iteration 338: ||grad|| = 0.007965335622429848\n","Iteration 339: ||grad|| = 0.007805973291397095\n","Iteration 340: ||grad|| = 0.007650049403309822\n","Iteration 341: ||grad|| = 0.0074970354326069355\n","Iteration 342: ||grad|| = 0.00734693044796586\n","Iteration 343: ||grad|| = 0.0072001321241259575\n","Iteration 344: ||grad|| = 0.007056243252009153\n","Iteration 345: ||grad|| = 0.006915263831615448\n","Iteration 346: ||grad|| = 0.006777061615139246\n","Iteration 347: ||grad|| = 0.0066413721069693565\n","Iteration 348: ||grad|| = 0.006508460268378258\n","Iteration 349: ||grad|| = 0.006378325633704662\n","Iteration 350: ||grad|| = 0.006250570993870497\n","Iteration 351: ||grad|| = 0.006125594023615122\n","Iteration 352: ||grad|| = 0.006002997513860464\n","Iteration 353: ||grad|| = 0.0058827814646065235\n","Iteration 354: ||grad|| = 0.005765210837125778\n","Iteration 355: ||grad|| = 0.005650019738823175\n","Iteration 356: ||grad|| = 0.005537078250199556\n","Iteration 357: ||grad|| = 0.005426384042948484\n","Iteration 358: ||grad|| = 0.005318070761859417\n","Iteration 359: ||grad|| = 0.005211608484387398\n","Iteration 360: ||grad|| = 0.005107394885271788\n","Iteration 361: ||grad|| = 0.005005297251045704\n","Iteration 362: ||grad|| = 0.004905051086097956\n","Iteration 363: ||grad|| = 0.004807053133845329\n","Iteration 364: ||grad|| = 0.00471077486872673\n","Iteration 365: ||grad|| = 0.004616744350641966\n","Iteration 366: ||grad|| = 0.004524433519691229\n","Iteration 367: ||grad|| = 0.00443397369235754\n","Iteration 368: ||grad|| = 0.004345233552157879\n","Iteration 369: ||grad|| = 0.0042582121677696705\n","Iteration 370: ||grad|| = 0.00417291047051549\n","Iteration 371: ||grad|| = 0.004089327994734049\n","Iteration 372: ||grad|| = 0.004007464740425348\n","Iteration 373: ||grad|| = 0.003927320707589388\n","Iteration 374: ||grad|| = 0.0038487636484205723\n","Iteration 375: ||grad|| = 0.0037719260435551405\n","Iteration 376: ||grad|| = 0.003696410683915019\n","Iteration 377: ||grad|| = 0.003622482530772686\n","Iteration 378: ||grad|| = 0.0035501413512974977\n","Iteration 379: ||grad|| = 0.0034791226498782635\n","Iteration 380: ||grad|| = 0.003409690922126174\n","Iteration 381: ||grad|| = 0.0033414496574550867\n","Iteration 382: ||grad|| = 0.0032745306380093098\n","Iteration 383: ||grad|| = 0.0032091985922306776\n","Iteration 384: ||grad|| = 0.0031450570095330477\n","Iteration 385: ||grad|| = 0.0030821056570857763\n","Iteration 386: ||grad|| = 0.0030203445348888636\n","Iteration 387: ||grad|| = 0.0029600381385535\n","Iteration 388: ||grad|| = 0.0029009219724684954\n","Iteration 389: ||grad|| = 0.002842996036633849\n","Iteration 390: ||grad|| = 0.002786260563880205\n","Iteration 391: ||grad|| = 0.0027305828407406807\n","Iteration 392: ||grad|| = 0.0026760955806821585\n","Iteration 393: ||grad|| = 0.0026226663030683994\n","Iteration 394: ||grad|| = 0.0025700305122882128\n","Iteration 395: ||grad|| = 0.0025184527039527893\n","Iteration 396: ||grad|| = 0.0024680651258677244\n","Iteration 397: ||grad|| = 0.0024187355302274227\n","Iteration 398: ||grad|| = 0.002370463917031884\n","Iteration 399: ||grad|| = 0.0023229860235005617\n","Iteration 400: ||grad|| = 0.0022765658795833588\n","Iteration 401: ||grad|| = 0.0022312039509415627\n","Iteration 402: ||grad|| = 0.0021865030284971\n","Iteration 403: ||grad|| = 0.002142860321328044\n","Iteration 404: ||grad|| = 0.002099878853186965\n","Iteration 405: ||grad|| = 0.0020579553674906492\n","Iteration 406: ||grad|| = 0.0020166931208223104\n","Iteration 407: ||grad|| = 0.001976488856598735\n","Iteration 408: ||grad|| = 0.001936945947818458\n","Iteration 409: ||grad|| = 0.0018980641616508365\n","Iteration 410: ||grad|| = 0.001860240357927978\n","Iteration 411: ||grad|| = 0.001822945661842823\n","Iteration 412: ||grad|| = 0.001786312204785645\n","Iteration 413: ||grad|| = 0.0017507367301732302\n","Iteration 414: ||grad|| = 0.0017158224945887923\n","Iteration 415: ||grad|| = 0.001681437250226736\n","Iteration 416: ||grad|| = 0.0016477132448926568\n","Iteration 417: ||grad|| = 0.001614518347196281\n","Iteration 418: ||grad|| = 0.0015823813155293465\n","Iteration 419: ||grad|| = 0.0015507735079154372\n","Iteration 420: ||grad|| = 0.0015198268229141831\n","Iteration 421: ||grad|| = 0.0014894091291353106\n","Iteration 422: ||grad|| = 0.0014596526743844151\n","Iteration 423: ||grad|| = 0.0014304252108559012\n","Iteration 424: ||grad|| = 0.0014017268549650908\n","Iteration 425: ||grad|| = 0.0013736896216869354\n","Iteration 426: ||grad|| = 0.0013461814960464835\n","Iteration 427: ||grad|| = 0.0013192023616284132\n","Iteration 428: ||grad|| = 0.001292884349822998\n","Iteration 429: ||grad|| = 0.0012670954456552863\n","Iteration 430: ||grad|| = 0.001241835649125278\n","Iteration 431: ||grad|| = 0.0012171048438176513\n","Iteration 432: ||grad|| = 0.0011925061699002981\n","Iteration 433: ||grad|| = 0.0011685686185956001\n","Iteration 434: ||grad|| = 0.0011451601749286056\n","Iteration 435: ||grad|| = 0.0011222807224839926\n","Iteration 436: ||grad|| = 0.0010999302612617612\n","Iteration 437: ||grad|| = 0.001078109024092555\n","Iteration 438: ||grad|| = 0.0010564198018983006\n","Iteration 439: ||grad|| = 0.0010352595709264278\n","Iteration 440: ||grad|| = 0.0010146284475922585\n","Iteration 441: ||grad|| = 0.0009945264318957925\n","Iteration 442: ||grad|| = 0.0009745564893819392\n","Iteration 443: ||grad|| = 0.0009551155962981284\n","Iteration 444: ||grad|| = 0.0009362036944366992\n","Iteration 445: ||grad|| = 0.0009172918507829309\n","Iteration 446: ||grad|| = 0.0008989089983515441\n","Iteration 447: ||grad|| = 0.0008810551953501999\n","Iteration 448: ||grad|| = 0.0008633335819467902\n","Iteration 449: ||grad|| = 0.0008461409597657621\n","Iteration 450: ||grad|| = 0.0008290805271826684\n","Iteration 451: ||grad|| = 0.0008124169544316828\n","Iteration 452: ||grad|| = 0.0007962823729030788\n","Iteration 453: ||grad|| = 0.0007802800391800702\n","Iteration 454: ||grad|| = 0.0007648066966794431\n","Iteration 455: ||grad|| = 0.0007493332959711552\n","Iteration 456: ||grad|| = 0.0007343890029005706\n","Iteration 457: ||grad|| = 0.0007195768412202597\n","Iteration 458: ||grad|| = 0.0007052937289699912\n","Iteration 459: ||grad|| = 0.0006910106167197227\n","Iteration 460: ||grad|| = 0.0006772565538994968\n","Iteration 461: ||grad|| = 0.0006636346806772053\n","Iteration 462: ||grad|| = 0.000650409609079361\n","Iteration 463: ||grad|| = 0.000637316785287112\n","Iteration 464: ||grad|| = 0.0006246207049116492\n","Iteration 465: ||grad|| = 0.0006120568723417819\n","Iteration 466: ||grad|| = 0.0006000220309942961\n","Iteration 467: ||grad|| = 0.0005879871896468103\n","Iteration 468: ||grad|| = 0.0005760846543125808\n","Iteration 469: ||grad|| = 0.0005645788041874766\n","Iteration 470: ||grad|| = 0.0005532053182832897\n","Iteration 471: ||grad|| = 0.0005422284011729062\n","Iteration 472: ||grad|| = 0.0005313839064911008\n","Iteration 473: ||grad|| = 0.0005205393536016345\n","Iteration 474: ||grad|| = 0.0005102237919345498\n","Iteration 475: ||grad|| = 0.0004999082302674651\n","Iteration 476: ||grad|| = 0.0004901216016151011\n","Iteration 477: ||grad|| = 0.000480335031170398\n","Iteration 478: ||grad|| = 0.00047068079584278166\n","Iteration 479: ||grad|| = 0.0004614231875166297\n","Iteration 480: ||grad|| = 0.00045229788520373404\n","Iteration 481: ||grad|| = 0.00044317261199466884\n","Iteration 482: ||grad|| = 0.00043444399489089847\n","Iteration 483: ||grad|| = 0.0004258476838003844\n","Iteration 484: ||grad|| = 0.00041725137270987034\n","Iteration 485: ||grad|| = 0.00040878739673644304\n","Iteration 486: ||grad|| = 0.00040072007686831057\n","Iteration 487: ||grad|| = 0.0003926527570001781\n","Iteration 488: ||grad|| = 0.0003847177722491324\n","Iteration 489: ||grad|| = 0.0003771793853957206\n","Iteration 490: ||grad|| = 0.0003696410858538002\n","Iteration 491: ||grad|| = 0.00036223503411747515\n","Iteration 492: ||grad|| = 0.0003548290114849806\n","Iteration 493: ||grad|| = 0.00034781970316544175\n","Iteration 494: ||grad|| = 0.0003409426426514983\n","Iteration 495: ||grad|| = 0.00033406561124138534\n","Iteration 496: ||grad|| = 0.00032718857983127236\n","Iteration 497: ||grad|| = 0.000320840539643541\n","Iteration 498: ||grad|| = 0.00031449252855964005\n","Iteration 499: ||grad|| = 0.0003081445465795696\n","Iteration 500: ||grad|| = 0.0003019286668859422\n","Iteration 501: ||grad|| = 0.00029571287450380623\n","Iteration 502: ||grad|| = 0.0002898938546422869\n","Iteration 503: ||grad|| = 0.0002842070534825325\n","Iteration 504: ||grad|| = 0.0002785202523227781\n","Iteration 505: ||grad|| = 0.00027283348026685417\n","Iteration 506: ||grad|| = 0.00026714670821093023\n","Iteration 507: ||grad|| = 0.0002619889273773879\n","Iteration 508: ||grad|| = 0.00025683114654384553\n","Iteration 509: ||grad|| = 0.0002516733657103032\n","Iteration 510: ||grad|| = 0.00024651558487676084\n","Iteration 511: ||grad|| = 0.00024149024102371186\n","Iteration 512: ||grad|| = 0.0002368613932048902\n","Iteration 513: ||grad|| = 0.00023223254538606852\n","Iteration 514: ||grad|| = 0.0002276037266710773\n","Iteration 515: ||grad|| = 0.0002229749079560861\n","Iteration 516: ||grad|| = 0.00021847846801392734\n","Iteration 517: ||grad|| = 0.00021398210083134472\n","Iteration 518: ||grad|| = 0.0002098821714753285\n","Iteration 519: ||grad|| = 0.0002057823003269732\n","Iteration 520: ||grad|| = 0.0002016825310420245\n","Iteration 521: ||grad|| = 0.00019771499501075596\n","Iteration 522: ||grad|| = 0.00019374748808331788\n","Iteration 523: ||grad|| = 0.00018977999570779502\n","Iteration 524: ||grad|| = 0.00018581253243610263\n","Iteration 525: ||grad|| = 0.00018224165251012892\n","Iteration 526: ||grad|| = 0.0001788031222531572\n","Iteration 527: ||grad|| = 0.0001753646065481007\n","Iteration 528: ||grad|| = 0.000171926076291129\n","Iteration 529: ||grad|| = 0.0001684875605860725\n","Iteration 530: ||grad|| = 0.00016504904488101602\n","Iteration 531: ||grad|| = 0.0001616105146240443\n","Iteration 532: ||grad|| = 0.0001581719989189878\n","Iteration 533: ||grad|| = 0.00015486584743484855\n","Iteration 534: ||grad|| = 0.00015195626474451274\n","Iteration 535: ||grad|| = 0.00014904669660609215\n","Iteration 536: ||grad|| = 0.00014613717212341726\n","Iteration 537: ||grad|| = 0.0001432276621926576\n","Iteration 538: ||grad|| = 0.0001403181959176436\n","Iteration 539: ||grad|| = 0.00013740875874646008\n","Iteration 540: ||grad|| = 0.00013463136565405875\n","Iteration 541: ||grad|| = 0.00013185410352889448\n","Iteration 542: ||grad|| = 0.00012907695781905204\n","Iteration 543: ||grad|| = 0.0001262999721802771\n","Iteration 544: ||grad|| = 0.00012391919153742492\n","Iteration 545: ||grad|| = 0.00012153853458585218\n","Iteration 546: ||grad|| = 0.0001191580158774741\n","Iteration 547: ||grad|| = 0.00011677765724016353\n","Iteration 548: ||grad|| = 0.00011452929902588949\n","Iteration 549: ||grad|| = 0.00011228097719140351\n","Iteration 550: ||grad|| = 0.0001100326917367056\n","Iteration 551: ||grad|| = 0.00010778444993775338\n","Iteration 552: ||grad|| = 0.00010553624451858923\n","Iteration 553: ||grad|| = 0.00010328809003112838\n","Iteration 554: ||grad|| = 0.00010103997919941321\n","Iteration 555: ||grad|| = 9.879192657535896e-05\n","Iteration 556: ||grad|| = 9.693994797999039e-05\n","Iteration 557: ||grad|| = 9.508836956229061e-05\n","Iteration 558: ||grad|| = 9.336911170976236e-05\n","Iteration 559: ||grad|| = 9.164985385723412e-05\n","Iteration 560: ||grad|| = 8.993058872874826e-05\n","Iteration 561: ||grad|| = 8.821133087622002e-05\n","Iteration 562: ||grad|| = 8.649207302369177e-05\n","Iteration 563: ||grad|| = 8.477280789520591e-05\n","Iteration 564: ||grad|| = 8.305355004267767e-05\n","Iteration 565: ||grad|| = 8.133429219014943e-05\n","Iteration 566: ||grad|| = 7.961502706166357e-05\n","Iteration 567: ||grad|| = 7.789576920913532e-05\n","Iteration 568: ||grad|| = 7.617651135660708e-05\n","Iteration 569: ||grad|| = 7.445724622812122e-05\n","Iteration 570: ||grad|| = 7.287033076863736e-05\n","Iteration 571: ||grad|| = 7.128396828193218e-05\n","Iteration 572: ||grad|| = 7.009343971731141e-05\n","Iteration 573: ||grad|| = 6.890296936035156e-05\n","Iteration 574: ||grad|| = 6.771255721105263e-05\n","Iteration 575: ||grad|| = 6.65222032694146e-05\n","Iteration 576: ||grad|| = 6.533191481139511e-05\n","Iteration 577: ||grad|| = 6.414168456103653e-05\n","Iteration 578: ||grad|| = 6.29515343462117e-05\n","Iteration 579: ||grad|| = 6.176145689096302e-05\n","Iteration 580: ||grad|| = 6.057145947124809e-05\n","Iteration 581: ||grad|| = 5.9381545725045726e-05\n","Iteration 582: ||grad|| = 5.819171929033473e-05\n","Iteration 583: ||grad|| = 5.700198744307272e-05\n","Iteration 584: ||grad|| = 5.581235745921731e-05\n","Iteration 585: ||grad|| = 5.462283661472611e-05\n","Iteration 586: ||grad|| = 5.343342854757793e-05\n","Iteration 587: ||grad|| = 5.2374001825228333e-05\n","Iteration 588: ||grad|| = 5.131485522724688e-05\n","Iteration 589: ||grad|| = 5.025601421948522e-05\n","Iteration 590: ||grad|| = 4.9197486077900976e-05\n","Iteration 591: ||grad|| = 4.81393035443034e-05\n","Iteration 592: ||grad|| = 4.708148117060773e-05\n","Iteration 593: ||grad|| = 4.602404806064442e-05\n","Iteration 594: ||grad|| = 4.496703331824392e-05\n","Iteration 595: ||grad|| = 4.391046240925789e-05\n","Iteration 596: ||grad|| = 4.285437171347439e-05\n","Iteration 597: ||grad|| = 4.219133188598789e-05\n","Iteration 598: ||grad|| = 4.15286558563821e-05\n","Iteration 599: ||grad|| = 4.0866361814551055e-05\n","Iteration 600: ||grad|| = 4.0204471588367596e-05\n","Iteration 601: ||grad|| = 3.954299972974695e-05\n","Iteration 602: ||grad|| = 3.888196806656197e-05\n","Iteration 603: ||grad|| = 3.8221405702643096e-05\n","Iteration 604: ||grad|| = 3.7561330827884376e-05\n","Iteration 605: ||grad|| = 3.690177254611626e-05\n","Iteration 606: ||grad|| = 3.624275996116921e-05\n","Iteration 607: ||grad|| = 3.5584322176873684e-05\n","Iteration 608: ||grad|| = 3.492649193503894e-05\n","Iteration 609: ||grad|| = 3.426930197747424e-05\n","Iteration 610: ||grad|| = 3.361279232194647e-05\n","Iteration 611: ||grad|| = 3.30818111251574e-05\n","Iteration 612: ||grad|| = 3.255090268794447e-05\n","Iteration 613: ||grad|| = 3.2020067010307685e-05\n","Iteration 614: ||grad|| = 3.1489307730225846e-05\n","Iteration 615: ||grad|| = 3.095863212365657e-05\n","Iteration 616: ||grad|| = 3.042804019059986e-05\n","Iteration 617: ||grad|| = 2.989753738802392e-05\n","Iteration 618: ||grad|| = 2.936713099188637e-05\n","Iteration 619: ||grad|| = 2.883682282117661e-05\n","Iteration 620: ||grad|| = 2.830662197084166e-05\n","Iteration 621: ||grad|| = 2.7776532078860328e-05\n","Iteration 622: ||grad|| = 2.7246560421190225e-05\n","Iteration 623: ||grad|| = 2.6716714273788966e-05\n","Iteration 624: ||grad|| = 2.6187000912614167e-05\n","Iteration 625: ||grad|| = 2.565742761362344e-05\n","Iteration 626: ||grad|| = 2.512800710974261e-05\n","Iteration 627: ||grad|| = 2.4598743038950488e-05\n","Iteration 628: ||grad|| = 2.40696517721517e-05\n","Iteration 629: ||grad|| = 2.3540740585303865e-05\n","Iteration 630: ||grad|| = 2.301202403032221e-05\n","Iteration 631: ||grad|| = 2.248351665912196e-05\n","Iteration 632: ||grad|| = 2.1955231204628944e-05\n","Iteration 633: ||grad|| = 2.1427185856737196e-05\n","Iteration 634: ||grad|| = 2.089939698635135e-05\n","Iteration 635: ||grad|| = 2.0371888240333647e-05\n","Iteration 636: ||grad|| = 1.9844677808578126e-05\n","Iteration 637: ||grad|| = 1.9317791156936437e-05\n","Iteration 638: ||grad|| = 1.8791255570249632e-05\n","Iteration 639: ||grad|| = 1.8265103790326975e-05\n","Iteration 640: ||grad|| = 1.773936855897773e-05\n","Iteration 641: ||grad|| = 1.721408625598997e-05\n","Iteration 642: ||grad|| = 1.6689300537109375e-05\n","Iteration 643: ||grad|| = 1.6165060515049845e-05\n","Iteration 644: ||grad|| = 1.564142257848289e-05\n","Iteration 645: ||grad|| = 1.511844493506942e-05\n","Iteration 646: ||grad|| = 1.4596203072869685e-05\n","Iteration 647: ||grad|| = 1.4449425179918762e-05\n","Iteration 648: ||grad|| = 1.430511474609375e-05\n","Iteration 649: ||grad|| = 1.4163348168949597e-05\n","Iteration 650: ||grad|| = 1.4024201846041251e-05\n","Iteration 651: ||grad|| = 1.3887753993913066e-05\n","Iteration 652: ||grad|| = 1.37540864670882e-05\n","Iteration 653: ||grad|| = 1.3623280210595112e-05\n","Iteration 654: ||grad|| = 1.3495417988451663e-05\n","Iteration 655: ||grad|| = 1.337058529315982e-05\n","Iteration 656: ||grad|| = 1.3248866707726847e-05\n","Iteration 657: ||grad|| = 1.3130349543644115e-05\n","Iteration 658: ||grad|| = 1.3015121112402994e-05\n","Iteration 659: ||grad|| = 1.2903269634989556e-05\n","Iteration 660: ||grad|| = 1.2794883332389873e-05\n","Iteration 661: ||grad|| = 1.2794883332389873e-05\n","Iteration 662: ||grad|| = 1.2794883332389873e-05\n","Iteration 663: ||grad|| = 1.2794883332389873e-05\n","Iteration 664: ||grad|| = 1.2794883332389873e-05\n","Iteration 665: ||grad|| = 1.2794883332389873e-05\n","Iteration 666: ||grad|| = 1.2794883332389873e-05\n","Iteration 667: ||grad|| = 1.2794883332389873e-05\n","Iteration 668: ||grad|| = 1.2794883332389873e-05\n","Iteration 669: ||grad|| = 1.2794883332389873e-05\n","Iteration 670: ||grad|| = 1.2794883332389873e-05\n","Iteration 671: ||grad|| = 1.2794883332389873e-05\n","Iteration 672: ||grad|| = 1.2794883332389873e-05\n","Iteration 673: ||grad|| = 1.2794883332389873e-05\n","Iteration 674: ||grad|| = 1.2794883332389873e-05\n","Iteration 675: ||grad|| = 1.2794883332389873e-05\n","Iteration 676: ||grad|| = 1.2794883332389873e-05\n","Iteration 677: ||grad|| = 1.2794883332389873e-05\n","Iteration 678: ||grad|| = 1.2794883332389873e-05\n","Iteration 679: ||grad|| = 1.2794883332389873e-05\n","Iteration 680: ||grad|| = 1.2794883332389873e-05\n","Iteration 681: ||grad|| = 1.2794883332389873e-05\n","Iteration 682: ||grad|| = 1.2794883332389873e-05\n","Iteration 683: ||grad|| = 1.2794883332389873e-05\n","Iteration 684: ||grad|| = 1.2794883332389873e-05\n","Iteration 685: ||grad|| = 1.2794883332389873e-05\n","Iteration 686: ||grad|| = 1.2794883332389873e-05\n","Iteration 687: ||grad|| = 1.2794883332389873e-05\n","Iteration 688: ||grad|| = 1.2794883332389873e-05\n","Iteration 689: ||grad|| = 1.2794883332389873e-05\n","Iteration 690: ||grad|| = 1.2794883332389873e-05\n","Iteration 691: ||grad|| = 1.2794883332389873e-05\n","Iteration 692: ||grad|| = 1.2794883332389873e-05\n","Iteration 693: ||grad|| = 1.2794883332389873e-05\n","Iteration 694: ||grad|| = 1.2794883332389873e-05\n","Iteration 695: ||grad|| = 1.2794883332389873e-05\n","Iteration 696: ||grad|| = 1.2794883332389873e-05\n","Iteration 697: ||grad|| = 1.2794883332389873e-05\n","Iteration 698: ||grad|| = 1.2794883332389873e-05\n","Iteration 699: ||grad|| = 1.2794883332389873e-05\n","Iteration 700: ||grad|| = 1.2794883332389873e-05\n","Iteration 701: ||grad|| = 1.2794883332389873e-05\n","Iteration 702: ||grad|| = 1.2794883332389873e-05\n","Iteration 703: ||grad|| = 1.2794883332389873e-05\n","Iteration 704: ||grad|| = 1.2794883332389873e-05\n","Iteration 705: ||grad|| = 1.2794883332389873e-05\n","Iteration 706: ||grad|| = 1.2794883332389873e-05\n","Iteration 707: ||grad|| = 1.2794883332389873e-05\n","Iteration 708: ||grad|| = 1.2794883332389873e-05\n","Iteration 709: ||grad|| = 1.2794883332389873e-05\n","Iteration 710: ||grad|| = 1.2794883332389873e-05\n","Iteration 711: ||grad|| = 1.2794883332389873e-05\n","Iteration 712: ||grad|| = 1.2794883332389873e-05\n","Iteration 713: ||grad|| = 1.2794883332389873e-05\n","Iteration 714: ||grad|| = 1.2794883332389873e-05\n","Iteration 715: ||grad|| = 1.2794883332389873e-05\n","Iteration 716: ||grad|| = 1.2794883332389873e-05\n","Iteration 717: ||grad|| = 1.2794883332389873e-05\n","Iteration 718: ||grad|| = 1.2794883332389873e-05\n","Iteration 719: ||grad|| = 1.2794883332389873e-05\n","Iteration 720: ||grad|| = 1.2794883332389873e-05\n","Iteration 721: ||grad|| = 1.2794883332389873e-05\n","Iteration 722: ||grad|| = 1.2794883332389873e-05\n","Iteration 723: ||grad|| = 1.2794883332389873e-05\n","Iteration 724: ||grad|| = 1.2794883332389873e-05\n","Iteration 725: ||grad|| = 1.2794883332389873e-05\n","Iteration 726: ||grad|| = 1.2794883332389873e-05\n","Iteration 727: ||grad|| = 1.2794883332389873e-05\n","Iteration 728: ||grad|| = 1.2794883332389873e-05\n","Iteration 729: ||grad|| = 1.2794883332389873e-05\n","Iteration 730: ||grad|| = 1.2794883332389873e-05\n","Iteration 731: ||grad|| = 1.2794883332389873e-05\n","Iteration 732: ||grad|| = 1.2794883332389873e-05\n","Iteration 733: ||grad|| = 1.2794883332389873e-05\n","Iteration 734: ||grad|| = 1.2794883332389873e-05\n","Iteration 735: ||grad|| = 1.2794883332389873e-05\n","Iteration 736: ||grad|| = 1.2794883332389873e-05\n","Iteration 737: ||grad|| = 1.2794883332389873e-05\n","Iteration 738: ||grad|| = 1.2794883332389873e-05\n","Iteration 739: ||grad|| = 1.2794883332389873e-05\n","Iteration 740: ||grad|| = 1.2794883332389873e-05\n","Iteration 741: ||grad|| = 1.2794883332389873e-05\n","Iteration 742: ||grad|| = 1.2794883332389873e-05\n","Iteration 743: ||grad|| = 1.2794883332389873e-05\n","Iteration 744: ||grad|| = 1.2794883332389873e-05\n","Iteration 745: ||grad|| = 1.2794883332389873e-05\n","Iteration 746: ||grad|| = 1.2794883332389873e-05\n","Iteration 747: ||grad|| = 1.2794883332389873e-05\n","Iteration 748: ||grad|| = 1.2794883332389873e-05\n","Iteration 749: ||grad|| = 1.2794883332389873e-05\n","Iteration 750: ||grad|| = 1.2794883332389873e-05\n","Iteration 751: ||grad|| = 1.2794883332389873e-05\n","Iteration 752: ||grad|| = 1.2794883332389873e-05\n","Iteration 753: ||grad|| = 1.2794883332389873e-05\n","Iteration 754: ||grad|| = 1.2794883332389873e-05\n","Iteration 755: ||grad|| = 1.2794883332389873e-05\n","Iteration 756: ||grad|| = 1.2794883332389873e-05\n","Iteration 757: ||grad|| = 1.2794883332389873e-05\n","Iteration 758: ||grad|| = 1.2794883332389873e-05\n","Iteration 759: ||grad|| = 1.2794883332389873e-05\n","Iteration 760: ||grad|| = 1.2794883332389873e-05\n","Iteration 761: ||grad|| = 1.2794883332389873e-05\n","Iteration 762: ||grad|| = 1.2794883332389873e-05\n","Iteration 763: ||grad|| = 1.2794883332389873e-05\n","Iteration 764: ||grad|| = 1.2794883332389873e-05\n","Iteration 765: ||grad|| = 1.2794883332389873e-05\n","Iteration 766: ||grad|| = 1.2794883332389873e-05\n","Iteration 767: ||grad|| = 1.2794883332389873e-05\n","Iteration 768: ||grad|| = 1.2794883332389873e-05\n","Iteration 769: ||grad|| = 1.2794883332389873e-05\n","Iteration 770: ||grad|| = 1.2794883332389873e-05\n","Iteration 771: ||grad|| = 1.2794883332389873e-05\n","Iteration 772: ||grad|| = 1.2794883332389873e-05\n","Iteration 773: ||grad|| = 1.2794883332389873e-05\n","Iteration 774: ||grad|| = 1.2794883332389873e-05\n","Iteration 775: ||grad|| = 1.2794883332389873e-05\n","Iteration 776: ||grad|| = 1.2794883332389873e-05\n","Iteration 777: ||grad|| = 1.2794883332389873e-05\n","Iteration 778: ||grad|| = 1.2794883332389873e-05\n","Iteration 779: ||grad|| = 1.2794883332389873e-05\n","Iteration 780: ||grad|| = 1.2794883332389873e-05\n","Iteration 781: ||grad|| = 1.2794883332389873e-05\n","Iteration 782: ||grad|| = 1.2794883332389873e-05\n","Iteration 783: ||grad|| = 1.2794883332389873e-05\n","Iteration 784: ||grad|| = 1.2794883332389873e-05\n","Iteration 785: ||grad|| = 1.2794883332389873e-05\n","Iteration 786: ||grad|| = 1.2794883332389873e-05\n","Iteration 787: ||grad|| = 1.2794883332389873e-05\n","Iteration 788: ||grad|| = 1.2794883332389873e-05\n","Iteration 789: ||grad|| = 1.2794883332389873e-05\n","Iteration 790: ||grad|| = 1.2794883332389873e-05\n","Iteration 791: ||grad|| = 1.2794883332389873e-05\n","Iteration 792: ||grad|| = 1.2794883332389873e-05\n","Iteration 793: ||grad|| = 1.2794883332389873e-05\n","Iteration 794: ||grad|| = 1.2794883332389873e-05\n","Iteration 795: ||grad|| = 1.2794883332389873e-05\n","Iteration 796: ||grad|| = 1.2794883332389873e-05\n","Iteration 797: ||grad|| = 1.2794883332389873e-05\n","Iteration 798: ||grad|| = 1.2794883332389873e-05\n","Iteration 799: ||grad|| = 1.2794883332389873e-05\n","Iteration 800: ||grad|| = 1.2794883332389873e-05\n","Iteration 801: ||grad|| = 1.2794883332389873e-05\n","Iteration 802: ||grad|| = 1.2794883332389873e-05\n","Iteration 803: ||grad|| = 1.2794883332389873e-05\n","Iteration 804: ||grad|| = 1.2794883332389873e-05\n","Iteration 805: ||grad|| = 1.2794883332389873e-05\n","Iteration 806: ||grad|| = 1.2794883332389873e-05\n","Iteration 807: ||grad|| = 1.2794883332389873e-05\n","Iteration 808: ||grad|| = 1.2794883332389873e-05\n","Iteration 809: ||grad|| = 1.2794883332389873e-05\n","Iteration 810: ||grad|| = 1.2794883332389873e-05\n","Iteration 811: ||grad|| = 1.2794883332389873e-05\n","Iteration 812: ||grad|| = 1.2794883332389873e-05\n","Iteration 813: ||grad|| = 1.2794883332389873e-05\n","Iteration 814: ||grad|| = 1.2794883332389873e-05\n","Iteration 815: ||grad|| = 1.2794883332389873e-05\n","Iteration 816: ||grad|| = 1.2794883332389873e-05\n","Iteration 817: ||grad|| = 1.2794883332389873e-05\n","Iteration 818: ||grad|| = 1.2794883332389873e-05\n","Iteration 819: ||grad|| = 1.2794883332389873e-05\n","Iteration 820: ||grad|| = 1.2794883332389873e-05\n","Iteration 821: ||grad|| = 1.2794883332389873e-05\n","Iteration 822: ||grad|| = 1.2794883332389873e-05\n","Iteration 823: ||grad|| = 1.2794883332389873e-05\n","Iteration 824: ||grad|| = 1.2794883332389873e-05\n","Iteration 825: ||grad|| = 1.2794883332389873e-05\n","Iteration 826: ||grad|| = 1.2794883332389873e-05\n","Iteration 827: ||grad|| = 1.2794883332389873e-05\n","Iteration 828: ||grad|| = 1.2794883332389873e-05\n","Iteration 829: ||grad|| = 1.2794883332389873e-05\n","Iteration 830: ||grad|| = 1.2794883332389873e-05\n","Iteration 831: ||grad|| = 1.2794883332389873e-05\n","Iteration 832: ||grad|| = 1.2794883332389873e-05\n","Iteration 833: ||grad|| = 1.2794883332389873e-05\n","Iteration 834: ||grad|| = 1.2794883332389873e-05\n","Iteration 835: ||grad|| = 1.2794883332389873e-05\n","Iteration 836: ||grad|| = 1.2794883332389873e-05\n","Iteration 837: ||grad|| = 1.2794883332389873e-05\n","Iteration 838: ||grad|| = 1.2794883332389873e-05\n","Iteration 839: ||grad|| = 1.2794883332389873e-05\n","Iteration 840: ||grad|| = 1.2794883332389873e-05\n","Iteration 841: ||grad|| = 1.2794883332389873e-05\n","Iteration 842: ||grad|| = 1.2794883332389873e-05\n","Iteration 843: ||grad|| = 1.2794883332389873e-05\n","Iteration 844: ||grad|| = 1.2794883332389873e-05\n","Iteration 845: ||grad|| = 1.2794883332389873e-05\n","Iteration 846: ||grad|| = 1.2794883332389873e-05\n","Iteration 847: ||grad|| = 1.2794883332389873e-05\n","Iteration 848: ||grad|| = 1.2794883332389873e-05\n","Iteration 849: ||grad|| = 1.2794883332389873e-05\n","Iteration 850: ||grad|| = 1.2794883332389873e-05\n","Iteration 851: ||grad|| = 1.2794883332389873e-05\n","Iteration 852: ||grad|| = 1.2794883332389873e-05\n","Iteration 853: ||grad|| = 1.2794883332389873e-05\n","Iteration 854: ||grad|| = 1.2794883332389873e-05\n","Iteration 855: ||grad|| = 1.2794883332389873e-05\n","Iteration 856: ||grad|| = 1.2794883332389873e-05\n","Iteration 857: ||grad|| = 1.2794883332389873e-05\n","Iteration 858: ||grad|| = 1.2794883332389873e-05\n","Iteration 859: ||grad|| = 1.2794883332389873e-05\n","Iteration 860: ||grad|| = 1.2794883332389873e-05\n","Iteration 861: ||grad|| = 1.2794883332389873e-05\n","Iteration 862: ||grad|| = 1.2794883332389873e-05\n","Iteration 863: ||grad|| = 1.2794883332389873e-05\n","Iteration 864: ||grad|| = 1.2794883332389873e-05\n","Iteration 865: ||grad|| = 1.2794883332389873e-05\n","Iteration 866: ||grad|| = 1.2794883332389873e-05\n","Iteration 867: ||grad|| = 1.2794883332389873e-05\n","Iteration 868: ||grad|| = 1.2794883332389873e-05\n","Iteration 869: ||grad|| = 1.2794883332389873e-05\n","Iteration 870: ||grad|| = 1.2794883332389873e-05\n","Iteration 871: ||grad|| = 1.2794883332389873e-05\n","Iteration 872: ||grad|| = 1.2794883332389873e-05\n","Iteration 873: ||grad|| = 1.2794883332389873e-05\n","Iteration 874: ||grad|| = 1.2794883332389873e-05\n","Iteration 875: ||grad|| = 1.2794883332389873e-05\n","Iteration 876: ||grad|| = 1.2794883332389873e-05\n","Iteration 877: ||grad|| = 1.2794883332389873e-05\n","Iteration 878: ||grad|| = 1.2794883332389873e-05\n","Iteration 879: ||grad|| = 1.2794883332389873e-05\n","Iteration 880: ||grad|| = 1.2794883332389873e-05\n","Iteration 881: ||grad|| = 1.2794883332389873e-05\n","Iteration 882: ||grad|| = 1.2794883332389873e-05\n","Iteration 883: ||grad|| = 1.2794883332389873e-05\n","Iteration 884: ||grad|| = 1.2794883332389873e-05\n","Iteration 885: ||grad|| = 1.2794883332389873e-05\n","Iteration 886: ||grad|| = 1.2794883332389873e-05\n","Iteration 887: ||grad|| = 1.2794883332389873e-05\n","Iteration 888: ||grad|| = 1.2794883332389873e-05\n","Iteration 889: ||grad|| = 1.2794883332389873e-05\n","Iteration 890: ||grad|| = 1.2794883332389873e-05\n","Iteration 891: ||grad|| = 1.2794883332389873e-05\n","Iteration 892: ||grad|| = 1.2794883332389873e-05\n","Iteration 893: ||grad|| = 1.2794883332389873e-05\n","Iteration 894: ||grad|| = 1.2794883332389873e-05\n","Iteration 895: ||grad|| = 1.2794883332389873e-05\n","Iteration 896: ||grad|| = 1.2794883332389873e-05\n","Iteration 897: ||grad|| = 1.2794883332389873e-05\n","Iteration 898: ||grad|| = 1.2794883332389873e-05\n","Iteration 899: ||grad|| = 1.2794883332389873e-05\n","Iteration 900: ||grad|| = 1.2794883332389873e-05\n","Iteration 901: ||grad|| = 1.2794883332389873e-05\n","Iteration 902: ||grad|| = 1.2794883332389873e-05\n","Iteration 903: ||grad|| = 1.2794883332389873e-05\n","Iteration 904: ||grad|| = 1.2794883332389873e-05\n","Iteration 905: ||grad|| = 1.2794883332389873e-05\n","Iteration 906: ||grad|| = 1.2794883332389873e-05\n","Iteration 907: ||grad|| = 1.2794883332389873e-05\n","Iteration 908: ||grad|| = 1.2794883332389873e-05\n","Iteration 909: ||grad|| = 1.2794883332389873e-05\n","Iteration 910: ||grad|| = 1.2794883332389873e-05\n","Iteration 911: ||grad|| = 1.2794883332389873e-05\n","Iteration 912: ||grad|| = 1.2794883332389873e-05\n","Iteration 913: ||grad|| = 1.2794883332389873e-05\n","Iteration 914: ||grad|| = 1.2794883332389873e-05\n","Iteration 915: ||grad|| = 1.2794883332389873e-05\n","Iteration 916: ||grad|| = 1.2794883332389873e-05\n","Iteration 917: ||grad|| = 1.2794883332389873e-05\n","Iteration 918: ||grad|| = 1.2794883332389873e-05\n","Iteration 919: ||grad|| = 1.2794883332389873e-05\n","Iteration 920: ||grad|| = 1.2794883332389873e-05\n","Iteration 921: ||grad|| = 1.2794883332389873e-05\n","Iteration 922: ||grad|| = 1.2794883332389873e-05\n","Iteration 923: ||grad|| = 1.2794883332389873e-05\n","Iteration 924: ||grad|| = 1.2794883332389873e-05\n","Iteration 925: ||grad|| = 1.2794883332389873e-05\n","Iteration 926: ||grad|| = 1.2794883332389873e-05\n","Iteration 927: ||grad|| = 1.2794883332389873e-05\n","Iteration 928: ||grad|| = 1.2794883332389873e-05\n","Iteration 929: ||grad|| = 1.2794883332389873e-05\n","Iteration 930: ||grad|| = 1.2794883332389873e-05\n","Iteration 931: ||grad|| = 1.2794883332389873e-05\n","Iteration 932: ||grad|| = 1.2794883332389873e-05\n","Iteration 933: ||grad|| = 1.2794883332389873e-05\n","Iteration 934: ||grad|| = 1.2794883332389873e-05\n","Iteration 935: ||grad|| = 1.2794883332389873e-05\n","Iteration 936: ||grad|| = 1.2794883332389873e-05\n","Iteration 937: ||grad|| = 1.2794883332389873e-05\n","Iteration 938: ||grad|| = 1.2794883332389873e-05\n","Iteration 939: ||grad|| = 1.2794883332389873e-05\n","Iteration 940: ||grad|| = 1.2794883332389873e-05\n","Iteration 941: ||grad|| = 1.2794883332389873e-05\n","Iteration 942: ||grad|| = 1.2794883332389873e-05\n","Iteration 943: ||grad|| = 1.2794883332389873e-05\n","Iteration 944: ||grad|| = 1.2794883332389873e-05\n","Iteration 945: ||grad|| = 1.2794883332389873e-05\n","Iteration 946: ||grad|| = 1.2794883332389873e-05\n","Iteration 947: ||grad|| = 1.2794883332389873e-05\n","Iteration 948: ||grad|| = 1.2794883332389873e-05\n","Iteration 949: ||grad|| = 1.2794883332389873e-05\n","Iteration 950: ||grad|| = 1.2794883332389873e-05\n","Iteration 951: ||grad|| = 1.2794883332389873e-05\n","Iteration 952: ||grad|| = 1.2794883332389873e-05\n","Iteration 953: ||grad|| = 1.2794883332389873e-05\n","Iteration 954: ||grad|| = 1.2794883332389873e-05\n","Iteration 955: ||grad|| = 1.2794883332389873e-05\n","Iteration 956: ||grad|| = 1.2794883332389873e-05\n","Iteration 957: ||grad|| = 1.2794883332389873e-05\n","Iteration 958: ||grad|| = 1.2794883332389873e-05\n","Iteration 959: ||grad|| = 1.2794883332389873e-05\n","Iteration 960: ||grad|| = 1.2794883332389873e-05\n","Iteration 961: ||grad|| = 1.2794883332389873e-05\n","Iteration 962: ||grad|| = 1.2794883332389873e-05\n","Iteration 963: ||grad|| = 1.2794883332389873e-05\n","Iteration 964: ||grad|| = 1.2794883332389873e-05\n","Iteration 965: ||grad|| = 1.2794883332389873e-05\n","Iteration 966: ||grad|| = 1.2794883332389873e-05\n","Iteration 967: ||grad|| = 1.2794883332389873e-05\n","Iteration 968: ||grad|| = 1.2794883332389873e-05\n","Iteration 969: ||grad|| = 1.2794883332389873e-05\n","Iteration 970: ||grad|| = 1.2794883332389873e-05\n","Iteration 971: ||grad|| = 1.2794883332389873e-05\n","Iteration 972: ||grad|| = 1.2794883332389873e-05\n","Iteration 973: ||grad|| = 1.2794883332389873e-05\n","Iteration 974: ||grad|| = 1.2794883332389873e-05\n","Iteration 975: ||grad|| = 1.2794883332389873e-05\n","Iteration 976: ||grad|| = 1.2794883332389873e-05\n","Iteration 977: ||grad|| = 1.2794883332389873e-05\n","Iteration 978: ||grad|| = 1.2794883332389873e-05\n","Iteration 979: ||grad|| = 1.2794883332389873e-05\n","Iteration 980: ||grad|| = 1.2794883332389873e-05\n","Iteration 981: ||grad|| = 1.2794883332389873e-05\n","Iteration 982: ||grad|| = 1.2794883332389873e-05\n","Iteration 983: ||grad|| = 1.2794883332389873e-05\n","Iteration 984: ||grad|| = 1.2794883332389873e-05\n","Iteration 985: ||grad|| = 1.2794883332389873e-05\n","Iteration 986: ||grad|| = 1.2794883332389873e-05\n","Iteration 987: ||grad|| = 1.2794883332389873e-05\n","Iteration 988: ||grad|| = 1.2794883332389873e-05\n","Iteration 989: ||grad|| = 1.2794883332389873e-05\n","Iteration 990: ||grad|| = 1.2794883332389873e-05\n","Iteration 991: ||grad|| = 1.2794883332389873e-05\n","Iteration 992: ||grad|| = 1.2794883332389873e-05\n","Iteration 993: ||grad|| = 1.2794883332389873e-05\n","Iteration 994: ||grad|| = 1.2794883332389873e-05\n","Iteration 995: ||grad|| = 1.2794883332389873e-05\n","Iteration 996: ||grad|| = 1.2794883332389873e-05\n","Iteration 997: ||grad|| = 1.2794883332389873e-05\n","Iteration 998: ||grad|| = 1.2794883332389873e-05\n","Iteration 999: ||grad|| = 1.2794883332389873e-05\n","Iteration 1000: ||grad|| = 1.2794883332389873e-05\n"]}],"source":["# Initialize weights as tensors with gradients\n","w = torch.tensor([0.0, 0.0], requires_grad = True)\n","\n","# Hyperparameters\n","maxiter = 1000\n","tol = 1e-06\n","lr = 1e-02\n","norm_grad = float('inf')\n","\n","k = 0\n","while k < maxiter and norm_grad > tol:\n","    # Zero the gradients\n","    if w.grad is not None:\n","        w.grad.zero_()\n","\n","    # Define the loss function\n","    L = (w[0] - 2)**2 + (w[1] + 3)**2\n","\n","    # Backpropagate to compute gradients\n","    L.backward()\n","\n","    # Update weights using gradient descent\n","    with torch.no_grad():\n","        w -= lr * w.grad\n","\n","    # Compute the norm of the gradient\n","    norm_grad = w.grad.norm().item()\n","    k += 1\n","\n","    print(f'Iteration {k}: ||grad|| = {norm_grad}')"]},{"cell_type":"markdown","metadata":{"id":"IvRw58l8p2T3"},"source":["---\n","\n","We will consider again the same toy data matrix with 6 samples and 3 possible output labels :\n","\n","\n","![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=960)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1ylibPssqTWs"},"source":["---\n","\n","Define the linear layer (dense layer) where the raw scores are calculated through the linear operation:\n","$$\\underbrace{\\mathbf{Z}}_{\\color{red}{6\\times3}} = \\underbrace{\\begin{bmatrix}{\\mathbf{z}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(1)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(2)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(3)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(4)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(5)}}^\\mathrm{T}\\end{bmatrix}}_{\\color{red}{6\\times3}}=\\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}} = \\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\mathbf{W}\\end{bmatrix}}_{\\color{red}{6\\times3}} + \\underbrace{\\begin{bmatrix}{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}}=\\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\end{bmatrix}}_{\\color{red}{6\\times5}}\\underbrace{\\mathbf{W}}_{\\color{red}{5\\times3}}=\\underbrace{\\underbrace{\\mathbf{X}}_{6\\times 5}\\underbrace{\\mathbf{W}}_{5\\times 3}}_{\\color{red}{6\\times3}} + \\underbrace{\\begin{bmatrix}{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}}.$$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vt14T0bgqUxF"},"outputs":[],"source":["class LinearLayer(torch.nn.Module):\n","    def __init__(self, input_dim, nodes = 2):\n","        super(LinearLayer, self).__init__()  # Initialize the parent class (nn.Module)\n","        self.nodes = nodes\n","        # Define the weights and bias as parameters\n","        self.W = torch.nn.Parameter(torch.Tensor(input_dim, self.nodes))\n","        torch.nn.init.xavier_uniform_(self.W)  # Xavier uniform initialization\n","        self.b = torch.nn.Parameter(torch.randn(self.nodes))  # Random Normal initialization\n","\n","    def forward(self, input):\n","        # Linear transformation (input * W + b)\n","        output = torch.matmul(input, self.W) + self.b\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"4Sb0TWaWqosF"},"source":["---\n","\n","Defining a LinearLayer object and calling the forward() method applied to the toy patient data matrix.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNrvxsYgqpve"},"outputs":[],"source":["layer1 = LinearLayer(num_features, 3)\n","print(layer1.W)\n","print(layer1.b)\n","layer1.forward(X_S)"]},{"cell_type":"markdown","metadata":{"id":"4P8i-XsrroCB"},"source":["---\n","\n","Define the softmax layer\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoACMi5FrokZ"},"outputs":[],"source":["class SoftmaxLayer(torch.nn.Module):\n","    def __init__(self):\n","        super(SoftmaxLayer, self).__init__()\n","        self.activation = torch.nn.Softmax(dim = 1)\n","\n","    def forward(self, input):\n","        output = self.activation(input)  # Apply softmax activation\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"wiSxk97fr4xJ"},"source":["---\n","\n","Defining a SoftmaxLayer object and calling the forward() method applied to the toy patient data matrix.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pn_dXu7xr7HV"},"outputs":[],"source":["actlayer1 = SoftmaxLayer()\n","print(actlayer1.activation)\n","actlayer1.forward(layer1.forward(X_S))"]},{"cell_type":"markdown","metadata":{"id":"JRQ0SwBxs3WL"},"source":["---\n","\n","Define the softmax classifier model\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKicexaas31_"},"outputs":[],"source":["class SoftmaxClassifierModel(torch.nn.Module):\n","    def __init__(self, input_dim, nodes=2):\n","        super(SoftmaxClassifierModel, self).__init__()\n","        self.nodes = nodes\n","        self.linearLayer = LinearLayer(input_dim, self.nodes)  # Linear layer\n","        self.softmaxLayer = SoftmaxLayer()  # Softmax activation layer\n","\n","    def forward(self, input):\n","        output = self.linearLayer(input)  # Forward pass through the linear layer\n","        output = self.softmaxLayer(output)  # Apply softmax activation\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"upcq6QXttB8y"},"source":["---\n","\n","Perform forward propagation to the toy patient dataset using the SoftmaxClassifierModel built above.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQIET4VFtNSA"},"outputs":[],"source":["model = SoftmaxClassifierModel(num_features, 3)\n","print(model(X_S))"]},{"cell_type":"markdown","metadata":{"id":"V3G_qHiVtWvj"},"source":["---\n","\n","Define loss function (categorical crossentropy).\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-QsTVjRtYAh"},"outputs":[],"source":["def loss_fn(true_labels, predicted_probs):\n","  loss = torch.mean(-torch.log(torch.sum(true_labels * predicted_probs, dim = 1)))\n","  return(loss)"]},{"cell_type":"markdown","metadata":{"id":"_LpnJT06u5KV"},"source":["---\n","\n","Apply the softmax classifier model to the toy data set and calculate the loss.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ih-XPwbdu5my"},"outputs":[],"source":["## Apply the softmax classifier model to the toy data set and calculate the loss\n","# Instantiate the model object\n","model = SoftmaxClassifierModel(num_features, 3) # invokes the constructor and sets up the layers\n","\n","# Calculate average data loss\n","print(Y)\n","print(model(X_S))\n","loss_fn(Y, model(X_S))"]},{"cell_type":"markdown","metadata":{"id":"s4A6iMHOzAAO"},"source":["---\n","\n","Softmax classifier for the [MNIST](https://www.tensorflow.org/datasets/catalog/mnist) dataset\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6Wc0cWFzI7X"},"outputs":[],"source":["## Load MNIST data (note that shape of X_train and y_train)\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","print(X_train.shape)\n","print(y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cP47GmErzKzV"},"outputs":[],"source":["## Reshape X_train and X_test such that the samples are along the rows\n","X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n","X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n","print(X_train_reshaped.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z54AQD5rzO12"},"outputs":[],"source":["## Problem parameters\n","num_samples_train = X_train_reshaped.shape[0]\n","num_samples_test = X_test_reshaped.shape[0]\n","num_features = X_train_reshaped.shape[1]\n","num_labels = len(np.unique(y_train))\n","print(f'No. of training samples = {num_samples_train},\\\n"," No. of test samples = {num_samples_test}, \\\n"," no. of features = {num_features}, no. of labels = {num_labels}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeTfB5-xzSGi"},"outputs":[],"source":["## One-hot encode output labels using scikit-learn (observe the shape of Y_train)\n","ohe = OneHotEncoder(sparse_output=False)\n","Y_train = torch.tensor(ohe.fit_transform(y_train.reshape(-1, 1)), dtype = torch.float32)\n","Y_test = torch.tensor(ohe.transform(y_test.reshape(-1, 1)), dtype = torch.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCD_yzX1zUE0"},"outputs":[],"source":["## Min-max scale the images using scikit-learn\n","mms = MinMaxScaler()\n","X_train_reshaped_scaled = torch.tensor(mms.fit_transform(X_train_reshaped), dtype=torch.float32)\n","X_test_reshaped_scaled = torch.tensor(mms.transform(X_test_reshaped), dtype=torch.float32)"]},{"cell_type":"markdown","metadata":{"id":"2OpSKsccees6"},"source":["---\n","\n","Train the softmax classifier on the MNIST dataset\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00eKgH6ez7Ix"},"outputs":[],"source":["## Train the softmax classifier on the MNIST dataset\n","# Initialize model\n","model = SoftmaxClassifierModel(num_features, num_labels)\n","\n","# Gradient descent\n","maxiter = 250\n","lr = 1e-03\n","\n","# Define loss function (CrossEntropyLoss in PyTorch includes softmax)\n","#loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer (RMSprop)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n","\n","# Lists to store training and test losses\n","loss_train = [None] * maxiter\n","loss_test = [None] * maxiter\n","\n","# Start training loop\n","for k in range(maxiter):\n","    model.train()  # Set model to training mode\n","\n","    # Forward pass: compute predicted probabilities\n","    Yhat = model(X_train_reshaped_scaled)  # predicted probabilities\n","\n","    # Compute training loss\n","    L_train = loss_fn(Y_train, Yhat)  # CrossEntropyLoss expects raw logits (no softmax needed)\n","\n","    # Append training and test loss values\n","    loss_train[k] = L_train.item()  # Convert to scalar\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation for testing\n","        Yhat_test = model(X_test_reshaped_scaled)  # predicted probabilities for test set\n","        L_test = loss_fn(Y_test, Yhat_test)  # Compute test loss\n","    loss_test[k] = L_test.item()  # Convert to scalar\n","\n","    # Print losses\n","    print(f'Iteration {k+1}, Training loss = {loss_train[k]}, Test loss = {loss_test[k]}')\n","\n","    # Backward pass: compute gradients\n","    optimizer.zero_grad()  # Zero the gradients before the backward pass\n","    L_train.backward()  # Backpropagate the gradients\n","\n","    # Update model parameters using optimizer\n","    optimizer.step()  # Perform one optimization step"]},{"cell_type":"markdown","metadata":{"id":"Yd8PGQKs15kB"},"source":["---\n","\n","Plot training and test loss in the same figure\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HL1069mE16U_"},"outputs":[],"source":["## Plot the training and test loss\n","fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n","ax.plot(loss_train, 'b', label = 'Train')\n","ax.plot(loss_test, 'r', label = 'Test')\n","ax.set_xlabel('Iteration')\n","ax.set_ylabel('Loss')\n","ax.legend();"]},{"cell_type":"markdown","metadata":{"id":"Eqxn_hXR2BM9"},"source":["---\n","\n","Assess model performance on test data\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVdiXg6q2B4I"},"outputs":[],"source":["## Assess model performance on test data\n","Yhat = model(X_test_reshaped_scaled)\n","\n","ypred = np.array(torch.argmax(Yhat, axis = 1)) # predicted labels for the test samples\n","ytrue = np.array(torch.argmax(Y_test, axis = 1)) # true labels for the test samples\n","print('Accuracy on test data = %3.2f'%(np.mean(ytrue == ypred)*100))\n","# Print confusion matrix\n","print(confusion_matrix(ytrue, ypred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11ubRM9p2W3A"},"outputs":[],"source":["## Plot a random test sample with its predicted label printed above the plot\n","test_index = np.random.choice(X_test.shape[0])\n","fig, ax = plt.subplots(1, 1, figsize = (2, 2))\n","print(f'Image classified as {ypred[test_index]}')\n","ax.imshow(tf.reshape(X_test_reshaped_scaled[test_index], [28, 28]).numpy(), cmap = 'gray');"]},{"cell_type":"markdown","metadata":{"id":"brjKnRYGbz6x"},"source":["---\n","\n","Define a nonlinear activation layer with ReLU activation\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKu3VuDGb41Y"},"outputs":[],"source":["class ReLULayer(torch.nn.Module):\n","    def __init__(self):\n","        super(ReLULayer, self).__init__()\n","        self.activation = torch.nn.ReLU()\n","\n","    def forward(self, input):\n","        output = self.activation(input)  # Apply softmax activation\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"NrbyHyBXcRGj"},"source":["---\n","\n","Define a one hidden layer neural network model\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ByD0iWjcXQF"},"outputs":[],"source":["class OneLayerNeuralNetworkModel(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_nodes = 2, nodes = 2):\n","        super(OneLayerNeuralNetworkModel, self).__init__()\n","        self.hidden_nodes = hidden_nodes\n","        self.nodes = nodes\n","        self.linearLayer1 = LinearLayer(input_dim, self.hidden_nodes)  # 1st Linear layer\n","        self.actlayer1 = ReLULayer() # 1st activation layer (ReLU)\n","        self.linearLayer2 = LinearLayer(self.hidden_nodes, self.nodes)  # 2nd Linear layer\n","        self.softmaxLayer = SoftmaxLayer()  # Softmax activation layer\n","\n","    def forward(self, input):\n","        output = self.linearLayer1(input)  # Forward pass through the 1st linear layer\n","        output = self.actlayer1(output) # ReLU activation\n","        output = self.linearLayer2(output)  # Forward pass through the 2nd linear layer\n","        output = self.softmaxLayer(output)  # Apply softmax activation\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"r3XJ5qcjdPaE"},"source":["---\n","\n","Perform forward propagation to the toy patient dataset using the NeuralNetworkModel built above.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skQYdCkadfCF"},"outputs":[],"source":["model = OneLayerNeuralNetworkModel(5, 4, 3) # 4 nodes in hidden layer\n","print(model(X_S))"]},{"cell_type":"markdown","metadata":{"id":"Ie2z0taWekVQ"},"source":["---\n","\n","Train the 1-hidden layer neural network classifier on the MNIST dataset\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWbCrpERen2d"},"outputs":[],"source":["# This is an exercise## Train the softmax classifier on the MNIST dataset\n","# Initialize model\n","model = OneLayerNeuralNetworkModel(num_features, 4, num_labels)\n","\n","# Gradient descent\n","maxiter = 2000\n","lr = 1e-03\n","\n","# Define loss function (CrossEntropyLoss in PyTorch includes softmax)\n","#loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer (RMSprop)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n","\n","# Lists to store training and test losses\n","loss_train = [None] * maxiter\n","loss_test = [None] * maxiter\n","\n","# Start training loop\n","for k in range(maxiter):\n","    model.train()  # Set model to training mode\n","\n","    # Forward pass: compute predicted probabilities\n","    Yhat = model(X_train_reshaped_scaled)  # predicted probabilities\n","\n","    # Compute training loss\n","    L_train = loss_fn(Y_train, Yhat)  # CrossEntropyLoss expects raw logits (no softmax needed)\n","\n","    # Append training and test loss values\n","    loss_train[k] = L_train.item()  # Convert to scalar\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation for testing\n","        Yhat_test = model(X_test_reshaped_scaled)  # predicted probabilities for test set\n","        L_test = loss_fn(Y_test, Yhat_test)  # Compute test loss\n","    loss_test[k] = L_test.item()  # Convert to scalar\n","\n","    # Print losses\n","    print(f'Iteration {k+1}, Training loss = {loss_train[k]}, Test loss = {loss_test[k]}')\n","\n","    # Backward pass: compute gradients\n","    optimizer.zero_grad()  # Zero the gradients before the backward pass\n","    L_train.backward()  # Backpropagate the gradients\n","\n","    # Update model parameters using optimizer\n","    optimizer.step()  # Perform one optimization step"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"colab-windows","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}