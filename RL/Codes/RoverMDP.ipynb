{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPL5GVi5psuH0WDhRcCtIHE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ztRrxGB8W_t-","executionInfo":{"status":"ok","timestamp":1737740476724,"user_tz":-330,"elapsed":1003,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","plt.style.use('dark_background')\n","%matplotlib inline"]},{"cell_type":"code","source":["class RoverMDP():\n","    def __init__(self):\n","        self.states = ['low', 'medium', 'high', 'top']\n","        self.energy_cost = {'slow': 4, 'rapid': 12}\n","\n","    def startState(self):\n","        return self.states[0]\n","\n","    def isEnd(self, state):\n","        return (state == self.states[3])\n","\n","    def actions(self, state):\n","        if state == self.states[3]:\n","            return []\n","        return ['slow', 'rapid']\n","\n","    def NewStateProbReward(self, state, action):\n","        newStateProbReward = []\n","        if action in self.actions(state):\n","            if state == self.states[0]:\n","                next_state = 1\n","            elif state == self.states[1]:\n","                next_state = 2\n","            elif state == self.states[2]:\n","                next_state = 3\n","            else:\n","                return [(state, 1, 0)]  # terminal state\n","\n","            if action == 'slow':\n","                newStateProbReward.append((self.states[next_state], 0.3, -self.energy_cost[action]))\n","                newStateProbReward.append((self.states[0], 0.7, -self.energy_cost[action]))\n","            elif action == 'rapid':\n","                newStateProbReward.append((self.states[next_state], 0.5, -self.energy_cost[action]))\n","                newStateProbReward.append((self.states[0], 0.5, -self.energy_cost[action]))\n","        return newStateProbReward\n","\n","    def discount(self):\n","        return 0.95"],"metadata":{"id":"oeTs9VykZt1z","executionInfo":{"status":"ok","timestamp":1737740515561,"user_tz":-330,"elapsed":453,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Create the die game MDP object\n","roverMdp = RoverMDP()"],"metadata":{"id":"LsAIcPaGZ1kb","executionInfo":{"status":"ok","timestamp":1737740520642,"user_tz":-330,"elapsed":727,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# What are the possible actions from state 0 (in the game)?\n","roverMdp.actions(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"-lFVx18EZ34i","executionInfo":{"status":"ok","timestamp":1737740520642,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"81a0c6e4-b35d-4250-aa3e-1f4dfbcd5b94"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['slow', 'rapid']"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["---\n"," Consider the following policies:\n","\n","\n","* Policy-1: π (slowly | low) = 0.9, π (slowly | medium) = 0.1, π (rapidly | high) = 0.95\n","\n","\n","* Policy-2: π (rapidly | low) = 0.9, π (slowly | medium) = 0.9, π (slowly | high) = 0.95.\n","\n","\n","Briefly describe the policies using plain English.\n","\n","---"],"metadata":{"id":"VZwl1q7saKVL"}},{"cell_type":"code","source":["def policy(roverMdp, state, policyType='Policy1'):\n","    if policyType == 'Policy1':\n","        # Policy-1 specifications:\n","        # π(slowly | low) = 0.9, π(slowly | medium) = 0.1, π(rapidly | high) = 0.95\n","        if state == roverMdp.states[0]:  # low state\n","            action_probs = [0.9, 0.1]  # [slowly, rapidly]\n","            action = np.random.choice(['slowly', 'rapidly'], p=action_probs)\n","        elif state == roverMdp.states[1]:  # medium state\n","            action_probs = [0.1, 0.9]  # [rapidly, slowly]\n","            action = np.random.choice(['rapidly', 'slowly'], p=action_probs)\n","        elif state == roverMdp.states[2]:  # high state\n","            action_probs = [0.05, 0.95]  # [rapidly, slowly]\n","            action = np.random.choice(['rapidly', 'slowly'], p=action_probs)\n","        else:\n","            action = roverMdp.actions(state)[0]\n","\n","    elif policyType == 'Policy2':\n","        if state == roverMdp.states[0]:  # low state\n","            action_probs = [0.9, 0.1]  # ['rapid' , 'slow']\n","            action = np.random.choice(['rapid' , 'slow'], p=action_probs)\n","        elif state == roverMdp.states[1]:  # medium state\n","            action_probs = [0.1, 0.9]  # ['rapid' , 'slow']\n","            action = np.random.choice(['rapid' , 'slow'], p=action_probs)\n","        elif state == roverMdp.states[2]:  # high state\n","            action_probs = [0.05, 0.95]  # ['rapid' , 'slow']\n","            action = np.random.choice(['rapid' , 'slow'], p=action_probs)\n","        else:\n","            action = roverMdp.actions(state)[0]\n","\n","    return action"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"B14bU5JLaVEw","executionInfo":{"status":"ok","timestamp":1737742103074,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"cbffd804-efc0-48f1-de09-de6d6b8a48fa"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'slow'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["---\n"," Calculate the values of the states under both policies above using simulation. Use 0.95\n","as the discount factor. Which policy is better?\n","\n","---"],"metadata":{"id":"Stt1iA8nb4iy"}},{"cell_type":"code","source":["import random\n","import numpy as np\n","\n","# Create the rover MDP object\n","roverMDP = RoverMDP()\n","\n","# Define policies\n","def policy1(state):\n","    if state == 'low':\n","        return 'slow' if random.random() < 0.9 else 'rapid'\n","    elif state == 'medium':\n","        return 'slow' if random.random() < 0.1 else 'rapid'\n","    elif state == 'high':\n","        return 'rapid' if random.random() < 0.95 else 'slow'\n","    return None\n","\n","def policy2(state):\n","    if state == 'low':\n","        return 'rapid' if random.random() < 0.9 else 'slow'\n","    elif state == 'medium':\n","        return 'slow' if random.random() < 0.9 else 'rapid'\n","    elif state == 'high':\n","        return 'slow' if random.random() < 0.95 else 'rapid'\n","    return None\n","\n","# Simulation function\n","def simulate(policy, roverMDP, num_episodes=1000):\n","    state_values = {state: 0 for state in roverMDP.states}\n","    for _ in range(num_episodes):\n","        G = 0  # cumulative reward\n","        k = 0  # time step\n","        state = roverMDP.startState()  # start state\n","        while not roverMDP.isEnd(state):\n","            action = policy(state)  # action to be taken at the current state\n","            if action is None:\n","                break\n","            SPR_list = roverMDP.NewStateProbReward(state, action)\n","            prob = [tup[1] for tup in SPR_list]\n","            index = np.random.choice(range(len(SPR_list)), size=1, p=prob)[0]\n","            newState = SPR_list[index][0]\n","            reward = SPR_list[index][2]  # reward\n","            G = G + (roverMDP.discount())**k * reward\n","            state = newState\n","            k += 1\n","        state_values[state] += G\n","    for state in state_values:\n","        state_values[state] /= num_episodes\n","    return state_values\n","\n","# Run simulations\n","valuesPolicy1 = simulate(policy1, roverMDP)\n","valuesPolicy2 = simulate(policy2, roverMDP)\n","\n","print(\"Values under Policy 1:\", valuesPolicy1)\n","print(\"Values under Policy 1:\", valuesPolicy2)\n","\n","#Determine the better policy\n","\n","betterPolocy = \"Policy 1\" if sum(valuesPolicy1.values()) > sum(valuesPolicy2.values()) else \"Policy 2\"\n","print(f\"Better policy is {betterPolocy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"WsvKWd7zd3fD","executionInfo":{"status":"ok","timestamp":1737741737598,"user_tz":-330,"elapsed":3156,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"d7dbc58b-e53a-45b5-c356-826fb4bffa19"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Values under Policy 1: {'low': 0.0, 'medium': 0.0, 'high': 0.0, 'top': -70.63696009268645}\n","Values under Policy 1: {'low': 0.0, 'medium': 0.0, 'high': 0.0, 'top': -116.44532025498033}\n","Better policy is Policy 1\n"]}]},{"cell_type":"markdown","source":["---\n","Solve the system of equations using the rref() function of the sympy library. Do\n","your answers for the values of the states under Policy-1 agree with what you had from\n","simulation in part (c)?\n","\n","---"],"metadata":{"id":"Z4LubikAercf"}},{"cell_type":"code","source":["import sympy\n","from sympy.matrices import Matrix\n","from sympy import symbols\n","\n","# Define the matrix and vector\n","A = Matrix([\n","    [0.335, -0.0285, 0, 0],\n","    [-0.665, 0.715, -0.285, 0],\n","    [0.065, 0, 0.715, 0],\n","    [0, 0, 0, 1]\n","])\n","\n","b = Matrix([-4.6, -11.6, -12, 0])\n","\n","# Solve the system using rref() method\n","Ab = A.row_join(b)\n","rref_Ab, pivots = Ab.rref(iszerofunc=lambda x: abs(x) < 1e-8, simplify=lambda x: x)\n","\n","x_solution = rref_Ab[:, -1]\n","\n","print(\"Solution using rref():\", x_solution)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhLiYJYMevzt","executionInfo":{"status":"ok","timestamp":1737745081189,"user_tz":-330,"elapsed":426,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}},"outputId":"43c53cf5-3c69-4b77-9711-50b28142e6a4"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Solution using rref(): Matrix([[-16.9712450857036], [-38.0830562705506], [-15.2403763208801], [0]])\n"]}]}]}