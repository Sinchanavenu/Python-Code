{"cells":[{"cell_type":"markdown","id":"HeWgOD1uD1YP","metadata":{"id":"HeWgOD1uD1YP"},"source":["---\n","\n","Load libraries\n","\n","---"]},{"cell_type":"code","execution_count":1,"id":"FXrh9fPyMtwx","metadata":{"id":"FXrh9fPyMtwx","executionInfo":{"status":"ok","timestamp":1736929273765,"user_tz":-330,"elapsed":17627,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"outputs":[],"source":["## Load libraries\n","import pandas as pd\n","import numpy as np\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import tensorflow as tf\n","from sklearn.metrics import confusion_matrix\n","plt.style.use('dark_background')\n","%matplotlib inline"]},{"cell_type":"markdown","id":"ttFTTWbqD4eQ","metadata":{"id":"ttFTTWbqD4eQ"},"source":["---\n","\n","Set printing precision\n","\n","---"]},{"cell_type":"code","execution_count":2,"id":"P3UMZJowDzo0","metadata":{"id":"P3UMZJowDzo0","executionInfo":{"status":"ok","timestamp":1736929273766,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sinchana Venugopal","userId":"08551745987462219121"}}},"outputs":[],"source":["np.set_printoptions(precision = 2)"]},{"cell_type":"markdown","id":"DIsc4Twv6UhG","metadata":{"id":"DIsc4Twv6UhG"},"source":["---\n","\n","Mount Google drive\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"0s0mudRDMxGf","metadata":{"id":"0s0mudRDMxGf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eff35c91-d28c-4f83-d1d9-f06857a3d172"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## Mount Google drive folder if running in Colab\n","if('google.colab' in sys.modules):\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount = True)\n","    DIR = '/content/drive/MyDrive/Colab Notebooks/EvenSemester2024/DL/Codes'\n","    DATA_DIR = DIR + '/Data/'\n","else:\n","    DATA_DIR = 'Data/'"]},{"cell_type":"markdown","id":"D6DHQ6D8CCH9","metadata":{"id":"D6DHQ6D8CCH9"},"source":["---\n","\n","Load MNIST Data\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"CRfIzvOsCDFR","metadata":{"id":"CRfIzvOsCDFR"},"outputs":[],"source":["## Load MNIST data\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n","\n","num_labels = len(np.unique(y_train))\n","num_features = X_train.shape[1]\n","num_samples = X_train.shape[0]\n","\n","# One-hot encode class labels\n","Y_train = tf.keras.utils.to_categorical(y_train)\n","Y_test = tf.keras.utils.to_categorical(y_test)\n","\n","# Normalize the samples (images) using the training data\n","xmax = np.amax(X_train) # 255\n","xmin = np.amin(X_train) # 0\n","X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n","X_test = (X_test - xmin) / (xmax - xmin)\n","\n","print('MNIST set')\n","print('---------------------')\n","print('Number of training samples = %d'%(num_samples))\n","print('Number of features = %d'%(num_features))\n","print('Number of output labels = %d'%(num_labels))"]},{"cell_type":"markdown","id":"BxG1YM906sEr","metadata":{"id":"BxG1YM906sEr"},"source":["---\n","\n","We consider a softmax classfier, which is a 1-layer neural network or a 0-hidden layer neural network, for a batch comprising $b$ samples represented as the $b\\times 784$-matrix $$\\mathbf{X} = \\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\\\\\vdots\\\\{\\mathbf{x}^{(b-1)}}^\\mathrm{T}\\end{bmatrix}$$ with one-hot encoded true labels represented as the $b\\times 10$-matrix (10 possible categories) $$\\mathbf{Y}=\\begin{bmatrix}{\\mathbf{y}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{y}^{(1)}}^\\mathrm{T}\\\\\\vdots\\\\{\\mathbf{y}^{(b-1)}}^\\mathrm{T}\\end{bmatrix}.$$\n","\n","The forward propagation for a generic sample in the batch seen as a $1\\times784$-object $\\mathbf{x}^\\mathrm{T}$ with the bias feature $1$ added is presented below:\n","\n","$$\\small\\begin{align*}\n","\\boxed{\\underbrace{\\mathbf{x}_B^\\mathrm{T}}_{1\\times785}=\\begin{bmatrix}\\mathbf{x}^\\mathrm{T}&1\\end{bmatrix}}&\\rightarrow\\boxed{\\underbrace{{\\mathbf{z}}^\\mathrm{T}}_{1\\times 10} = \\underbrace{\\mathbf{x}_B^\\mathrm{T}}_{1\\times785}\\underbrace{{\\mathbf{W}}}_{785\\times10}}\\rightarrow\\boxed{\\underbrace{{\\mathbf{a}}^\\mathrm{T}}_{1\\times10}=\\text{softmax}\\left(\\underbrace{{\\mathbf{z}}^\\mathrm{T}}_{1\\times10}\\right)}\\rightarrow\\boxed{L = \\sum\\limits_{k=0}^9-y_k\\log\\left(\\hat{y}_k\\right)}.\n","\\end{align*}$$\n","\n","The forward propagation for the same generic sample seen as a $784$-vector $\\mathbf{x}$ with the bias feature $1$ added is presented below (note that the weight matrix has the same name $\\mathbf{W}$ as above for simplicity even though it should show up as $\\mathbf{W}^\\mathrm{T}$):\n","\n","$$\\small\\begin{align*}\n","\\boxed{\\underbrace{\\mathbf{x}_B}_{785}=\\begin{bmatrix}\\mathbf{x}\\\\1\\end{bmatrix}}&\\rightarrow\\boxed{\\underbrace{\\mathbf{z}}_{10} = \\underbrace{\\mathbf{W}}_{10\\times785}\\underbrace{\\mathbf{x}_B}_{785}}\\rightarrow\\boxed{\\underbrace{\\mathbf{a}}_{10}=\\text{softmax}\\left(\\underbrace{\\mathbf{z}}_{10}\\right)}\\rightarrow\\boxed{L = \\sum\\limits_{k=0}^9-y_k\\log\\left(\\hat{y}_k\\right)}.\\end{align*}$$\n","\n","We will derive the update rule for the weights matrix $\\mathbf{W}$ using the setup above.\n","\n","\n","The average crossentropy (CCE) loss for the batch is:$$\\begin{align*}L &=\\frac{1}{b}\\left[L_0+L_1+\\cdots+L_{b-1}\\right]\\\\&=\\frac{1}{b}\\left[\\sum\\limits_{k=0}^9{\\color{yellow}-}y_k^{(0)}\\log\\left(\\hat{y}^{(0)}_k\\right)+\\sum\\limits_{k=0}^9{\\color{yellow}-}y_k^{(1)}\\log\\left(\\hat{y}^{(1)}_k\\right)+\\cdots+\\sum\\limits_{k=0}^9{\\color{yellow}-}y_k^{(b-1)}\\log\\left(\\hat{y}^{(b-1)}_k\\right)\\right]\\\\&=\\frac{1}{b}\\left[{\\color{yellow}-}{\\mathbf{y}^{(0)}}^\\mathrm{T}\\log\\left({\\hat{\\mathbf{y}}^{(0)}}\\right)+{\\color{yellow}-}{\\mathbf{y}^{(1)}}^\\mathrm{T}\\log\\left({\\hat{\\mathbf{y}}^{(1)}}\\right)+\\cdots+{\\color{yellow}-}{\\mathbf{y}^{(b-1)}}^\\mathrm{T}\\log\\left({\\hat{\\mathbf{y}}^{(b-1)}}\\right)\\right].\\end{align*}$$\n","\n","The computational graph for the samples, each at a time treated as a $785$-vector, in the batch are presented below where the weights matrix has shape $10\\times 785.$\n","\n","$\\hspace{1.5in}\\begin{align*}L_0\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(0)} &= \\mathbf{a}^{(0)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(0)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\\hspace{0.25in}\\begin{align*}L_1\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(1)} &= \\mathbf{a}^{(1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\\qquad\\cdots\\qquad$$\\begin{align*} L_{b-1}\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(b-1)} &= \\mathbf{a}^{(b-1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(b-1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$\n","\n","The gradient of the average batch loss w.r.t. the weights is:\n","$$\\small\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &=\\frac{1}{b}\\left[\\nabla_\\mathbf{W}(L_0)+\\nabla_\\mathbf{W}(L_1)+\\cdots+\\nabla_\\mathbf{W}(L_{b-1})\\right]\\end{align*}$$\n","which by chain rule can be written as:\n","\n","$$\\small\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\frac{1}{b}\\left(\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(0)}\\right) \\times\\nabla_{\\mathbf{z}^{(0)}}\\left(\\hat{\\mathbf{y}}^{(0)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)\\right]}_{\\nabla_\\mathbf{W}(L_0)}+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(1)}\\right) \\times\\nabla_{\\mathbf{z}^{(1)}}\\left(\\hat{\\mathbf{y}}^{(1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(1)}}(L_1)\\right]}_{\\nabla_\\mathbf{W}(L_1)}+\\cdots+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(b-1)}\\right) \\times\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\right]}_{\\nabla_\\mathbf{W}(L_{b-1})}\\right)\\\\&=\\frac{1}{b}\\left(\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(0)}\\right) \\times\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)\\right]}_{\\nabla_\\mathbf{W}(L_0)}+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(1)}\\right) \\times\\nabla_{\\mathbf{z}^{(1)}}\\left({\\mathbf{a}}^{(1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(1)}}(L_1)\\right]}_{\\nabla_\\mathbf{W}(L_1)}+\\cdots+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(b-1)}\\right) \\times\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\right]}_{\\nabla_\\mathbf{W}(L_{b-1})}\\right)\\\\&=\\frac{1}{b}\\sum_{i=0}^{b-1}\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(i)}\\right) \\times\\nabla_{\\mathbf{z}^{(i)}}\\left({\\mathbf{a}}^{(i)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(i)}}(L_i)\\right]\\\\&=\\frac{1}{b}\\sum_{i=0}^{b-1}\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{W}{\\mathbf{x}^{(i)}_B}\\right) \\times\\nabla_{\\mathbf{z}^{(i)}}\\left(\\text{softmax}\\left({\\mathbf{z}}^{(i)}\\right)\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(i)}}\\left(-{\\mathbf{y}^{(i)}}^\\mathrm{T}\\log\\left(\\hat{\\mathbf{y}}^{(i)}\\right)\\right)\\right],\\end{align*}$$\n","which can be written as\n","\n","$$\\begin{align*}\\nabla_{\\mathbf{W}}(L) &= \\dfrac{1}{b}\\displaystyle\\sum_{i=0}^{b-1}\\underbrace{\\begin{bmatrix}\\boxed{{\\mathbf{x}^{(i)}_B}\\ \\pmb{0}\\ \\pmb{0}\\ \\ldots\\ \\pmb{0}}&&&&\\\\\\\\&\\boxed{\\pmb{0}\\ {\\mathbf{x}^{(i)}_B}\\ \\pmb{0}\\ \\ldots\\ \\pmb{0}}&&&\\\\&\\hspace{1cm}\\ddots&&&\\\\&&\\hspace{-0.5cm}\\ddots&&\\\\&&&\\boxed{\\pmb{0}\\ \\pmb{0}\\ \\ldots\\ \\pmb{0}\\ {\\mathbf{x}^{(i)}_B}}&\\end{bmatrix}}_{\\color{cyan}{\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(i)}\\right)=\\nabla_\\mathbf{W}\\left(\\mathbf{W}{\\mathbf{x}^{(i)}_B}\\right):\\ 10\\times785\\times10}}\\underbrace{\\begin{bmatrix}a^{(i)}_0 (1 - a^{(i)}_0) & -a^{(i)}_0 a^{(i)}_1 & \\cdots & -a^{(i)}_0 a^{(i)}_9\\\\-a^{(i)}_1 a^{(i)}_0 & a^{(i)}_1 (1 - a^{(i)}_1) & \\cdots & -a^{(i)}_1 a^{(i)}_9\\\\\\vdots & \\vdots & \\ddots & \\vdots\\\\-a^{(i)}_9 a^{(i)}_0 & -a^{(i)}_9 a^{(i)}_1 & \\cdots & a^{(i)}_9 (1 - a^{(i)}_9)\\end{bmatrix}}_{\\color{cyan}{\\nabla_{\\mathbf{z}^{(i)}}\\left({\\mathbf{a}}^{(i)}\\right) = \\nabla_{\\mathbf{z}^{(i)}}\\left(\\text{softmax}\\left({\\mathbf{z}}^{(i)}\\right)\\right):\\ 10\\times10}}\\underbrace{\\begin{bmatrix}-\\frac{y^{(i)}_0}{\\hat{y}^{(i)}_0}\\\\-\\frac{y^{(i)}_1}{\\hat{y}^{(i)}_1}\\\\\\vdots\\\\-\\frac{y^{(i)}_9}{\\hat{y}^{(i)}_9}\\end{bmatrix}}_{\\color{cyan}{\\nabla_{\\hat{\\mathbf{y}}^{(i)}}(L_i)=\\nabla_{\\hat{\\mathbf{y}}^{(i)}}\\left(-{\\mathbf{y}^{(i)}}^\\mathrm{T}\\log\\left(\\hat{\\mathbf{y}}^{(i)}\\right)\\right):\\ 10\\times1}}\\end{align*}$$\n","\n","The forward and backward propagation showing the gradient flow for a generic sample is shown below:\n","\n","![](https://1drv.ms/i/c/37720f927b6ddc34/IQS3b-biQ4W9QpCtJzaZnyCoAQ8_r9i707rpOE1O9I0yntM?width=686&height=93)\n","\n","$$\\begin{align*}\\nabla_{\\mathbf{W}}(L) &=\\dfrac{1}{b}\\displaystyle\\sum_{i=0}^{b-1}\\underbrace{\\begin{bmatrix}a^{(i)}_0 (1 - a^{(i)}_0) & -a^{(i)}_0 a^{(i)}_1 & \\cdots & -a^{(i)}_0 a^{(i)}_9\\\\-a^{(i)}_1 a^{(i)}_0 & a^{(i)}_1 (1 - a^{(i)}_1) & \\cdots & -a^{(i)}_1 a^{(i)}_9\\\\\\vdots & \\vdots & \\ddots & \\vdots\\\\-a^{(i)}_9 a^{(i)}_0 & -a^{(i)}_9 a^{(i)}_1 & \\cdots & a^{(i)}_9 (1 - a^{(i)}_9)\\end{bmatrix}}_{\\color{cyan}{=\\left(\\mathbf{I}-{\\mathbf{a}^{(i)}}^\\mathrm{T}\\right)\\otimes\\mathbf{a}^{(i)}}}\\underbrace{\\begin{bmatrix}-\\frac{y^{(i)}_0}{\\hat{y}^{(i)}_0} \\\\\n","-\\frac{y^{(i)}_1}{\\hat{y}^{(i)}_1}\\\\\\vdots\\\\-\\frac{y^{(i)}_9}{\\hat{y}^{(i)}_9}\\end{bmatrix}}_{\\color{cyan}{=-\\frac{\\mathbf{y}^{(i)}}{\\hat{\\mathbf{y}}^{(i)}}}}{\\mathbf{x}^{(i)}_B}^\\mathrm{T}.\\end{align*}$$\n","\n","We can write the gradient in the following way for efficient coding purposes: $$\\nabla_\\mathbf{W}(L) = \\frac{1}{b}\\sum_{i=0}^{b-1}\\left[\\left(\\mathbf{I}-{\\mathbf{a}^{(i)}}^\\mathrm{T}\\right)\\otimes\\mathbf{a}^{(i)}\\right]\\left[-\\frac{\\mathbf{y}^{(i)}}{\\hat{\\mathbf{y}}^{(i)}}\\right]{\\mathbf{x}^{(i)}_B}^\\mathrm{T}.$$\n","\n","\n","\n","It can be seen that the gradient object has shape $(10\\times 10)\\times(10\\times1)\\times(1\\times785)=10\\times785,$ which is the same shape as the weights matrix $\\mathbf{W}.$ However, our derivation here assumed that the samples are seen as column vectors of the data matrix. The original data matrix has the samples arranged as rows which corresponded to the weights matrix of shape $785\\times10.$ In order to get the gradient w.r.t. that weights matrix, we have to transpose this expression resulting in the update $$\\nabla_\\mathbf{W}(L) = \\frac{1}{b}\\sum_{i=0}^{b-1}\\underbrace{\\mathbf{x}^{(i)}_B}_{\\color{yellow}{785\\times1}}\\underbrace{\\underbrace{\\left[-\\frac{{\\mathbf{y}^{(i)}}^\\mathrm{T}}{{\\hat{\\mathbf{y}}^{(i)}}^\\mathrm{T}}\\right]}_{\\color{magenta}{\\text{output side gradient of softmax layer: }1\\times10}}\\underbrace{\\left[\\left(\\mathbf{I}-{\\mathbf{a}^{(i)}}\\right)\\otimes{\\mathbf{a}^{(i)}}^\\mathrm{T}\\right]}_{\\color{magenta}{\\text{local gradient of softmax layer: }10\\times10}}}_{\\color{yellow}{\\text{output side gradient of dense layer: }1\\times10}}.$$\n","\n","\n","\n","\n","---"]},{"cell_type":"markdown","id":"oKggGU1vRBif","metadata":{"id":"oKggGU1vRBif"},"source":["---\n","\n","A generic layer class with forward and backward methods\n","\n","----"]},{"cell_type":"code","execution_count":null,"id":"tZHmwpk4Q404","metadata":{"id":"tZHmwpk4Q404"},"outputs":[],"source":["class Layer:\n","  def __init__(self):\n","    self.input = None\n","    self.output = None\n","\n","  def forward(self, input):\n","    pass\n","\n","  def backward(self, output_gradient, learning_rate):\n","    pass"]},{"cell_type":"markdown","id":"s6nr7yGIRsa3","metadata":{"id":"s6nr7yGIRsa3"},"source":["---\n","\n","CCE loss and its gradient for the batch samples\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"ivzPpy3FRtQE","metadata":{"id":"ivzPpy3FRtQE"},"outputs":[],"source":["## Define the loss function and its gradient\n","def CCE(Y, Yhat):\n","  return(np.mean(np.sum(-Y*np.log(Yhat), axis = 1)))\n","  # TensorFlow in-built functions\n","  #cce = tf.keras.losses.CategoricalCrossentropy()\n","  #return(cce(Y, Yhat).numpy())\n","\n","def cce_gradient(Y, Yhat):\n","  return(-Y/Yhat)"]},{"cell_type":"markdown","id":"t_hwSUUiVlWD","metadata":{"id":"t_hwSUUiVlWD"},"source":["---\n","\n","Softmax activation layer class\n","\n","\n","---"]},{"cell_type":"code","source":["## Softmax activation layer class\n","class Softmax(Layer):\n","  def forward(self, input):\n","    self.input = input\n","    self.output = tf.nn.softmax(self.input, axis = 1).numpy()\n","\n","  def backward(self, output_gradient, learning_rate= None):\n","    I = np.identity(self.output.shape[1])\n","    local_gradient = (I - self.output[:, :, np.newaxis]) * self.output[:, np.newaxis, :]\n","    return(np.einsum('ij, ijk -> ik', output_gradient, local_gradient))\n"],"metadata":{"id":"6bVplzEcMWlP"},"id":"6bVplzEcMWlP","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"HCDIPQkp7JmN","metadata":{"id":"HCDIPQkp7JmN"},"source":["---\n","\n","Dense layer class\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"aLs-dsgv7Nxl","metadata":{"id":"aLs-dsgv7Nxl"},"outputs":[],"source":["## Dense layer class\n","class Dense(Layer):\n","  def __init__(self, input_size, output_size):\n","    self.weights = 0.01*np.random.randn(input_size+1, output_size) # Bias trick\n","    self.weights[-1, :] = 0.01 # set all the bias values to the same nonzero constant\n","\n","  def forward(self, input):\n","    self.input = np.hstack([input, np.ones((input.shape[0], 1))]) # bias feature as last column\n","    self.output = np.dot(self.input, self.weights)\n","\n","  def backward(self, output_gradient, learning_rate):\n","    # Calculate gradient w.r.t dense layer weights from all input samples in the batch\n","    weights_gradient = (1/output_gradient.shape[0]) * np.einsum('ij, ik -> jk', self.input, output_gradient)\n","    # Update weights for dense layer\n","    self.weights = self.weights + learning_rate*(-weights_gradient)\n","\n"]},{"cell_type":"markdown","id":"Pr2pX28071bj","metadata":{"id":"Pr2pX28071bj"},"source":["---\n","\n","Function to generate sample indices for batch processing according to batch size\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"z7TmsRrw72LB","metadata":{"id":"z7TmsRrw72LB"},"outputs":[],"source":["## Function to generate sample indices for batch processing according to batch size\n","def generate_batch_indices(num_samples, batch_size):\n","  # Reorder sample indices\n","  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n","  # Generate batch indices for batch processing\n","  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n","  return(batch_indices)"]},{"cell_type":"markdown","id":"XNNMRK7u75l7","metadata":{"id":"XNNMRK7u75l7"},"source":["---\n","\n","Example generation of batch indices\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"Q34Ulk_v78bv","metadata":{"id":"Q34Ulk_v78bv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c39d4cf2-b8da-4663-91a6-14c8d29d6ddb"},"outputs":[{"output_type":"stream","name":"stdout","text":["[array([12, 15, 10, 31]), array([11, 13, 30, 24]), array([ 6,  4, 28, 22]), array([ 9,  3, 18, 21]), array([ 5, 27,  8, 23]), array([19, 17,  0, 26]), array([29, 16,  7,  1]), array([ 2, 20, 25, 14])]\n"]}],"source":["## Example generation of batch indices\n","batch_size = 4\n","batch_indices = generate_batch_indices(32, batch_size)\n","print(batch_indices)"]},{"cell_type":"markdown","id":"BzW-2lIo8As4","metadata":{"id":"BzW-2lIo8As4"},"source":["---\n","\n","Train the 1-layer neural (softmax) neural network using batch training with batch size = 100\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"Jzk8_9Xl8Bdu","metadata":{"id":"Jzk8_9Xl8Bdu"},"outputs":[],"source":["## Train the 1-layer neural network using batch training with batch size = 100\n","learning_rate = ?\n","batch_size = ?\n","nepochs = ?\n","# Create empty array to store training losses over each epoch\n","loss_train_epoch = np.empty(nepochs, dtype = np.float64)\n","# Create empty array to store test losses over each epoch\n","loss_test_epoch = np.empty(nepochs, dtype = np.float64)\n","\n","# Neural network architecture ()\n","dlayer1 = ? # define dense layer 1\n","softmax = ? # define softmax activation layer\n","\n","# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n","# and update weights.\n","epoch = 0\n","# Run over each epoch\n","while epoch < nepochs:\n","  # Generate the batches\n","  batch_indices = generate_batch_indices(num_samples, batch_size)\n","  loss = 0\n","  # Run over each batch of samples\n","  for b in range(len(?)):\n","    # Forward prop starts here\n","    ?\n","    # Forward prop ends and backward prop starts here\n","    ?\n","  # Calculate the average training loss for the current epoch\n","  loss_train_epoch[epoch] = ?\n","\n","  # Forward propagation for test data\n","  ?\n","\n","  # Calculate test data loss\n","  loss_test_epoch[epoch] = ?\n","\n","  print('Epoch %d: train loss = %f, test loss = %f'%(epoch+1, loss_train_epoch[epoch], loss_test_epoch[epoch]))\n","  epoch = epoch + 1"]},{"cell_type":"markdown","id":"8GWSFNvg8arf","metadata":{"id":"8GWSFNvg8arf"},"source":["---\n","\n","Plot training and test loss vs. epoch\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"S72SmRqD8dca","metadata":{"id":"S72SmRqD8dca"},"outputs":[],"source":["## Plot train and test loss as a function of epoch:\n","fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n","fig.tight_layout(pad = 4.0)\n","ax.plot(loss_train_epoch, 'b', label = 'Train')\n","ax.plot(loss_test_epoch, 'r', label = 'Test')\n","ax.set_xlabel('Epoch', fontsize = 12)\n","ax.set_ylabel('Loss value', fontsize = 12)\n","ax.legend()\n","ax.set_title('Loss vs. Epoch for Softmax Classifier', fontsize = 14);"]},{"cell_type":"markdown","id":"oqR5Q5qS8mWm","metadata":{"id":"oqR5Q5qS8mWm"},"source":["---\n","\n","Assess model performance on test data\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"Su24oMSJTxEb","metadata":{"id":"Su24oMSJTxEb"},"outputs":[],"source":["## Assess model performance on test data\n","?\n","ypred = ?\n","ytrue = ?\n","print('Accuracy on test data = %3.2f'%(np.mean(ytrue == ypred)*100))\n","# Print confusion matrix\n","print(confusion_matrix(ytrue, ypred))"]},{"cell_type":"code","source":["## Plot a random test sample with its predicted label printed above the plot\n","test_index = np.random.choice(X_test.shape[0])\n","fig, ax = plt.subplots(1, 1, figsize = (2, 2))\n","print(f'Image classified as {ypred[test_index]}')\n","ax.imshow(tf.reshape(X_test[test_index], [28, 28]).numpy(), cmap = 'gray');"],"metadata":{"id":"tC5jdfKsaFq6"},"id":"tC5jdfKsaFq6","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"required_libs":[]},"nbformat":4,"nbformat_minor":5}